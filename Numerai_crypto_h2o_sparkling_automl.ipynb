{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/develuse/numerai-crypto-h2o-sw-automl2?scriptVersionId=233128247\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install -q numerapi requests pyarrow fastparquet pydrive2\n# Installeer Java (vereist voor H2O en Spark)\n!apt-get update -qq\n!apt-get install -y default-jre > /dev/null\n!java -version\n\n# Installeer Spark en PySpark\n!pip install -q pyspark==3.1.2\n\n# Installeer H2O Sparkling Water\n!pip install -q h2o-pysparkling-3.1\n\n# Installeer andere benodigde packages\n# Gebruik scikit-learn 1.0.2 voor compatibiliteit, zonder waarschuwingen weer te geven\n!pip install -q numerapi pandas h2o cloudpickle==2.2.1 scikit-learn==1.0.2 scipy==1.10.1 matplotlib xgboost==1.6.2 --no-deps\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T15:31:00.801849Z","iopub.execute_input":"2025-04-10T15:31:00.8022Z","iopub.status.idle":"2025-04-10T15:32:48.460354Z","shell.execute_reply.started":"2025-04-10T15:31:00.802173Z","shell.execute_reply":"2025-04-10T15:32:48.459288Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hW: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\nopenjdk version \"11.0.26\" 2025-01-21\nOpenJDK Runtime Environment (build 11.0.26+4-post-Ubuntu-1ubuntu122.04)\nOpenJDK 64-Bit Server VM (build 11.0.26+4-post-Ubuntu-1ubuntu122.04, mixed mode, sharing)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.4/212.4 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.6/198.6 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.4/232.4 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for h2o-pysparkling-3.1 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.9/255.9 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Numerai Crypto Competitie Voorspellingsmodel met H2O Sparkling Water\n\nDit notebook implementeert een voorspellingsmodel voor de Numerai/Numerai Crypto competitie met behulp van H2O Sparkling Water, wat H2O integreert met Apache Spark voor gedistribueerde verwerking.","metadata":{}},{"cell_type":"markdown","source":"## Installatie van benodigde packages\n\nEerst moeten we Java, Spark en H2O Sparkling Water installeren. Dit kan enige tijd duren.","metadata":{}},{"cell_type":"code","source":"# Importeer benodigde bibliotheken\n# Numerapi imports\nfrom numerapi import NumerAPI, CryptoAPI\n\n# Data download en preparatie imports\nimport pandas as pd\nimport json\nimport os\nfrom typing import List\nimport gc\n\n# Berekening imports\nimport numpy as np\nimport time\nimport random\n\n# Spark imports\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.sql.functions import col\n\n# H2O Sparkling Water imports\nfrom pysparkling import H2OContext\nfrom h2o.estimators.xgboost import H2OXGBoostEstimator\nimport h2o\nimport cloudpickle\nfrom datetime import datetime\n\n# Visualization imports\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Model imports\nimport lightgbm as lgb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T15:36:18.765614Z","iopub.execute_input":"2025-04-10T15:36:18.765982Z","iopub.status.idle":"2025-04-10T15:36:18.771127Z","shell.execute_reply.started":"2025-04-10T15:36:18.765957Z","shell.execute_reply":"2025-04-10T15:36:18.770127Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## Initialiseren van Spark en H2O Sparkling Water","metadata":{}},{"cell_type":"code","source":"# Maak een map voor het opslaan van gegevens en modellen\n!mkdir -p /kaggle/working/numerai\n\n# Initialiseer Spark sessie met betere resources (pas aan op basis van je Kaggle-omgeving)\nspark = SparkSession.builder \\\n    .appName(\"NumeraiSparklingWater\") \\\n    .config(\"spark.executor.memory\", \"5g\") \\\n    .config(\"spark.driver.memory\", \"5g\") \\\n    .config(\"spark.executor.cores\", \"2\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n    .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n    .config(\"spark.locality.wait\", \"0s\") \\\n    .getOrCreate()\n\n# Initialiseer H2O Sparkling Water context\nh2o_context = H2OContext.getOrCreate()\n\n# Print Spark en H2O versie informatie\nprint(f\"Spark version: {spark.version}\")\nprint(f\"H2O cluster version: {h2o.__version__}\")  # Gecorrigeerde versie-attribuut\n# De getSparklingWaterVersion methode bestaat niet, we slaan deze over\n# In plaats daarvan kunnen we de H2O cluster info printen\nprint(f\"H2O cluster info: {h2o.cluster().show_status()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T15:35:08.421982Z","iopub.execute_input":"2025-04-10T15:35:08.422644Z","iopub.status.idle":"2025-04-10T15:35:28.674008Z","shell.execute_reply.started":"2025-04-10T15:35:08.422615Z","shell.execute_reply":"2025-04-10T15:35:28.673097Z"}},"outputs":[{"name":"stdout","text":"Connecting to H2O server at http://3c0a1f79c03a:54323 ... successful.\nWarning: Your H2O cluster version is (5 months and 8 days) old.  There may be a newer version available.\nPlease download and install the latest version from: https://h2o-release.s3.amazonaws.com/h2o/latest_stable.html\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"--------------------------  ----------------------------------------\nH2O_cluster_uptime:         12 secs\nH2O_cluster_timezone:       Etc/UTC\nH2O_data_parsing_timezone:  UTC\nH2O_cluster_version:        3.46.0.6\nH2O_cluster_version_age:    5 months and 8 days\nH2O_cluster_name:           sparkling-water-root_local-1744299311916\nH2O_cluster_total_nodes:    1\nH2O_cluster_free_memory:    5 Gb\nH2O_cluster_total_cores:    4\nH2O_cluster_allowed_cores:  2\nH2O_cluster_status:         locked, healthy\nH2O_connection_url:         http://3c0a1f79c03a:54323\nH2O_connection_proxy:       null\nH2O_internal_security:      False\nPython_version:             3.10.12 final\n--------------------------  ----------------------------------------","text/html":"\n<style>\n\n#h2o-table-1.h2o-container {\n  overflow-x: auto;\n}\n#h2o-table-1 .h2o-table {\n  /* width: 100%; */\n  margin-top: 1em;\n  margin-bottom: 1em;\n}\n#h2o-table-1 .h2o-table caption {\n  white-space: nowrap;\n  caption-side: top;\n  text-align: left;\n  /* margin-left: 1em; */\n  margin: 0;\n  font-size: larger;\n}\n#h2o-table-1 .h2o-table thead {\n  white-space: nowrap; \n  position: sticky;\n  top: 0;\n  box-shadow: 0 -1px inset;\n}\n#h2o-table-1 .h2o-table tbody {\n  overflow: auto;\n}\n#h2o-table-1 .h2o-table th,\n#h2o-table-1 .h2o-table td {\n  text-align: right;\n  /* border: 1px solid; */\n}\n#h2o-table-1 .h2o-table tr:nth-child(even) {\n  /* background: #F5F5F5 */\n}\n\n</style>      \n<div id=\"h2o-table-1\" class=\"h2o-container\">\n  <table class=\"h2o-table\">\n    <caption></caption>\n    <thead></thead>\n    <tbody><tr><td>H2O_cluster_uptime:</td>\n<td>12 secs</td></tr>\n<tr><td>H2O_cluster_timezone:</td>\n<td>Etc/UTC</td></tr>\n<tr><td>H2O_data_parsing_timezone:</td>\n<td>UTC</td></tr>\n<tr><td>H2O_cluster_version:</td>\n<td>3.46.0.6</td></tr>\n<tr><td>H2O_cluster_version_age:</td>\n<td>5 months and 8 days</td></tr>\n<tr><td>H2O_cluster_name:</td>\n<td>sparkling-water-root_local-1744299311916</td></tr>\n<tr><td>H2O_cluster_total_nodes:</td>\n<td>1</td></tr>\n<tr><td>H2O_cluster_free_memory:</td>\n<td>5 Gb</td></tr>\n<tr><td>H2O_cluster_total_cores:</td>\n<td>4</td></tr>\n<tr><td>H2O_cluster_allowed_cores:</td>\n<td>2</td></tr>\n<tr><td>H2O_cluster_status:</td>\n<td>locked, healthy</td></tr>\n<tr><td>H2O_connection_url:</td>\n<td>http://3c0a1f79c03a:54323</td></tr>\n<tr><td>H2O_connection_proxy:</td>\n<td>null</td></tr>\n<tr><td>H2O_internal_security:</td>\n<td>False</td></tr>\n<tr><td>Python_version:</td>\n<td>3.10.12 final</td></tr></tbody>\n  </table>\n</div>\n"},"metadata":{}},{"name":"stdout","text":"\nSparkling Water Context:\n * Sparkling Water Version: 3.46.0.6-1-3.1\n * H2O name: sparkling-water-root_local-1744299311916\n * cluster size: 1\n * list of used nodes:\n  (executorId, host, port)\n  ------------------------\n  (0,172.19.2.2,54321)\n  ------------------------\n\n  Open H2O Flow in browser: http://3c0a1f79c03a:54323 (CMD + click in Mac OSX)\n\n    \nSpark version: 3.1.2\nH2O cluster version: 3.46.0.6\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"--------------------------  ----------------------------------------\nH2O_cluster_uptime:         12 secs\nH2O_cluster_timezone:       Etc/UTC\nH2O_data_parsing_timezone:  UTC\nH2O_cluster_version:        3.46.0.6\nH2O_cluster_version_age:    5 months and 8 days\nH2O_cluster_name:           sparkling-water-root_local-1744299311916\nH2O_cluster_total_nodes:    1\nH2O_cluster_free_memory:    5 Gb\nH2O_cluster_total_cores:    4\nH2O_cluster_allowed_cores:  2\nH2O_cluster_status:         locked, healthy\nH2O_connection_url:         http://3c0a1f79c03a:54323\nH2O_connection_proxy:       null\nH2O_internal_security:      False\nPython_version:             3.10.12 final\n--------------------------  ----------------------------------------","text/html":"\n<style>\n\n#h2o-table-2.h2o-container {\n  overflow-x: auto;\n}\n#h2o-table-2 .h2o-table {\n  /* width: 100%; */\n  margin-top: 1em;\n  margin-bottom: 1em;\n}\n#h2o-table-2 .h2o-table caption {\n  white-space: nowrap;\n  caption-side: top;\n  text-align: left;\n  /* margin-left: 1em; */\n  margin: 0;\n  font-size: larger;\n}\n#h2o-table-2 .h2o-table thead {\n  white-space: nowrap; \n  position: sticky;\n  top: 0;\n  box-shadow: 0 -1px inset;\n}\n#h2o-table-2 .h2o-table tbody {\n  overflow: auto;\n}\n#h2o-table-2 .h2o-table th,\n#h2o-table-2 .h2o-table td {\n  text-align: right;\n  /* border: 1px solid; */\n}\n#h2o-table-2 .h2o-table tr:nth-child(even) {\n  /* background: #F5F5F5 */\n}\n\n</style>      \n<div id=\"h2o-table-2\" class=\"h2o-container\">\n  <table class=\"h2o-table\">\n    <caption></caption>\n    <thead></thead>\n    <tbody><tr><td>H2O_cluster_uptime:</td>\n<td>12 secs</td></tr>\n<tr><td>H2O_cluster_timezone:</td>\n<td>Etc/UTC</td></tr>\n<tr><td>H2O_data_parsing_timezone:</td>\n<td>UTC</td></tr>\n<tr><td>H2O_cluster_version:</td>\n<td>3.46.0.6</td></tr>\n<tr><td>H2O_cluster_version_age:</td>\n<td>5 months and 8 days</td></tr>\n<tr><td>H2O_cluster_name:</td>\n<td>sparkling-water-root_local-1744299311916</td></tr>\n<tr><td>H2O_cluster_total_nodes:</td>\n<td>1</td></tr>\n<tr><td>H2O_cluster_free_memory:</td>\n<td>5 Gb</td></tr>\n<tr><td>H2O_cluster_total_cores:</td>\n<td>4</td></tr>\n<tr><td>H2O_cluster_allowed_cores:</td>\n<td>2</td></tr>\n<tr><td>H2O_cluster_status:</td>\n<td>locked, healthy</td></tr>\n<tr><td>H2O_connection_url:</td>\n<td>http://3c0a1f79c03a:54323</td></tr>\n<tr><td>H2O_connection_proxy:</td>\n<td>null</td></tr>\n<tr><td>H2O_internal_security:</td>\n<td>False</td></tr>\n<tr><td>Python_version:</td>\n<td>3.10.12 final</td></tr></tbody>\n  </table>\n</div>\n"},"metadata":{}},{"name":"stdout","text":"H2O cluster info: None\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## Initialiseren van de Numerai API","metadata":{}},{"cell_type":"code","source":"# Initialiseer de Numerai API client\n# Voor het indienen van voorspellingen zijn API keys nodig\n# napi = NumerAPI(public_id=\"UW_PUBLIC_ID\", secret_key=\"UW_SECRET_KEY\")\nnapi = NumerAPI()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T15:35:48.938287Z","iopub.execute_input":"2025-04-10T15:35:48.93859Z","iopub.status.idle":"2025-04-10T15:35:48.943187Z","shell.execute_reply.started":"2025-04-10T15:35:48.938562Z","shell.execute_reply":"2025-04-10T15:35:48.942345Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Data downloaden en laden","metadata":{}},{"cell_type":"code","source":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nstart_mem = df.memory_usage().sum() / 1024**2\n\nfor col in df.columns:\n    col_type = df[col].dtypes\n    if col_type in numerics:\n        c_min = df[col].min()\n        c_max = df[col].max()\n        if str(col_type)[:3] == 'int':\n            if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                df[col] = df[col].astype(np.int8)\n            elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                df[col] = df[col].astype(np.int16)\n            elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                df[col] = df[col].astype(np.int32)\n            elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                df[col] = df[col].astype(np.int64)\n        else:\n            if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                df[col] = df[col].astype(np.float16)\n            elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                df[col] = df[col].astype(np.float32)\n            else:\n                df[col] = df[col].astype(np.float64)\n\nend_mem = df.memory_usage().sum() / 1024**2\nprint('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\nprint('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\nreturn df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T15:48:58.107791Z","iopub.execute_input":"2025-04-10T15:48:58.108088Z","iopub.status.idle":"2025-04-10T15:48:58.204985Z","shell.execute_reply.started":"2025-04-10T15:48:58.108048Z","shell.execute_reply":"2025-04-10T15:48:58.20329Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-ad557c17e698>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnumerics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'int16'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int32'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int64'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'float16'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'float32'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'float64'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstart_mem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcol_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"],"ename":"NameError","evalue":"name 'df' is not defined","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"%%time\n# Download the Numerai training data to the current directory\nnapi.download_dataset(filename = \"crypto/v1.0/train_targets.parquet\", \n                      dest_path = os.getcwd() + \"/numerai_train_targets.parquet\")\n#napi.download_dataset(filename = \"crypto/v2.0/train_targets.parquet\", \n#                      dest_path = os.getcwd() + \"/numerai_train_targets.parquet\")\n# Download the Numerai live crypto universe to the current directory\nnapi.download_dataset(filename = \"crypto/v1.0/live_universe.parquet\", \n                      dest_path = os.getcwd() + \"/numerai_live_universe.parquet\")\n#napi.download_dataset(filename = \"crypto/v2.0/live_universe.parquet\", \n#                      dest_path = os.getcwd() + \"/numerai_live_universe.parquet\")\n\n# Load the Numerai training targets\ntrain_df = pd.read_parquet(\"numerai_train_targets.parquet\")\n# Load the Numerai live universe\nlive = pd.read_parquet(\"numerai_live_universe.parquet\")\ndisplay(df_numerai_universe.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T15:35:51.445992Z","iopub.execute_input":"2025-04-10T15:35:51.446307Z","iopub.status.idle":"2025-04-10T15:35:52.281946Z","shell.execute_reply.started":"2025-04-10T15:35:51.44628Z","shell.execute_reply":"2025-04-10T15:35:52.281121Z"}},"outputs":[{"name":"stderr","text":"/kaggle/working/numerai_train_targets.parquet: 797kB [00:00, 11.5MB/s]                   \n/kaggle/working/numerai_live_universe.parquet: 8.19kB [00:00, 7.86MB/s]                   \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   symbol\n0     BTC\n12    ETH\n36    NMR\n2     XRP\n47    BNB","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>symbol</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>BTC</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>ETH</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>NMR</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>XRP</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>BNB</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"CPU times: user 136 ms, sys: 58.2 ms, total: 195 ms\nWall time: 830 ms\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"%%time\napi = CryptoAPI()\n# Parquet files\napi.download_dataset(\n\t\"crypto/v1.0/live_universe.parquet\",\n\t\"numerai_crypto_live_universe.parquet\"\n)\ngc.collect()\napi.download_dataset(\n\t\"crypto/v1.0/train_targets.parquet\",\n\t\"numerai_crypto_train_targets.parquet\"\n)\ngc.collect()\napi.download_dataset(\n\t\"crypto/v1.0/meta_model.parquet\",\n\t\"numerai_crypto_meta_model.parquet\"\n)\ngc.collect()\napi.download_dataset(\n\t\"crypto/v1.0/historical_meta_models.parquet\",\n\t\"numerai_crypto_historical_meta_models.parquet\"\n)\ngc.collect()\n# CSV Files\napi.download_dataset(\n\t\"crypto/v1.0/meta_model.csv\",\n\t\"numerai_crypto_meta_model.csv\"\n)\ngc.collect()\napi.download_dataset(\n\t\"crypto/v1.0/historical_meta_models.csv\",\n\t\"numerai_crypto_historical_meta_models.csv\"\n)\ngc.collect()\nworking_dir = '/kaggle/working/'\nfiles = os.listdir(working_dir)\nprint(\"Files in /kaggle/working/:\")\nfor f in files:\n    print(f)\n# # Load the data\nnumerai_crypto_train_targets = pd.read_parquet('numerai_crypto_train_targets.parquet')\nnumerai_crypto_live_universe = pd.read_parquet('numerai_crypto_live_universe.parquet')\ngc.collect()\nhistorical_meta_model_preds = pd.read_parquet('numerai_crypto_historical_meta_models.parquet')\nlive_meta_model_preds = pd.read_parquet('numerai_crypto_meta_model.parquet')\ngc.collect()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T15:36:25.221635Z","iopub.execute_input":"2025-04-10T15:36:25.221976Z","iopub.status.idle":"2025-04-10T15:36:28.293369Z","shell.execute_reply.started":"2025-04-10T15:36:25.221948Z","shell.execute_reply":"2025-04-10T15:36:28.292572Z"}},"outputs":[{"name":"stderr","text":"numerai_crypto_train_targets.parquet: 797kB [00:00, 12.2MB/s]                   \nnumerai_crypto_meta_model.parquet: 9.22kB [00:00, 7.57MB/s]                   \nnumerai_crypto_historical_meta_models.parquet: 387kB [00:00, 6.88MB/s]                   \nnumerai_crypto_meta_model.csv: 6.14kB [00:00, 6.62MB/s]                   \nnumerai_crypto_historical_meta_models.csv: 2.64MB [00:00, 23.5MB/s]                            \n","output_type":"stream"},{"name":"stdout","text":"Files in /kaggle/working/:\n.virtual_documents\nnumerai_live_universe.parquet\nnumerai\nnumerai_crypto_meta_model.csv\nnumerai_crypto_train_targets.parquet\nnumerai_crypto_live_universe.parquet\nnumerai_crypto_meta_model.parquet\nnumerai_crypto_historical_meta_models.parquet\nnumerai_crypto_historical_meta_models.csv\nnumerai_train_targets.parquet\nh2ologs\nCPU times: user 1.3 s, sys: 44.2 ms, total: 1.34 s\nWall time: 3.06 s\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# Helper Function from example: https://github.com/councilofelders/notebooks/blob/main/yiedl_crypto_data/yiedl_crypto_data_for_numerai_example.ipynb\nimport requests\n\ndef download_file(url, output_filename):\n    response = requests.get(url)\n    if response.status_code == 200:\n        with open(output_filename, 'wb') as file:\n            file.write(response.content)\n        print(f\"File downloaded successfully as {output_filename}\")\n    else:\n        print(\"Failed to download file\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T15:36:45.673521Z","iopub.execute_input":"2025-04-10T15:36:45.673934Z","iopub.status.idle":"2025-04-10T15:36:45.679924Z","shell.execute_reply.started":"2025-04-10T15:36:45.6739Z","shell.execute_reply":"2025-04-10T15:36:45.679039Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"%%time\n# Download YIEDL crypto latest dataset to current directory\nurl = 'https://api.yiedl.ai/yiedl/v1/downloadDataset?type=latest'\noutput_filename = 'yiedl_latest.parquet'\ndownload_file(url, output_filename)\n\n\n# Download YIEDL crypto historical dataset to current directory\n# NOTE: it is a huge file in zip format. We need to unzip it afterwards\nurl = 'https://api.yiedl.ai/yiedl/v1/downloadDataset?type=historical'\noutput_filename = 'yiedl_historical.zip'\ndownload_file(url, output_filename)\n#10m9s","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T15:36:50.612722Z","iopub.execute_input":"2025-04-10T15:36:50.61307Z","iopub.status.idle":"2025-04-10T15:43:59.975818Z","shell.execute_reply.started":"2025-04-10T15:36:50.613044Z","shell.execute_reply":"2025-04-10T15:43:59.974907Z"}},"outputs":[{"name":"stdout","text":"File downloaded successfully as yiedl_latest.parquet\nFile downloaded successfully as yiedl_historical.zip\nCPU times: user 27.3 s, sys: 33.8 s, total: 1min 1s\nWall time: 7min 9s\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"%%time\n# Unzip and rename the file\n!unzip -p yiedl_historical.zip > yiedl_historical.parquet\n!rm yiedl_historical.zip\n## 1m12s","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T15:43:59.977142Z","iopub.execute_input":"2025-04-10T15:43:59.977469Z","iopub.status.idle":"2025-04-10T15:44:54.747276Z","shell.execute_reply.started":"2025-04-10T15:43:59.977435Z","shell.execute_reply":"2025-04-10T15:44:54.746159Z"}},"outputs":[{"name":"stdout","text":"CPU times: user 996 ms, sys: 205 ms, total: 1.2 s\nWall time: 54.8 s\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"%%time\n# Load and display the YIEDL historical crypto dataset\ndf_yield_historical = pd.read_parquet(\"yiedl_historical.parquet\",\n                                      engine = \"pyarrow\",\n                                      dtype_backend = \"numpy_nullable\")                                      \n# Check dtypes\ndf_yield_historical.dtypes\n# Display\n#display(df_yield_historical)\n## ms","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T15:44:54.749223Z","iopub.execute_input":"2025-04-10T15:44:54.749547Z","execution_failed":"2025-04-10T15:46:41.623Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n# Load and display the YIEDL latest crypto dataset\ndf_yield_latest = pd.read_parquet(\"yiedl_latest.parquet\", \n                                  engine = \"pyarrow\",\n                                  dtype_backend = \"numpy_nullable\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-10T15:46:41.624Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"''' numerai competitie dataset\n# Gebruik een van de nieuwste dataversies\nDATA_VERSION = \"v5.0\"\n\n# Maak een data directory\n!mkdir -p {DATA_VERSION}\n\n# Download data\nprint(\"Downloading training data...\")\nnapi.download_dataset(f\"{DATA_VERSION}/train.parquet\")\nnapi.download_dataset(f\"{DATA_VERSION}/features.json\")\n\n# Laad feature metadata\nfeature_metadata = json.load(open(f\"{DATA_VERSION}/features.json\"))\nprint(\"Available feature sets:\", list(feature_metadata[\"feature_sets\"].keys()))\nfeatures = feature_metadata[\"feature_sets\"][\"small\"]  # gebruik \"small\" voor sneller testen, \"medium\" of \"all\" voor betere prestaties\n'''","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-10T15:29:33.491Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\n# PyDrive implementation for Google Drive integration\nfrom pydrive2.auth import GoogleAuth\nfrom pydrive2.drive import GoogleDrive\nfrom google.colab import auth\nfrom oauth2client.client import GoogleCredentials\n\ndef setup_pydrive():\n    # Authenticate and create the PyDrive client\n    auth.authenticate_user()\n    gauth = GoogleAuth()\n    gauth.credentials = GoogleCredentials.get_application_default()\n    drive = GoogleDrive(gauth)\n    return drive\n\ndef create_folder_if_not_exists(drive, folder_name):\n    # Check if folder exists\n    file_list = drive.ListFile({\"q\": f\"title='{folder_name}' and mimeType='application/vnd.google-apps.folder' and trashed=false\"}).GetList()\n    \n    if len(file_list) > 0:\n        # Folder exists, return the folder ID\n        return file_list[0][\"id\"]\n    else:\n        # Create folder\n        folder = drive.CreateFile({\"title\": folder_name, \"mimeType\": \"application/vnd.google-apps.folder\"})\n        folder.Upload()\n        return folder[\"id\"]\n\ndef save_notebook_to_drive(drive, folder_id, notebook_name):\n    # Create a file in the folder\n    file = drive.CreateFile({\"title\": notebook_name, \"parents\": [{\"id\": folder_id}]})\n    \n    # Get the content of the current notebook\n    notebook_content = open(notebook_name, \"r\").read()\n    \n    # Set the content of the file\n    file.SetContentString(notebook_content)\n    file.Upload()\n    \n    return file[\"id\"]\n\ntry:\n    # Setup PyDrive\n    drive = setup_pydrive()\n    print(\"Successfully authenticated with Google Drive\")\n    \n    # Create Numer_crypto folder if it doesn't exist\n    folder_id = create_folder_if_not_exists(drive, \"Numer_crypto\")\n    print(f\"Numer_crypto folder ID: {folder_id}\")\n    \n    ## Save the current notebook to the folder\n    #notebook_name = \"numerai_sparkling_water_kaggle.ipynb\"\n    #file_id = save_notebook_to_drive(drive, folder_id, notebook_name)\n    #print(f\"Notebook saved to Google Drive with file ID: {file_id}\")\n    \n    ## List files in the folder\n    #file_list = drive.ListFile({\"q\": f\"'{folder_id}' in parents and trashed=false\"}).GetList()\n    #print(\"Files in Numer_crypto folder:\")\n    #for file in file_list:\n    #    print(f\"- {file['title']} (ID: {file['id']})\")\nexcept Exception as e:\n    print(f\"Error with PyDrive: {e}\")\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T15:32:58.464448Z","iopub.execute_input":"2025-04-10T15:32:58.464768Z","iopub.status.idle":"2025-04-10T15:33:39.964091Z","shell.execute_reply.started":"2025-04-10T15:32:58.464742Z","shell.execute_reply":"2025-04-10T15:33:39.963232Z"}},"outputs":[{"name":"stdout","text":"Successfully authenticated with Google Drive\nNumer_crypto folder ID: 1nLS8F4unm5wKYIgzTqk8tPyURpRx17w3\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Data laden met PySpark","metadata":{}},{"cell_type":"code","source":"'''\n# Laad trainingsdata met Spark\nprint(\"Loading training data with Spark...\")\ntrain_spark = spark.read.parquet(f\"{DATA_VERSION}/train.parquet\")\n\n# Selecteer alleen de benodigde kolommen\ncolumns_to_select = [\"era\"] + features + [\"target\"]\ntrain_spark = train_spark.select(*columns_to_select)\n\n# Downsampling voor snelheid (optioneel)\nprint(\"Preparing data for training...\")\n# Haal unieke era's op en sample 25% (elke 4e era)\nunique_eras = [row.era for row in train_spark.select(\"era\").distinct().collect()]\nsampled_eras = unique_eras[::4]\ntrain_spark = train_spark.filter(col(\"era\").isin(sampled_eras))\n\n# Bekijk de data\nprint(f\"Training data count: {train_spark.count()}\")\nprint(f\"Number of features: {len(features)}\")\nprint(f\"Number of eras: {len(sampled_eras)}\")\n\n# Toon schema\ntrain_spark.printSchema()\n'''","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-10T15:29:33.491Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data voorbereiden met PySpark","metadata":{}},{"cell_type":"code","source":"'''\n# Bereid data voor met Spark ML Pipeline\nprint(\"Preparing feature vector with Spark...\")\n\n# Maak een feature vector van alle features\nassembler = VectorAssembler(inputCols=features, outputCol=\"features\")\ntrain_spark = assembler.transform(train_spark)\n\n# Toon een voorbeeld van de getransformeerde data\ntrain_spark.select(\"era\", \"features\", \"target\").show(5, truncate=True)\n'''","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-10T15:29:33.492Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Converteren van Spark DataFrame naar H2O Frame","metadata":{}},{"cell_type":"code","source":"'''\n# Converteer Spark DataFrame naar H2O Frame\nprint(\"Converting Spark DataFrame to H2O Frame...\")\ntrain_h2o = h2o_context.asH2OFrame(train_spark)\n\n# Bekijk H2O Frame info\ntrain_h2o.describe()\n'''","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-10T15:29:33.492Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature engineering","metadata":{}},{"cell_type":"code","source":"# using standard Feature Engineering from here: https://www.kaggle.com/code/lucasmorin/crypto-forecasting-lgbm-feval-feature-importance\n# https://stackoverflow.com/questions/38641691/weighted-correlation-coefficient-with-pandas\ndef wmean(x, w):\n    return np.sum(x * w) / np.sum(w)\n\ndef wcov(x, y, w):\n    return np.sum(w * (x - wmean(x, w)) * (y - wmean(y, w))) / np.sum(w)\n\ndef wcorr(x, y, w):\n    return wcov(x, y, w) / np.sqrt(wcov(x, x, w) * wcov(y, y, w))\n\ndef eval_wcorr(preds, train_data):\n    w = train_data.add_w.values.flatten()\n    y_true = train_data.get_label()\n    return 'eval_wcorr', wcorr(preds, y_true, w), True\n\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-10T15:46:41.625Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Numerai crypto voorbeeld model","metadata":{}},{"cell_type":"code","source":"%%time\ndef generate_training_features(df: pd.DataFrame) -> List[str]:\n    # TODO: Get your data and create features\n    df['fake_feature_1'] = df.groupby([\"symbol\", \"date\"])['symbol'].transform(lambda x: random.uniform(0, 1))\n    return ['fake_feature_1']\n\n# Historical targets file contains [\"symbol\", \"date\", \"target\"] columns\n#train_df\n\n# Add training features for each (symbol, date)\nfeature_cols = generate_training_features(train_df)\n\nmodel = lgb.LGBMRegressor(\n    n_estimators=2000,\n    learning_rate=0.01,\n    max_depth=5,\n    num_leaves=2 ** 5,\n    colsample_bytree=0.1\n)\n\nmodel.fit(\n    train_df[feature_cols],\n    train_df[\"target\"]\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_features(df: pd.DataFrame):\n    # TODO: Get your data and create features for live universe\n    df['fake_feature_1'] = df['symbol'].transform(lambda x: random.uniform(0, 1))\n\n# Use API keys to authenticate\nnapi = NumerAPI(\"[your api public id]\", \"[your api secret key]\")\n\n# Generate features for the live universe\ngenerate_features(live)\n\n# Get live predictions\nlive[\"signal\"] = model.predict(live[feature_cols])\n\n# Predictions must be between 0 and 1\nlive[\"signal\"] = live[\"signal\"].rank(pct=True)\n\n# Format and save submission\nlive[['symbol', 'signal']].to_parquet(\"submission.parquet\")\n\n# Get model ids and submit models\nmodels = napi.get_models(tournament=12)\nfor model_name, model_id in models.items():\n    print(f'submitting {model_name}...')\n    napi.upload_predictions(\"submission.parquet\", model_id=model_id, tournament=12)\n\nprint('done!')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Claude.ai versie van model met behulp van GPU","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nimport random\nfrom typing import List\nimport time\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\n\n# Controleer of GPU beschikbaar is\ndef check_gpu_availability():\n    try:\n        # Voor Kaggle P100\n        !nvidia-smi\n        print(\"GPU is beschikbaar\")\n        return True\n    except:\n        print(\"GPU niet gevonden, fall back naar CPU\")\n        return False\n\n# Verbeterde versie van feature generatie functie\ndef generate_training_features(df: pd.DataFrame) -> List[str]:\n    \"\"\"\n    Genereert features voor het trainingsmodel met betere prestaties.\n    \n    Args:\n        df: DataFrame met minimaal 'symbol' en 'date' kolommen\n    \n    Returns:\n        List met namen van gegenereerde feature kolommen\n    \"\"\"\n    # Start timer voor benchmarking\n    start_time = time.time()\n    \n    # Lijst om feature namen bij te houden\n    feature_cols = []\n    \n    # Sorteer de data op symbol en date - belangrijk voor tijdreekseigenschappen\n    df = df.sort_values(['symbol', 'date'])\n    \n    # Basis statistieken per symbol\n    print(\"Berekenen groepsstatistieken...\")\n    \n    # Meer betekenisvolle features genereren (voorbeeld)\n    # Voor een crypto competitie zouden we features toe kunnen voegen zoals:\n    \n    # 1. Mean encoding van symbol om rekening te houden met crypto-specifieke eigenschappen\n    df['symbol_mean_target'] = df.groupby('symbol')['target'].transform('mean')\n    feature_cols.append('symbol_mean_target')\n    \n    # 2. Tijdsdimensie features\n    if 'date' in df.columns:\n        df['date'] = pd.to_datetime(df['date'])\n        df['day_of_week'] = df['date'].dt.dayofweek\n        df['month'] = df['date'].dt.month\n        df['quarter'] = df['date'].dt.quarter\n        feature_cols.extend(['day_of_week', 'month', 'quarter'])\n    \n    # 3. Rolling statistieken (als we aanvullende prijs/volume data zouden hebben)\n    # Als voorbeeld, simuleren we hier wat prijsdata\n    if 'fake_price' not in df.columns:\n        df['fake_price'] = np.random.normal(100, 10, size=len(df))\n    \n    # Bereken rolling statistieken met window size 7\n    for window in [7, 14, 30]:\n        # Rolling mean\n        df[f'price_rolling_mean_{window}'] = df.groupby('symbol')['fake_price'].transform(\n            lambda x: x.rolling(window=window, min_periods=1).mean())\n        \n        # Rolling volatility (std)\n        df[f'price_rolling_std_{window}'] = df.groupby('symbol')['fake_price'].transform(\n            lambda x: x.rolling(window=window, min_periods=1).std())\n        \n        # Momentum (% verandering)\n        df[f'price_momentum_{window}'] = df.groupby('symbol')['fake_price'].transform(\n            lambda x: x.pct_change(periods=window).fillna(0))\n        \n        feature_cols.extend([\n            f'price_rolling_mean_{window}',\n            f'price_rolling_std_{window}',\n            f'price_momentum_{window}'\n        ])\n    \n    # 4. Kruisende moving averages (technische indicators)\n    df['sma_short'] = df.groupby('symbol')['fake_price'].transform(\n        lambda x: x.rolling(window=7, min_periods=1).mean())\n    df['sma_long'] = df.groupby('symbol')['fake_price'].transform(\n        lambda x: x.rolling(window=21, min_periods=1).mean())\n    df['sma_cross'] = (df['sma_short'] > df['sma_long']).astype(int)\n    feature_cols.append('sma_cross')\n    \n    # Random noise feature (als placeholder)\n    df['random_feature'] = np.random.normal(0, 1, size=len(df))\n    feature_cols.append('random_feature')\n    \n    # Log execution time\n    end_time = time.time()\n    print(f\"Feature generatie voltooid in {end_time - start_time:.2f} seconden\")\n    print(f\"Gegenereerde features: {len(feature_cols)}\")\n    \n    return feature_cols\n\n# Train een GPU-versneld LightGBM model\ndef train_lgbm_model(train_df, feature_cols, target_col=\"target\", use_gpu=False):\n    \"\"\"\n    Traint een LightGBM model met GPU acceleratie indien beschikbaar\n    \n    Args:\n        train_df: DataFrame met trainingsdata\n        feature_cols: Lijst met feature kolommen\n        target_col: Naam van de target kolom\n        use_gpu: Boolean om GPU te gebruiken\n    \n    Returns:\n        Getraind LightGBM model\n    \"\"\"\n    print(f\"Training model op {'GPU' if use_gpu else 'CPU'}...\")\n    start_time = time.time()\n    \n    # Bereken optimale parameters op basis van dataset grootte\n    num_samples = len(train_df)\n    num_features = len(feature_cols)\n    \n    # Pas hyperparameters aan voor GPU training\n    params = {\n        'objective': 'regression',\n        'metric': 'rmse',\n        'n_estimators': 2000,\n        'learning_rate': 0.01,\n        'max_depth': 5,\n        'num_leaves': 2**5,\n        'colsample_bytree': 0.1,\n        'verbosity': -1,\n        'early_stopping_rounds': 50\n    }\n    \n    # GPU-specifieke parameters toevoegen indien nodig\n    if use_gpu:\n        params.update({\n            'device': 'gpu',\n            'gpu_platform_id': 0,\n            'gpu_device_id': 0,\n            'use_gpu_hist': True,\n            'gpu_use_dp': True  # Gebruik dubbele precisie voor betere nauwkeurigheid\n        })\n    \n    # Split data voor early stopping\n    from sklearn.model_selection import train_test_split\n    X_train, X_val, y_train, y_val = train_test_split(\n        train_df[feature_cols], \n        train_df[target_col],\n        test_size=0.2,\n        random_state=42\n    )\n    \n    # Maak LGBMRegressor met aangepaste parameters\n    model = lgb.LGBMRegressor(**params)\n    \n    # Fit het model met evaluation set\n    model.fit(\n        X_train, y_train,\n        eval_set=[(X_val, y_val)],\n        verbose=100  # Toon training voortgang elke 100 iteraties\n    )\n    \n    # Evalueer het model\n    val_preds = model.predict(X_val)\n    rmse = mean_squared_error(y_val, val_preds, squared=False)\n    print(f\"Validatie RMSE: {rmse:.6f}\")\n    \n    # Check vroeg stoppen\n    print(f\"Model stopte na {model.best_iteration_} iteraties\")\n    \n    # Print feature importance\n    feature_importance = pd.DataFrame({\n        'Feature': feature_cols,\n        'Importance': model.feature_importances_\n    }).sort_values('Importance', ascending=False)\n    \n    print(\"\\nTop 10 belangrijkste features:\")\n    print(feature_importance.head(10))\n    \n    # Plot feature importance\n    plt.figure(figsize=(10, 6))\n    plt.barh(feature_importance['Feature'][:15], feature_importance['Importance'][:15])\n    plt.xlabel('Importance')\n    plt.title('Feature Importance (Top 15)')\n    plt.gca().invert_yaxis()\n    plt.show()\n    \n    end_time = time.time()\n    print(f\"Model training voltooid in {end_time - start_time:.2f} seconden\")\n    \n    return model\n\n# Hoofdprogramma\nif __name__ == \"__main__\":\n    # Controleer GPU beschikbaarheid\n    use_gpu = check_gpu_availability()\n    \n    # Laad trainingsdata (vervang dit met je echte data loading logica)\n    print(\"Laden van trainingsdata...\")\n    \n    # Voorbeeld: als je trainingsdata uit een lokaal bestand laadt\n    # train_df = pd.read_csv('/path/to/train.csv')\n    \n    # Voor demonstratie, maken we synthetische data\n    symbols = ['BTC', 'ETH', 'XRP', 'ADA', 'SOL', 'DOT', 'AVAX', 'MATIC']\n    dates = pd.date_range(start='2020-01-01', end='2023-01-01', freq='D')\n    \n    data = []\n    for symbol in symbols:\n        for date in dates:\n            data.append({\n                'symbol': symbol,\n                'date': date,\n                'target': np.random.normal(0, 1)  # random target waarde\n            })\n    \n    train_df = pd.DataFrame(data)\n    print(f\"Trainingsdata geladen: {train_df.shape}\")\n    \n    # Genereer features\n    feature_cols = generate_training_features(train_df)\n    \n    # Train model met GPU indien beschikbaar\n    model = train_lgbm_model(train_df, feature_cols, use_gpu=use_gpu)\n    \n    # Sla model op\n    import pickle\n    with open('numerai_crypto_model.pkl', 'wb') as f:\n        pickle.dump(model, f)\n    \n    print(\"Model opgeslagen als 'numerai_crypto_model.pkl'\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluation and model flow nfold lgbm","metadata":{}},{"cell_type":"code","source":"n_fold = 5\n\nimportances = []\n\nfor fold in range(n_fold):\n    print('Fold: '+str(fold))\n\n    train = pd.read_parquet('../input/crypto-forecasting-static-feature-engineering/train_fold_'+str(fold)+'.parquet')\n    test = pd.read_parquet('../input/crypto-forecasting-static-feature-engineering/test_fold_'+str(fold)+'.parquet')\n    \n    if DEBUG:\n        timestamp_sample_train = train.timestamp.unique()[:np.int(len(train.timestamp.unique())*0.05)]\n        timestamp_sample_test = test.timestamp.unique()[:np.int(len(test.timestamp.unique())*0.05)]\n        train = train[train.timestamp.isin(timestamp_sample_train)]\n        test = test[test.timestamp.isin(timestamp_sample_test)]\n\n    y_train = train['Target']\n    y_test = test['Target']\n\n    features = [col for col in train.columns if col not in {'timestamp', 'Target', 'Target_M','weights'}]\n\n    weights_train = train[['weights']]\n    weights_test = test[['weights']]\n\n    train = train[features]\n    test = test[features]\n    \n    train_dataset = lgb.Dataset(train, y_train, feature_name = features, categorical_feature= ['Asset_ID'])\n    val_dataset = lgb.Dataset(test, y_test, feature_name = features, categorical_feature= ['Asset_ID'])\n\n    train_dataset.add_w = weights_train\n    val_dataset.add_w = weights_test\n\n    val_data = test\n    val_y = y_test\n\n    del train\n    \n    evals_result = {}\n    \n    # parameters\n    params = {'n_estimators': 2000,\n            'objective': 'regression',\n            'metric': 'None',\n            'boosting_type': 'gbdt',\n            'max_depth': -1,\n            'learning_rate': 0.05,\n            'subsample': 0.72,\n            'subsample_freq': 4,\n            'feature_fraction': 0.4,\n            'lambda_l1': 1,\n            'lambda_l2': 1,\n            'seed': 46,\n            'verbose': -1,\n            }\n\n    model = lgb.train(params = params,\n                      train_set = train_dataset, \n                      valid_sets = [val_dataset],\n                      #early_stopping_rounds=1000,\n                      verbose_eval = 100,\n                      feval=eval_wcorr,\n                      evals_result = evals_result \n                     )\n    \n    importances.append(model.feature_importance(importance_type='gain'))\n    \n    plt.plot(np.array(evals_result['valid_0']['eval_wcorr']), label='fold '+str(fold))\n    \nplt.legend(loc=\"upper left\")\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-10T15:29:33.492Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model trainen met H2O XGBoost via Sparkling Water","metadata":{}},{"cell_type":"code","source":"# Train model met H2O XGBoost via Sparkling Water\nprint(\"Training H2O XGBoost model via Sparkling Water...\")\nstart_time = time.time()\n\n# Configureer XGBoost model\nfrom h2o.estimators.xgboost import H2OXGBoostEstimator\n\nxgb_model = H2OXGBoostEstimator(\n    ntrees=2000,\n    max_depth=5,\n    learn_rate=0.01,\n    sample_rate=0.8,\n    col_sample_rate=0.8,\n    tree_method=\"auto\",  # auto selecteert GPU indien beschikbaar\n    booster=\"gbtree\",\n    seed=42\n)\n\n# Train het model\nxgb_model.train(x=features, y=\"target\", training_frame=train_h2o)\n\ntraining_time = time.time() - start_time\nprint(f\"Training completed in {training_time:.2f} seconds\")\n\n# Toon model informatie\nprint(xgb_model)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-10T15:29:33.492Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature importance visualiseren","metadata":{}},{"cell_type":"code","source":"# Feature importance visualiseren\nfeature_importance = xgb_model.varimp(use_pandas=True)\nif feature_importance is not None:\n    plt.figure(figsize=(10, 8))\n    plt.barh(range(len(feature_importance[:20])), feature_importance[:20]['relative_importance'])\n    plt.yticks(range(len(feature_importance[:20])), feature_importance[:20]['variable'])\n    plt.title('H2O XGBoost Feature Importance (top 20)')\n    plt.xlabel('Relative Importance')\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-10T15:29:33.493Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model opslaan als MOJO","metadata":{}},{"cell_type":"code","source":"# Sla het model op als MOJO (Model Object, Optimized)\nmojo_path = xgb_model.download_mojo(path=\"./\", get_genmodel_jar=True)\nprint(f\"Model saved as MOJO: {mojo_path}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-10T15:29:33.493Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Validatiedata laden en voorbereiden met PySpark","metadata":{}},{"cell_type":"code","source":"# Download validatiedata voor testen\nprint(\"Downloading validation data for testing...\")\nnapi.download_dataset(f\"{DATA_VERSION}/validation.parquet\")\n\n# Laad validatiedata met Spark\nprint(\"Loading validation data with Spark...\")\nvalidation_spark = spark.read.parquet(f\"{DATA_VERSION}/validation.parquet\")\n\n# Selecteer alleen de benodigde kolommen\ncolumns_to_select = [\"era\", \"data_type\"] + features\nvalidation_spark = validation_spark.select(*columns_to_select)\n\n# Filter alleen validatie data\nvalidation_spark = validation_spark.filter(col(\"data_type\") == \"validation\")\n\n# Neem een kleine subset voor geheugenefficiëntie\nvalidation_spark = validation_spark.limit(1000)\n\n# Maak een feature vector van alle features\nvalidation_spark = assembler.transform(validation_spark)\n\n# Converteer Spark DataFrame naar H2O Frame\nvalidation_h2o = h2o_context.asH2OFrame(validation_spark)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-10T15:29:33.493Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Voorspellingen maken met het model","metadata":{}},{"cell_type":"code","source":"# Maak voorspellingen met het model\nprint(\"Making predictions...\")\npredictions_h2o = xgb_model.predict(validation_h2o)\n\n# Converteer H2O Frame terug naar Spark DataFrame\npredictions_spark = h2o_context.asSparkFrame(predictions_h2o)\n\n# Toon voorspellingen\nprint(\"Sample predictions:\")\npredictions_spark.show(5)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-10T15:29:33.494Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Voorspellingsfunctie definiëren","metadata":{}},{"cell_type":"code","source":"# Definieer voorspellingsfunctie die werkt met H2O model\ndef predict(\n    live_features: pd.DataFrame,\n    live_benchmark_models: pd.DataFrame\n) -> pd.DataFrame:\n    # Converteer pandas DataFrame naar Spark DataFrame\n    live_features_spark = spark.createDataFrame(live_features[features])\n    \n    # Maak een feature vector van alle features\n    live_features_spark = assembler.transform(live_features_spark)\n    \n    # Converteer Spark DataFrame naar H2O Frame\n    live_features_h2o = h2o_context.asH2OFrame(live_features_spark)\n    \n    # Maak voorspellingen met het H2O model\n    preds = xgb_model.predict(live_features_h2o)\n    \n    # Converteer H2O voorspellingen terug naar pandas\n    predictions = h2o.as_list(preds)[\"predict\"].values\n    \n    # Maak submission DataFrame\n    submission = pd.Series(predictions, index=live_features.index)\n    return submission.to_frame(\"prediction\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-10T15:29:33.495Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Voorspellingsfunctie testen","metadata":{}},{"cell_type":"code","source":"# Converteer Spark DataFrame terug naar pandas voor testen\nvalidation_pd = validation_spark.toPandas()\n\n# Test voorspellingsfunctie\nprint(\"Testing prediction function...\")\n# Maak een lege DataFrame voor benchmark_models (niet gebruikt in onze voorspellingsfunctie)\nempty_benchmark = pd.DataFrame(index=validation_pd.index)\npredictions = predict(validation_pd, empty_benchmark)\n\nprint(f\"Predictions shape: {predictions.shape}\")\nprint(\"\\nSample predictions:\")\nprint(predictions.head())","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-10T15:29:33.495Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Voorspellingsfunctie opslaan met cloudpickle","metadata":{}},{"cell_type":"code","source":"# Pickle voorspellingsfunctie\nprint(\"Saving prediction function with cloudpickle...\")\np = cloudpickle.dumps(predict)\nwith open(\"numerai_sparkling_water_model.pkl\", \"wb\") as f:\n    f.write(p)\n\nprint(\"Prediction function saved as 'numerai_sparkling_water_model.pkl'\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-10T15:29:33.496Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Kaggle specifieke functies voor het opslaan van resultaten","metadata":{}},{"cell_type":"code","source":"# Opslaan van resultaten in Kaggle output\n# Dit maakt het mogelijk om de resultaten te downloaden of als dataset te gebruiken\ntry:\n    # Maak een output directory\n    !mkdir -p /kaggle/working/output\n    \n    # Kopieer de belangrijke bestanden\n    !cp numerai_sparkling_water_model.pkl /kaggle/working/output/\n    !cp {mojo_path} /kaggle/working/output/\n    \n    print(\"Model bestanden opgeslagen in Kaggle output directory\")\nexcept Exception as e:\n    print(f\"Fout bij opslaan in Kaggle output: {e}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-10T15:29:33.496Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Voordelen van Sparkling Water","metadata":{}},{"cell_type":"code","source":"# Hier zou je een vergelijking kunnen maken tussen standaard H2O en Sparkling Water\nprint(\"Sparkling Water Voordelen:\")\nprint(\"1. Gedistribueerde verwerking met Spark voor grote datasets\")\nprint(\"2. Combinatie van Spark's data processing met H2O's machine learning algoritmes\")\nprint(\"3. Betere schaalbaarheid voor complexe modellen en grote datasets\")\nprint(\"4. Mogelijkheid om Spark ML Pipeline te integreren met H2O modellen\")\nprint(f\"5. Onze training duurde {training_time:.2f} seconden met Sparkling Water\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-10T15:29:33.496Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Sluit H2O cluster af\nh2o.cluster().shutdown()\n\n# Sluit Spark sessie af\nspark.stop()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-10T15:29:33.496Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Financial Modeling Prep API Integration\nimport requests\nimport pandas as pd\n\nFMP_API_KEY = \"aDFEO9rxgvGL3VQgPcBxXblSZ3laRLap\"\nDEEPSEEK_API_KEY = \"sk-6a3502649b0048259e0009a328c71960\"\n\n# Function to get economic indicators from Financial Modeling Prep\ndef get_economic_indicators():\n    url = f\"https://financialmodelingprep.com/api/v3/economic/economic_indicators?apikey={FMP_API_KEY}\"\n    response = requests.get(url)\n    data = response.json()\n    return pd.DataFrame(data)\n\n# Get country and currency data\ndef get_country_currency_data():\n    url = f\"https://financialmodelingprep.com/api/v3/fx?apikey={FMP_API_KEY}\"\n    response = requests.get(url)\n    fx_data = response.json()\n    \n    # Get country profiles for ISO codes\n    url = f\"https://financialmodelingprep.com/api/v4/country_list?apikey={FMP_API_KEY}\"\n    response = requests.get(url)\n    country_data = response.json()\n    \n    # Create comprehensive country-currency mapping\n    country_df = pd.DataFrame(country_data)\n    fx_df = pd.DataFrame(fx_data)\n    \n    # Extract currency codes from FX pairs\n    currency_codes = set()\n    for pair in fx_df[\"ticker\"].values:\n        if \"/\" in pair:\n            base, quote = pair.split(\"/\")\n            currency_codes.add(base)\n            currency_codes.add(quote)\n    \n    # Create final mapping dataframe\n    mapping_data = []\n    for country in country_df.to_dict(\"records\"):\n        country_name = country.get(\"name\", \"\")\n        country_code = country.get(\"code\", \"\")\n        currency_name = country.get(\"currency\", \"\")\n        currency_code = \"\"\n        \n        # Try to find currency code\n        for code in currency_codes:\n            if len(code) == 3 and code.upper() in currency_name.upper():\n                currency_code = code\n                break\n        \n        mapping_data.append({\n            \"country_name\": country_name,\n            \"country_code\": country_code,\n            \"currency_name\": currency_name,\n            \"currency_code\": currency_code\n        })\n    \n    return pd.DataFrame(mapping_data)\n\n# Get economic indicators\ntry:\n    economic_indicators = get_economic_indicators()\n    print(\"Economic Indicators:\")\n    print(economic_indicators.head())\nexcept Exception as e:\n    print(f\"Error fetching economic indicators: {e}\")\n\n# Get country-currency mapping\ntry:\n    country_currency_mapping = get_country_currency_data()\n    print(\"\nCountry-Currency Mapping:\")\n    print(country_currency_mapping.head(20))\n    \n    # Save the mapping to CSV\n    country_currency_mapping.to_csv(\"country_currency_mapping.csv\", index=False)\n    print(\"\nSaved country-currency mapping to CSV file\")\nexcept Exception as e:\n    print(f\"Error creating country-currency mapping: {e}\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-10T15:29:33.496Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# DeepSeek API Integration for Crypto-Country Association\nimport requests\nimport json\n\ndef get_crypto_country_associations(cryptocurrencies):\n    url = \"https://api.deepseek.com/v1/chat/completions\"\n    \n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"Authorization\": f\"Bearer {DEEPSEEK_API_KEY}\"\n    }\n    \n    crypto_list = \", \".join(cryptocurrencies)\n    \n    data = {\n        \"model\": \"deepseek-chat\",\n        \"messages\": [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful assistant that provides accurate information about cryptocurrencies.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"For each of these cryptocurrencies: {crypto_list}, provide the country where they have their entity registered or where they primarily report taxes. Return the data in JSON format with cryptocurrency name, country name, and ISO country code.\"\n            }\n        ],\n        \"temperature\": 0.1,\n        \"max_tokens\": 2000\n    }\n    \n    try:\n        response = requests.post(url, headers=headers, json=data)\n        response_data = response.json()\n        \n        if \"choices\" in response_data and len(response_data[\"choices\"]) > 0:\n            content = response_data[\"choices\"][0][\"message\"][\"content\"]\n            \n            # Extract JSON from the response\n            try:\n                # Try to find JSON in the response\n                start_idx = content.find(\"{\")\n                end_idx = content.rfind(\"}\")\n                \n                if start_idx != -1 and end_idx != -1:\n                    json_str = content[start_idx:end_idx+1]\n                    return json.loads(json_str)\n                else:\n                    return {\"error\": \"No JSON found in response\", \"raw_response\": content}\n            except json.JSONDecodeError:\n                return {\"error\": \"Failed to parse JSON\", \"raw_response\": content}\n        else:\n            return {\"error\": \"No response from DeepSeek API\"}\n    except Exception as e:\n        return {\"error\": str(e)}\n\n# Example usage\ncryptocurrencies = [\"Bitcoin\", \"Ethereum\", \"Ripple\", \"Cardano\", \"Solana\"]\ntry:\n    crypto_country_data = get_crypto_country_associations(cryptocurrencies)\n    print(\"Cryptocurrency Country Associations:\")\n    print(json.dumps(crypto_country_data, indent=2))\n    \n    # Convert to DataFrame and save\n    if not isinstance(crypto_country_data, dict) or not crypto_country_data.get(\"error\"):\n        crypto_df = pd.DataFrame(crypto_country_data)\n        crypto_df.to_csv(\"crypto_country_associations.csv\", index=False)\n        print(\"\nSaved cryptocurrency country associations to CSV file\")\nexcept Exception as e:\n    print(f\"Error getting cryptocurrency country associations: {e}\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-10T15:29:33.497Z"}},"outputs":[],"execution_count":null}]}