{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerai Crypto Competitie Voorspellingsmodel met PySpark Preprocessing\n",
    "\n",
    "Dit notebook implementeert een voorspellingsmodel voor de Numerai/Numerai Crypto competitie met behulp van PySpark voor data preprocessing en XGBoost voor model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installatie van benodigde packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Mount Google Drive for persistent storage\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# Create directory for Numerai data and models\n",
    "!mkdir -p \"/content/drive/My Drive/Numerai_Crypto\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check if GPU is available\n",
    "!nvidia-smi"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Installeer Java (vereist voor Spark)\n",
    "!apt-get update -qq\n",
    "!apt-get install -y default-jre > /dev/null\n",
    "!java -version\n",
    "\n",
    "# Installeer Spark en PySpark\n",
    "!pip install -q pyspark==3.1.2\n",
    "\n",
    "# Installeer XGBoost\n",
    "!pip install -q xgboost==1.5.0\n",
    "\n",
    "# Installeer andere benodigde packages\n",
    "!pip install -q numerapi pandas cloudpickle==2.2.1 pyarrow scikit-learn scipy==1.10.1 matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importeren van benodigde libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from numerapi import NumerAPI\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Spark imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DoubleType, ArrayType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# XGBoost imports\n",
    "import xgboost as xgb\n",
    "import cloudpickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialiseren van Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialiseer Spark sessie\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NumeraiPySparkPreprocessing\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Print Spark versie informatie\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialiseren van de Numerai API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialiseer de Numerai API client\n",
    "# Voor het indienen van voorspellingen zijn API keys nodig\n",
    "# napi = NumerAPI(public_id=\"UW_PUBLIC_ID\", secret_key=\"UW_SECRET_KEY\")\n",
    "napi = NumerAPI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data downloaden en laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Gebruik een van de nieuwste dataversies\n",
    "DATA_VERSION = \"v5.0\"\n",
    "\n",
    "# Maak een data directory\n",
    "!mkdir -p {DATA_VERSION}\n",
    "\n",
    "# Download data\n",
    "print(\"Downloading training data...\")\n",
    "napi.download_dataset(f\"{DATA_VERSION}/train.parquet\")\n",
    "napi.download_dataset(f\"{DATA_VERSION}/features.json\")\n",
    "\n",
    "# Laad feature metadata\n",
    "feature_metadata = json.load(open(f\"{DATA_VERSION}/features.json\"))\n",
    "print(\"Available feature sets:\", list(feature_metadata[\"feature_sets\"].keys()))\n",
    "features = feature_metadata[\"feature_sets\"][\"small\"]  # gebruik \"small\" voor sneller testen, \"medium\" of \"all\" voor betere prestaties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data laden met PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Laad trainingsdata met Spark\n",
    "print(\"Loading training data with Spark...\")\n",
    "train_spark = spark.read.parquet(f\"{DATA_VERSION}/train.parquet\")\n",
    "\n",
    "# Selecteer alleen de benodigde kolommen\n",
    "columns_to_select = [\"era\"] + features + [\"target\"]\n",
    "train_spark = train_spark.select(*columns_to_select)\n",
    "\n",
    "# Downsampling voor snelheid (optioneel)\n",
    "print(\"Preparing data for training...\")\n",
    "# Haal unieke era's op en sample 25% (elke 4e era)\n",
    "unique_eras = [row.era for row in train_spark.select(\"era\").distinct().collect()]\n",
    "sampled_eras = unique_eras[::4]\n",
    "train_spark = train_spark.filter(col(\"era\").isin(sampled_eras))\n",
    "\n",
    "# Bekijk de data\n",
    "print(f\"Training data count: {train_spark.count()}\")\n",
    "print(f\"Number of features: {len(features)}\")\n",
    "print(f\"Number of eras: {len(sampled_eras)}\")\n",
    "\n",
    "# Toon schema\n",
    "train_spark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data voorbereiden met PySpark ML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Bereid data voor met Spark ML Pipeline\n",
    "print(\"Preparing feature vector with Spark ML Pipeline...\")\n",
    "\n",
    "# Maak een feature vector van alle features\n",
    "assembler = VectorAssembler(inputCols=features, outputCol=\"features_vec\")\n",
    "\n",
    "# Standaardiseer de features (optioneel)\n",
    "scaler = StandardScaler(inputCol=\"features_vec\", outputCol=\"features_scaled\", withStd=True, withMean=True)\n",
    "\n",
    "# Maak een pipeline\n",
    "pipeline = Pipeline(stages=[assembler, scaler])\n",
    "\n",
    "# Fit de pipeline op de trainingsdata\n",
    "pipeline_model = pipeline.fit(train_spark)\n",
    "\n",
    "# Transformeer de data\n",
    "train_prepared = pipeline_model.transform(train_spark)\n",
    "\n",
    "# Toon een voorbeeld van de getransformeerde data\n",
    "train_prepared.select(\"era\", \"features_scaled\", \"target\").show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converteren van Spark DataFrame naar pandas voor XGBoost training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Functie om Spark Vector naar numpy array te converteren\n",
    "@udf(returnType=ArrayType(DoubleType()))\n",
    "def vector_to_array(vector):\n",
    "    return vector.toArray().tolist()\n",
    "\n",
    "# Converteer Spark Vector naar array kolom\n",
    "train_prepared = train_prepared.withColumn(\"features_array\", vector_to_array(\"features_scaled\"))\n",
    "\n",
    "# Selecteer alleen de benodigde kolommen voor training\n",
    "train_for_xgb = train_prepared.select(\"features_array\", \"target\")\n",
    "\n",
    "# Converteer Spark DataFrame naar pandas\n",
    "print(\"Converting Spark DataFrame to pandas for XGBoost training...\")\n",
    "train_pd = train_for_xgb.toPandas()\n",
    "\n",
    "# Converteer features_array kolom naar numpy arrays\n",
    "X_train = np.stack(train_pd[\"features_array\"].values)\n",
    "y_train = train_pd[\"target\"].values\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model trainen met XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train model met XGBoost\n",
    "print(\"Training XGBoost model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Converteer data naar DMatrix formaat\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "\n",
    "# Configureer XGBoost parameters\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse',\n",
    "    'max_depth': 5,\n",
    "    'eta': 0.01,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'tree_method': 'auto',  # auto selecteert GPU indien beschikbaar\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Train het model\n",
    "num_rounds = 2000\n",
    "xgb_model = xgb.train(params, dtrain, num_rounds)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Training completed in {training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance visualiseren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Feature importance visualiseren\n",
    "feature_importance = xgb_model.get_score(importance_type='gain')\n",
    "if feature_importance:\n",
    "    # Sorteer op importance\n",
    "    sorted_importance = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "    features_names = [item[0] for item in sorted_importance[:20]]\n",
    "    importance_values = [item[1] for item in sorted_importance[:20]]\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.barh(range(len(features_names)), importance_values)\n",
    "    plt.yticks(range(len(features_names)), features_names)\n",
    "    plt.title('XGBoost Feature Importance (top 20)')\n",
    "    plt.xlabel('Importance (gain)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model opslaan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Sla het model op\n",
    "model_path = \"xgb_model.json\"\n",
    "xgb_model.save_model(model_path)\n",
    "print(f\"Model saved as: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validatiedata laden en voorbereiden met PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Download validatiedata voor testen\n",
    "print(\"Downloading validation data for testing...\")\n",
    "napi.download_dataset(f\"{DATA_VERSION}/validation.parquet\")\n",
    "\n",
    "# Laad validatiedata met Spark\n",
    "print(\"Loading validation data with Spark...\")\n",
    "validation_spark = spark.read.parquet(f\"{DATA_VERSION}/validation.parquet\")\n",
    "\n",
    "# Selecteer alleen de benodigde kolommen\n",
    "columns_to_select = [\"era\", \"data_type\"] + features\n",
    "validation_spark = validation_spark.select(*columns_to_select)\n",
    "\n",
    "# Filter alleen validatie data\n",
    "validation_spark = validation_spark.filter(col(\"data_type\") == \"validation\")\n",
    "\n",
    "# Neem een kleine subset voor geheugeneffici\u00ebntie\n",
    "validation_spark = validation_spark.limit(1000)\n",
    "\n",
    "# Transformeer de data met dezelfde pipeline\n",
    "validation_prepared = pipeline_model.transform(validation_spark)\n",
    "\n",
    "# Converteer Spark Vector naar array kolom\n",
    "validation_prepared = validation_prepared.withColumn(\"features_array\", vector_to_array(\"features_scaled\"))\n",
    "\n",
    "# Selecteer alleen de benodigde kolommen voor voorspelling\n",
    "validation_for_xgb = validation_prepared.select(\"features_array\")\n",
    "\n",
    "# Converteer Spark DataFrame naar pandas\n",
    "validation_pd = validation_for_xgb.toPandas()\n",
    "\n",
    "# Converteer features_array kolom naar numpy arrays\n",
    "X_validation = np.stack(validation_pd[\"features_array\"].values)\n",
    "\n",
    "# Bewaar de originele validatie data voor het maken van de submission\n",
    "validation_original = validation_spark.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voorspellingen maken met het model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Maak voorspellingen met het model\n",
    "print(\"Making predictions...\")\n",
    "dvalidation = xgb.DMatrix(X_validation)\n",
    "predictions = xgb_model.predict(dvalidation)\n",
    "\n",
    "# Toon voorspellingen\n",
    "print(\"Sample predictions:\")\n",
    "print(predictions[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voorspellingsfunctie defini\u00ebren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Definieer voorspellingsfunctie die werkt met PySpark preprocessing en XGBoost\n",
    "def predict(\n",
    "    live_features: pd.DataFrame,\n",
    "    live_benchmark_models: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    # Converteer pandas DataFrame naar Spark DataFrame\n",
    "    live_features_spark = spark.createDataFrame(live_features[features])\n",
    "    \n",
    "    # Transformeer de data met dezelfde pipeline\n",
    "    live_features_prepared = pipeline_model.transform(live_features_spark)\n",
    "    \n",
    "    # Converteer Spark Vector naar array kolom\n",
    "    live_features_prepared = live_features_prepared.withColumn(\"features_array\", vector_to_array(\"features_scaled\"))\n",
    "    \n",
    "    # Converteer Spark DataFrame naar pandas\n",
    "    live_features_pd = live_features_prepared.select(\"features_array\").toPandas()\n",
    "    \n",
    "    # Converteer features_array kolom naar numpy arrays\n",
    "    X_live = np.stack(live_features_pd[\"features_array\"].values)\n",
    "    \n",
    "    # Maak voorspellingen met het XGBoost model\n",
    "    dlive = xgb.DMatrix(X_live)\n",
    "    predictions = xgb_model.predict(dlive)\n",
    "    \n",
    "    # Maak submission DataFrame\n",
    "    submission = pd.Series(predictions, index=live_features.index)\n",
    "    return submission.to_frame(\"prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voorspellingsfunctie testen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test voorspellingsfunctie\n",
    "print(\"Testing prediction function...\")\n",
    "# Maak een lege DataFrame voor benchmark_models (niet gebruikt in onze voorspellingsfunctie)\n",
    "empty_benchmark = pd.DataFrame(index=validation_original.index)\n",
    "predictions_df = predict(validation_original, empty_benchmark)\n",
    "\n",
    "print(f\"Predictions shape: {predictions_df.shape}\")\n",
    "print(\"\\nSample predictions:\")\n",
    "print(predictions_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voorspellingsfunctie opslaan met cloudpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Pickle voorspellingsfunctie\n",
    "print(\"Saving prediction function with cloudpickle...\")\n",
    "p = cloudpickle.dumps(predict)\n",
    "with open(\"numerai_pyspark_xgb_model.pkl\", \"wb\") as f:\n",
    "    f.write(p)\n",
    "\n",
    "print(\"Prediction function saved as 'numerai_pyspark_xgb_model.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle specifieke functies voor het opslaan van resultaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Opslaan van resultaten in Kaggle output\n",
    "# Dit maakt het mogelijk om de resultaten te downloaden of als dataset te gebruiken\n",
    "try:\n",
    "    # Maak een output directory\n",
    "    !mkdir -p /kaggle/working/output\n",
    "    \n",
    "    # Kopieer de belangrijke bestanden\n",
    "    !cp numerai_pyspark_xgb_model.pkl /kaggle/working/output/\n",
    "    !cp {model_path} /kaggle/working/output/\n",
    "    \n",
    "    # Sla de pipeline op\n",
    "    pipeline_path = \"/kaggle/working/output/pipeline_model\"\n",
    "    pipeline_model.save(pipeline_path)\n",
    "    \n",
    "    print(\"Model bestanden opgeslagen in Kaggle output directory\")\n",
    "except Exception as e:\n",
    "    print(f\"Fout bij opslaan in Kaggle output: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voordelen van PySpark Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Hier zou je een vergelijking kunnen maken tussen standaard preprocessing en PySpark\n",
    "print(\"PySpark Preprocessing Voordelen:\")\n",
    "print(\"1. Gedistribueerde data verwerking voor grote datasets\")\n",
    "print(\"2. Effici\u00ebnte feature engineering met Spark ML Pipeline\")\n",
    "print(\"3. Betere schaalbaarheid voor complexe transformaties\")\n",
    "print(\"4. Mogelijkheid om data te verwerken die niet in geheugen past\")\n",
    "print(\"5. Integratie met verschillende data bronnen en formaten\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Afsluiten van Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Sluit Spark sessie af\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}