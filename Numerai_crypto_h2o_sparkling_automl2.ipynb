{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/develuse/numerai-crypto-h2o-sw-automl2?scriptVersionId=233140655\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install -q numerapi requests pyarrow fastparquet pydrive2\n# Installeer Java (vereist voor H2O en Spark)\n!apt-get update -qq\n!apt-get install -y default-jre > /dev/null\n!java -version\n\n# Installeer Spark en PySpark\n!pip install -q pyspark==3.1.2\n\n# Installeer H2O Sparkling Water\n!pip install -q h2o-pysparkling-3.1\n\n# Installeer andere benodigde packages\n# Gebruik scikit-learn 1.0.2 voor compatibiliteit, zonder waarschuwingen weer te geven\n!pip install -q numerapi pandas h2o cloudpickle==2.2.1 scikit-learn==1.0.2 scipy==1.10.1 matplotlib xgboost==1.6.2 --no-deps\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:33:09.402772Z","iopub.execute_input":"2025-04-10T16:33:09.405259Z","iopub.status.idle":"2025-04-10T16:33:34.34511Z","shell.execute_reply.started":"2025-04-10T16:33:09.405172Z","shell.execute_reply":"2025-04-10T16:33:34.343419Z"}},"outputs":[{"name":"stdout","text":"W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\nopenjdk version \"11.0.26\" 2025-01-21\nOpenJDK Runtime Environment (build 11.0.26+4-post-Ubuntu-1ubuntu122.04)\nOpenJDK 64-Bit Server VM (build 11.0.26+4-post-Ubuntu-1ubuntu122.04, mixed mode, sharing)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Numerai Crypto Competitie Voorspellingsmodel met H2O Sparkling Water\n\nDit notebook implementeert een voorspellingsmodel voor de Numerai/Numerai Crypto competitie met behulp van H2O Sparkling Water, wat H2O integreert met Apache Spark voor gedistribueerde verwerking.","metadata":{}},{"cell_type":"markdown","source":"## Installatie van benodigde packages\n\nEerst moeten we Java, Spark en H2O Sparkling Water installeren. Dit kan enige tijd duren.","metadata":{}},{"cell_type":"code","source":"# Importeer benodigde bibliotheken\n# Numerapi imports\nfrom numerapi import NumerAPI, CryptoAPI\n\n# Data download en preparatie imports\nimport pandas as pd\nimport json\nimport os\nfrom typing import List\nimport gc\n\n# Berekening imports\nimport numpy as np\nimport time\nimport random\n\n# Spark imports\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.sql.functions import col\n\n# H2O Sparkling Water imports\nfrom pysparkling import H2OContext\nfrom h2o.estimators.xgboost import H2OXGBoostEstimator\nimport h2o\nimport cloudpickle\nfrom datetime import datetime\n\n# Visualization imports\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Model imports\nimport lightgbm as lgb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:33:34.347032Z","iopub.execute_input":"2025-04-10T16:33:34.347415Z","iopub.status.idle":"2025-04-10T16:33:38.223547Z","shell.execute_reply.started":"2025-04-10T16:33:34.347382Z","shell.execute_reply":"2025-04-10T16:33:38.222305Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Initialiseren van Spark en H2O Sparkling Water","metadata":{}},{"cell_type":"code","source":"# Maak een map voor het opslaan van gegevens en modellen\n!mkdir -p /kaggle/working/numerai\n\n# Initialiseer Spark sessie met betere resources (pas aan op basis van je Kaggle-omgeving)\nspark = SparkSession.builder \\\n    .appName(\"NumeraiSparklingWater\") \\\n    .config(\"spark.executor.memory\", \"5g\") \\\n    .config(\"spark.driver.memory\", \"5g\") \\\n    .config(\"spark.executor.cores\", \"2\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n    .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n    .config(\"spark.locality.wait\", \"0s\") \\\n    .getOrCreate()\n\n# Initialiseer H2O Sparkling Water context\nh2o_context = H2OContext.getOrCreate()\n\n# Print Spark en H2O versie informatie\nprint(f\"Spark version: {spark.version}\")\nprint(f\"H2O cluster version: {h2o.__version__}\")  # Gecorrigeerde versie-attribuut\n# De getSparklingWaterVersion methode bestaat niet, we slaan deze over\n# In plaats daarvan kunnen we de H2O cluster info printen\nprint(f\"H2O cluster info: {h2o.cluster().show_status()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:33:38.225386Z","iopub.execute_input":"2025-04-10T16:33:38.226213Z","iopub.status.idle":"2025-04-10T16:34:11.348005Z","shell.execute_reply.started":"2025-04-10T16:33:38.226141Z","shell.execute_reply":"2025-04-10T16:34:11.344932Z"}},"outputs":[{"name":"stdout","text":"Connecting to H2O server at http://8652cf76d465:54323 ... successful.\nWarning: Your H2O cluster version is (5 months and 8 days) old.  There may be a newer version available.\nPlease download and install the latest version from: https://h2o-release.s3.amazonaws.com/h2o/latest_stable.html\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"--------------------------  ----------------------------------------\nH2O_cluster_uptime:         18 secs\nH2O_cluster_timezone:       Etc/UTC\nH2O_data_parsing_timezone:  UTC\nH2O_cluster_version:        3.46.0.6\nH2O_cluster_version_age:    5 months and 8 days\nH2O_cluster_name:           sparkling-water-root_local-1744302825258\nH2O_cluster_total_nodes:    1\nH2O_cluster_free_memory:    5 Gb\nH2O_cluster_total_cores:    4\nH2O_cluster_allowed_cores:  2\nH2O_cluster_status:         locked, healthy\nH2O_connection_url:         http://8652cf76d465:54323\nH2O_connection_proxy:       null\nH2O_internal_security:      False\nPython_version:             3.10.12 final\n--------------------------  ----------------------------------------","text/html":"\n<style>\n\n#h2o-table-1.h2o-container {\n  overflow-x: auto;\n}\n#h2o-table-1 .h2o-table {\n  /* width: 100%; */\n  margin-top: 1em;\n  margin-bottom: 1em;\n}\n#h2o-table-1 .h2o-table caption {\n  white-space: nowrap;\n  caption-side: top;\n  text-align: left;\n  /* margin-left: 1em; */\n  margin: 0;\n  font-size: larger;\n}\n#h2o-table-1 .h2o-table thead {\n  white-space: nowrap; \n  position: sticky;\n  top: 0;\n  box-shadow: 0 -1px inset;\n}\n#h2o-table-1 .h2o-table tbody {\n  overflow: auto;\n}\n#h2o-table-1 .h2o-table th,\n#h2o-table-1 .h2o-table td {\n  text-align: right;\n  /* border: 1px solid; */\n}\n#h2o-table-1 .h2o-table tr:nth-child(even) {\n  /* background: #F5F5F5 */\n}\n\n</style>      \n<div id=\"h2o-table-1\" class=\"h2o-container\">\n  <table class=\"h2o-table\">\n    <caption></caption>\n    <thead></thead>\n    <tbody><tr><td>H2O_cluster_uptime:</td>\n<td>18 secs</td></tr>\n<tr><td>H2O_cluster_timezone:</td>\n<td>Etc/UTC</td></tr>\n<tr><td>H2O_data_parsing_timezone:</td>\n<td>UTC</td></tr>\n<tr><td>H2O_cluster_version:</td>\n<td>3.46.0.6</td></tr>\n<tr><td>H2O_cluster_version_age:</td>\n<td>5 months and 8 days</td></tr>\n<tr><td>H2O_cluster_name:</td>\n<td>sparkling-water-root_local-1744302825258</td></tr>\n<tr><td>H2O_cluster_total_nodes:</td>\n<td>1</td></tr>\n<tr><td>H2O_cluster_free_memory:</td>\n<td>5 Gb</td></tr>\n<tr><td>H2O_cluster_total_cores:</td>\n<td>4</td></tr>\n<tr><td>H2O_cluster_allowed_cores:</td>\n<td>2</td></tr>\n<tr><td>H2O_cluster_status:</td>\n<td>locked, healthy</td></tr>\n<tr><td>H2O_connection_url:</td>\n<td>http://8652cf76d465:54323</td></tr>\n<tr><td>H2O_connection_proxy:</td>\n<td>null</td></tr>\n<tr><td>H2O_internal_security:</td>\n<td>False</td></tr>\n<tr><td>Python_version:</td>\n<td>3.10.12 final</td></tr></tbody>\n  </table>\n</div>\n"},"metadata":{}},{"name":"stdout","text":"\nSparkling Water Context:\n * Sparkling Water Version: 3.46.0.6-1-3.1\n * H2O name: sparkling-water-root_local-1744302825258\n * cluster size: 1\n * list of used nodes:\n  (executorId, host, port)\n  ------------------------\n  (0,172.19.2.2,54321)\n  ------------------------\n\n  Open H2O Flow in browser: http://8652cf76d465:54323 (CMD + click in Mac OSX)\n\n    \nSpark version: 3.1.2\nH2O cluster version: 3.46.0.6\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"--------------------------  ----------------------------------------\nH2O_cluster_uptime:         18 secs\nH2O_cluster_timezone:       Etc/UTC\nH2O_data_parsing_timezone:  UTC\nH2O_cluster_version:        3.46.0.6\nH2O_cluster_version_age:    5 months and 8 days\nH2O_cluster_name:           sparkling-water-root_local-1744302825258\nH2O_cluster_total_nodes:    1\nH2O_cluster_free_memory:    5 Gb\nH2O_cluster_total_cores:    4\nH2O_cluster_allowed_cores:  2\nH2O_cluster_status:         locked, healthy\nH2O_connection_url:         http://8652cf76d465:54323\nH2O_connection_proxy:       null\nH2O_internal_security:      False\nPython_version:             3.10.12 final\n--------------------------  ----------------------------------------","text/html":"\n<style>\n\n#h2o-table-2.h2o-container {\n  overflow-x: auto;\n}\n#h2o-table-2 .h2o-table {\n  /* width: 100%; */\n  margin-top: 1em;\n  margin-bottom: 1em;\n}\n#h2o-table-2 .h2o-table caption {\n  white-space: nowrap;\n  caption-side: top;\n  text-align: left;\n  /* margin-left: 1em; */\n  margin: 0;\n  font-size: larger;\n}\n#h2o-table-2 .h2o-table thead {\n  white-space: nowrap; \n  position: sticky;\n  top: 0;\n  box-shadow: 0 -1px inset;\n}\n#h2o-table-2 .h2o-table tbody {\n  overflow: auto;\n}\n#h2o-table-2 .h2o-table th,\n#h2o-table-2 .h2o-table td {\n  text-align: right;\n  /* border: 1px solid; */\n}\n#h2o-table-2 .h2o-table tr:nth-child(even) {\n  /* background: #F5F5F5 */\n}\n\n</style>      \n<div id=\"h2o-table-2\" class=\"h2o-container\">\n  <table class=\"h2o-table\">\n    <caption></caption>\n    <thead></thead>\n    <tbody><tr><td>H2O_cluster_uptime:</td>\n<td>18 secs</td></tr>\n<tr><td>H2O_cluster_timezone:</td>\n<td>Etc/UTC</td></tr>\n<tr><td>H2O_data_parsing_timezone:</td>\n<td>UTC</td></tr>\n<tr><td>H2O_cluster_version:</td>\n<td>3.46.0.6</td></tr>\n<tr><td>H2O_cluster_version_age:</td>\n<td>5 months and 8 days</td></tr>\n<tr><td>H2O_cluster_name:</td>\n<td>sparkling-water-root_local-1744302825258</td></tr>\n<tr><td>H2O_cluster_total_nodes:</td>\n<td>1</td></tr>\n<tr><td>H2O_cluster_free_memory:</td>\n<td>5 Gb</td></tr>\n<tr><td>H2O_cluster_total_cores:</td>\n<td>4</td></tr>\n<tr><td>H2O_cluster_allowed_cores:</td>\n<td>2</td></tr>\n<tr><td>H2O_cluster_status:</td>\n<td>locked, healthy</td></tr>\n<tr><td>H2O_connection_url:</td>\n<td>http://8652cf76d465:54323</td></tr>\n<tr><td>H2O_connection_proxy:</td>\n<td>null</td></tr>\n<tr><td>H2O_internal_security:</td>\n<td>False</td></tr>\n<tr><td>Python_version:</td>\n<td>3.10.12 final</td></tr></tbody>\n  </table>\n</div>\n"},"metadata":{}},{"name":"stdout","text":"H2O cluster info: None\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## Initialiseren van de Numerai API","metadata":{}},{"cell_type":"code","source":"# Initialiseer de Numerai API client\n# Voor het indienen van voorspellingen zijn API keys nodig\n# napi = NumerAPI(public_id=\"UW_PUBLIC_ID\", secret_key=\"UW_SECRET_KEY\")\nnapi = NumerAPI()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:34:11.34966Z","iopub.execute_input":"2025-04-10T16:34:11.350071Z","iopub.status.idle":"2025-04-10T16:34:11.362984Z","shell.execute_reply.started":"2025-04-10T16:34:11.350029Z","shell.execute_reply":"2025-04-10T16:34:11.361335Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Data downloaden en laden","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n#reduce_memory_usage(rmd)\ndef rmd(df, use_float16=1, verbose=True):\n    \"\"\"\n    Vermindert het geheugengebruik van een pandas DataFrame door\n    de datatypes te optimaliseren naar de kleinst mogelijke types.\n    \n    Parameters:\n    -----------\n    df : pandas.DataFrame\n        De DataFrame waarvan het geheugengebruik geoptimaliseerd moet worden\n    use_float16 : int\n        1 om float16 toe te staan, 0 om alleen float32/float64 te gebruiken\n        Dit is handig wanneer je later met libraries zoals PySpark werkt die geen float16 ondersteunen\n    verbose : bool\n        Als True, print voortgangsinformatie\n        \n    Returns:\n    --------\n    pandas.DataFrame\n        Een kopie van de originele DataFrame met geoptimaliseerde datatypes\n    \"\"\"\n    # Maak een kopie om de originele dataframe niet te wijzigen\n    df_copy = df.copy()\n    \n    # Definieer numerieke types die we kunnen optimaliseren\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    \n    # Bereken het startgeheugengebruik\n    start_mem = df_copy.memory_usage().sum() / 1024**2\n    if verbose:\n        print(f'Geheugengebruik van DataFrame is {start_mem:.2f} MB')\n    \n    # Loop door alle kolommen\n    for col in df_copy.columns:\n        col_type = df_copy[col].dtypes\n        \n        # Alleen numerieke kolommen optimaliseren\n        if col_type in numerics:\n            # Bereken min en max waarden\n            c_min = df_copy[col].min()\n            c_max = df_copy[col].max()\n            \n            # Integer types\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df_copy[col] = df_copy[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df_copy[col] = df_copy[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df_copy[col] = df_copy[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df_copy[col] = df_copy[col].astype(np.int64)\n            # Float types\n            else:\n                # Gebruik float16 alleen als use_float16 is ingeschakeld\n                if use_float16 == 1 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df_copy[col] = df_copy[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df_copy[col] = df_copy[col].astype(np.float32)\n                else:\n                    df_copy[col] = df_copy[col].astype(np.float64)\n    \n    # Bereken het eindgeheugengebruik\n    end_mem = df_copy.memory_usage().sum() / 1024**2\n    \n    # Print statistieken als verbose is ingeschakeld\n    if verbose:\n        print(f'Geheugengebruik na optimalisatie is: {end_mem:.2f} MB')\n        reduction = 100 * (start_mem - end_mem) / start_mem\n        print(f'Geheugengebruik verminderd met {reduction:.1f}%')\n    \n    return df_copy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:34:11.367045Z","iopub.execute_input":"2025-04-10T16:34:11.367537Z","iopub.status.idle":"2025-04-10T16:34:11.385212Z","shell.execute_reply.started":"2025-04-10T16:34:11.367492Z","shell.execute_reply":"2025-04-10T16:34:11.383904Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import pyarrow as pa\nimport pyarrow.parquet as pq\nimport dask.dataframe as dd\n\n# 1. PyArrow chunked reading\ndef read_parquet_with_pyarrow_chunked(file_path, chunksize=1000000, columns=None, filters=None, verbose=True):\n    \"\"\"\n    Leest een parquet bestand in chunks met PyArrow, wat veel efficiënter is dan Pandas.\n    \"\"\"\n    dataset = pq.ParquetDataset(file_path)\n    schema = dataset.schema\n    if verbose:\n        print(f\"Schema: {schema}\")\n        print(f\"Totaal aantal kolommen: {len(schema.names)}\")\n    scanner = dataset.scanner(columns=columns, filter=filters, batch_size=chunksize)\n    for i, batch in enumerate(scanner.to_batches()):\n        chunk_df = batch.to_pandas()\n        if verbose and i % 10 == 0:\n            print(f\"Verwerking chunk {i}, grootte: {len(chunk_df)} rijen\")\n        yield chunk_df\n        del batch\n        gc.collect()\n\n# 2. Dask voor out-of-core verwerking\ndef process_with_dask(file_path, sample_freq='W', output_path=None, memory_limit='16GB', date_column='timestamp', verbose=True):\n    \"\"\"\n    Gebruikt Dask voor out-of-core verwerking van grote parquet bestanden.\n    \"\"\"\n    import dask\n    dask.config.set({\"distributed.worker.memory.limit\": memory_limit})\n    if verbose:\n        print(f\"Start verwerking met Dask, geheugenlimiet: {memory_limit}\")\n    ddf = dd.read_parquet(file_path)\n    if verbose:\n        print(f\"Dask DataFrame info: {len(ddf.columns)} kolommen\")\n    if date_column in ddf.columns:\n        ddf[date_column] = dd.to_datetime(ddf[date_column])\n        if verbose:\n            print(f\"Resampling data naar {sample_freq} frequentie...\")\n        ddf = ddf.set_index(date_column)\n        resampled = ddf.resample(sample_freq).mean()\n        if output_path:\n            if verbose:\n                print(f\"Opslaan van geresampelde data naar {output_path}\")\n            resampled.to_parquet(output_path)\n        return resampled\n    return ddf\n\n# 3. SQLite voor query-gebaseerde verwerking\ndef process_with_sqlite(file_path, query, output_path=None, chunk_size=1000000, verbose=True):\n    \"\"\"\n    Converteert parquet naar SQLite voor efficiënte verwerking zonder alles in geheugen te laden.\n    \"\"\"\n    import sqlite3\n    import tempfile\n    with tempfile.NamedTemporaryFile(suffix='.db') as temp:\n        db_path = temp.name\n        if verbose:\n            print(f\"SQLite database aangemaakt op: {db_path}\")\n        conn = sqlite3.connect(db_path)\n        chunks = read_parquet_with_pyarrow_chunked(file_path, chunksize=chunk_size, verbose=verbose)\n        for i, chunk in enumerate(chunks):\n            if i == 0:\n                chunk.to_sql('data', conn, if_exists='replace', index=False)\n                if verbose:\n                    print(f\"Tabel aangemaakt. Schema: {list(chunk.columns)}\")\n            else:\n                chunk.to_sql('data', conn, if_exists='append', index=False)\n            if verbose:\n                print(f\"Chunk {i} toegevoegd aan database, {len(chunk)} rijen\")\n        if verbose:\n            print(f\"Query uitvoeren: {query}\")\n        result = pd.read_sql_query(query, conn)\n        if output_path:\n            result.to_parquet(output_path)\n            if verbose:\n                print(f\"Resultaat opgeslagen naar {output_path}\")\n        conn.close()\n        return result\n\n# 4. Directe wekelijkse sampling\ndef sample_weekly_direct(file_path, date_column='timestamp', value_columns=None, agg_func='mean', verbose=True):\n    \"\"\"\n    Leest en samplet data zonder alles in geheugen te houden.\n    \"\"\"\n    try:\n        if verbose:\n            print(f\"Start directe wekelijkse sampling van {file_path}\")\n        parquet_file = pq.ParquetFile(file_path)\n        all_columns = parquet_file.schema.names\n        if verbose:\n            print(f\"Beschikbare kolommen: {all_columns}\")\n        if date_column not in all_columns:\n            timestamp_cols = [col for col in all_columns if 'time' in col.lower() or 'date' in col.lower()]\n            if verbose and timestamp_cols:\n                print(f\"Waarschuwing: {date_column} niet gevonden, gebruik {timestamp_cols[0]}\")\n            if timestamp_cols:\n                date_column = timestamp_cols[0]\n        cols_to_read = [date_column]\n        if value_columns:\n            cols_to_read.extend([c for c in value_columns if c in all_columns])\n        else:\n            cols_to_read.extend([col for col in all_columns if col != date_column])\n        if verbose:\n            print(f\"Kolommen om te lezen: {len(cols_to_read)}\")\n        chunks_generator = read_parquet_with_pyarrow_chunked(\n            file_path, \n            columns=cols_to_read,\n            chunksize=5000000,\n            verbose=verbose\n        )\n        weekly_dfs = []\n        for i, chunk_df in enumerate(chunks_generator):\n            try:\n                chunk_df[date_column] = pd.to_datetime(chunk_df[date_column])\n                chunk_df.set_index(date_column, inplace=True)\n                if agg_func == 'mean':\n                    weekly_chunk = chunk_df.resample('W').mean()\n                elif agg_func == 'sum':\n                    weekly_chunk = chunk_df.resample('W').sum()\n                elif agg_func == 'first':\n                    weekly_chunk = chunk_df.resample('W').first()\n                elif agg_func == 'last':\n                    weekly_chunk = chunk_df.resample('W').last()\n                else:\n                    weekly_chunk = chunk_df.resample('W').mean()\n                weekly_dfs.append(weekly_chunk)\n            except Exception as e:\n                if verbose:\n                    print(f\"Fout bij verwerken chunk {i}: {e}\")\n            del chunk_df\n            gc.collect()\n        if weekly_dfs:\n            combined_weekly = pd.concat(weekly_dfs)\n            final_weekly = combined_weekly.groupby(level=0).agg(agg_func)\n            if verbose:\n                print(f\"Wekelijkse data gegenereerd: {len(final_weekly)} rijen\")\n            return final_weekly\n        else:\n            if verbose:\n                print(\"Geen wekelijkse data kunnen genereren!\")\n            return None\n    except Exception as e:\n        if verbose:\n            print(f\"Fout bij directe wekelijkse sampling: {e}\")\n            import traceback\n            traceback.print_exc()\n        return None\n\n# 5. Gecombineerde functie voor YIEDL data verwerking\ndef process_yiedl_data(file_path, method='direct', date_column='timestamp', output_path=None, verbose=True):\n    \"\"\"\n    Verwerkt YIEDL data met optimaal geheugengebruik.\n    \"\"\"\n    if method == 'direct':\n        if verbose:\n            print(\"\\n=== Direct weekly sampling ===\")\n        result = sample_weekly_direct(file_path, date_column=date_column, verbose=verbose)\n        if output_path and result is not None:\n            result.to_parquet(output_path)\n            if verbose:\n                print(f\"Wekelijkse data opgeslagen naar {output_path}\")\n        return result\n    elif method == 'dask':\n        if verbose:\n            print(\"\\n=== Dask processing ===\")\n        result = process_with_dask(\n            file_path, \n            sample_freq='W', \n            output_path=output_path,\n            date_column=date_column,\n            verbose=verbose\n        )\n        if output_path is None and result is not None:\n            if verbose:\n                print(\"Berekenen van Dask resultaat...\")\n            return result.compute()\n        return None\n    elif method == 'chunks':\n        if verbose:\n            print(\"\\n=== PyArrow chunked processing ===\")\n        chunks = read_parquet_with_pyarrow_chunked(file_path, verbose=verbose)\n        sample_chunks = []\n        for i, chunk in enumerate(chunks):\n            if i < 3:\n                if verbose:\n                    print(f\"Chunk {i}: {len(chunk)} rijen, {len(chunk.columns)} kolommen\")\n                sample_chunks.append(chunk.head(5))\n            else:\n                break\n        if sample_chunks:\n            return pd.concat(sample_chunks)\n        return None\n    elif method == 'sqlite':\n        if verbose:\n            print(\"\\n=== SQLite processing ===\")\n        # Voorbeeld query voor wekelijkse aggregatie\n        date_extract = f\"strftime('%Y-%W', {date_column})\"\n        query = f\"\"\"\n        SELECT {date_extract} as week, \n               AVG(price) as avg_price,\n               COUNT(*) as count\n        FROM data\n        GROUP BY week\n        ORDER BY week\n        \"\"\"\n        return process_with_sqlite(file_path, query, output_path=output_path, verbose=verbose)\n    else:\n        if verbose:\n            print(f\"Onbekende methode: {method}\")\n        return None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T17:02:20.159694Z","iopub.execute_input":"2025-04-10T17:02:20.160176Z","iopub.status.idle":"2025-04-10T17:02:20.185978Z","shell.execute_reply.started":"2025-04-10T17:02:20.160127Z","shell.execute_reply":"2025-04-10T17:02:20.184795Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"%%time\n# Download the Numerai training data to the current directory\nnapi.download_dataset(filename = \"crypto/v1.0/train_targets.parquet\", \n                      dest_path = os.getcwd() + \"/numerai_train_targets.parquet\")\n#napi.download_dataset(filename = \"crypto/v2.0/train_targets.parquet\", \n#                      dest_path = os.getcwd() + \"/numerai_train_targets.parquet\")\n# Download the Numerai live crypto universe to the current directory\nnapi.download_dataset(filename = \"crypto/v1.0/live_universe.parquet\", \n                      dest_path = os.getcwd() + \"/numerai_live_universe.parquet\")\n#napi.download_dataset(filename = \"crypto/v2.0/live_universe.parquet\", \n#                      dest_path = os.getcwd() + \"/numerai_live_universe.parquet\")\n\n# Load the Numerai training targets\ntrain_df = rmd(pd.read_parquet(\"numerai_train_targets.parquet\"))\n# Load the Numerai live universe\nlive = rmd(pd.read_parquet(\"numerai_live_universe.parquet\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:34:11.590476Z","iopub.execute_input":"2025-04-10T16:34:11.590964Z","iopub.status.idle":"2025-04-10T16:34:13.507702Z","shell.execute_reply.started":"2025-04-10T16:34:11.590909Z","shell.execute_reply":"2025-04-10T16:34:13.506401Z"}},"outputs":[{"name":"stdout","text":"Geheugengebruik van DataFrame is 14.12 MB\nGeheugengebruik na optimalisatie is: 13.11 MB\nGeheugengebruik verminderd met 7.1%\nGeheugengebruik van DataFrame is 0.01 MB\nGeheugengebruik na optimalisatie is: 0.01 MB\nGeheugengebruik verminderd met 0.0%\nCPU times: user 162 ms, sys: 144 ms, total: 306 ms\nWall time: 1.9 s\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"%%time\napi = CryptoAPI()\n# Parquet files\napi.download_dataset(\n\t\"crypto/v1.0/live_universe.parquet\",\n\t\"numerai_crypto_live_universe.parquet\"\n)\ngc.collect()\napi.download_dataset(\n\t\"crypto/v1.0/train_targets.parquet\",\n\t\"numerai_crypto_train_targets.parquet\"\n)\ngc.collect()\napi.download_dataset(\n\t\"crypto/v1.0/meta_model.parquet\",\n\t\"numerai_crypto_meta_model.parquet\"\n)\ngc.collect()\napi.download_dataset(\n\t\"crypto/v1.0/historical_meta_models.parquet\",\n\t\"numerai_crypto_historical_meta_models.parquet\"\n)\ngc.collect()\n# CSV Files\napi.download_dataset(\n\t\"crypto/v1.0/meta_model.csv\",\n\t\"numerai_crypto_meta_model.csv\"\n)\ngc.collect()\napi.download_dataset(\n\t\"crypto/v1.0/historical_meta_models.csv\",\n\t\"numerai_crypto_historical_meta_models.csv\"\n)\ngc.collect()\nworking_dir = '/kaggle/working/'\nfiles = os.listdir(working_dir)\nprint(\"Files in /kaggle/working/:\")\nfor f in files:\n    print(f)\n# # Load the data\nnumerai_crypto_train_targets = rmd(pd.read_parquet('numerai_crypto_train_targets.parquet'))\nnumerai_crypto_live_universe = rmd(pd.read_parquet('numerai_crypto_live_universe.parquet'))\ngc.collect()\nhistorical_meta_model_preds = rmd(pd.read_parquet('numerai_crypto_historical_meta_models.parquet'))\nlive_meta_model_preds = rmd(pd.read_parquet('numerai_crypto_meta_model.parquet'))\ngc.collect()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:34:13.511362Z","iopub.execute_input":"2025-04-10T16:34:13.511739Z","iopub.status.idle":"2025-04-10T16:34:19.812941Z","shell.execute_reply.started":"2025-04-10T16:34:13.511693Z","shell.execute_reply":"2025-04-10T16:34:19.8117Z"}},"outputs":[{"name":"stdout","text":"Files in /kaggle/working/:\n.virtual_documents\nnumerai_live_universe.parquet\nyiedl_historical.parquet\nh2ologs\nnumerai_crypto_historical_meta_models.parquet\nnumerai_train_targets.parquet\nnumerai_crypto_meta_model.csv\nnumerai\nnumerai_crypto_meta_model.parquet\nnumerai_crypto_train_targets.parquet\nnumerai_crypto_historical_meta_models.csv\nnumerai_crypto_live_universe.parquet\nyiedl_latest.parquet\nGeheugengebruik van DataFrame is 14.12 MB\nGeheugengebruik na optimalisatie is: 13.11 MB\nGeheugengebruik verminderd met 7.1%\nGeheugengebruik van DataFrame is 0.01 MB\nGeheugengebruik na optimalisatie is: 0.01 MB\nGeheugengebruik verminderd met 0.0%\nGeheugengebruik van DataFrame is 3.69 MB\nGeheugengebruik na optimalisatie is: 3.00 MB\nGeheugengebruik verminderd met 18.8%\nGeheugengebruik van DataFrame is 0.01 MB\nGeheugengebruik na optimalisatie is: 0.00 MB\nGeheugengebruik verminderd met 37.5%\nCPU times: user 1.71 s, sys: 97.2 ms, total: 1.81 s\nWall time: 6.29 s\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# Helper Function from example: https://github.com/councilofelders/notebooks/blob/main/yiedl_crypto_data/yiedl_crypto_data_for_numerai_example.ipynb\nimport requests\n\ndef download_file(url, output_filename):\n    response = requests.get(url)\n    if response.status_code == 200:\n        with open(output_filename, 'wb') as file:\n            file.write(response.content)\n        print(f\"File downloaded successfully as {output_filename}\")\n    else:\n        print(\"Failed to download file\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:34:19.814336Z","iopub.execute_input":"2025-04-10T16:34:19.814717Z","iopub.status.idle":"2025-04-10T16:34:19.820612Z","shell.execute_reply.started":"2025-04-10T16:34:19.814686Z","shell.execute_reply":"2025-04-10T16:34:19.819131Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"%%time\n# Download YIEDL crypto latest dataset to current directory\nurl = 'https://api.yiedl.ai/yiedl/v1/downloadDataset?type=latest'\noutput_filename = 'yiedl_latest.parquet'\ndownload_file(url, output_filename)\n\n\n# Download YIEDL crypto historical dataset to current directory\n# NOTE: it is a huge file in zip format. We need to unzip it afterwards\nurl = 'https://api.yiedl.ai/yiedl/v1/downloadDataset?type=historical'\noutput_filename = 'yiedl_historical.zip'\ndownload_file(url, output_filename)\n#10m9s-13m48s","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:34:19.821939Z","iopub.execute_input":"2025-04-10T16:34:19.822496Z","iopub.status.idle":"2025-04-10T16:48:08.523051Z","shell.execute_reply.started":"2025-04-10T16:34:19.822451Z","shell.execute_reply":"2025-04-10T16:48:08.5216Z"}},"outputs":[{"name":"stdout","text":"File downloaded successfully as yiedl_latest.parquet\nFile downloaded successfully as yiedl_historical.zip\nCPU times: user 51.2 s, sys: 1min 12s, total: 2min 3s\nWall time: 13min 48s\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"%%time\n# Unzip and rename the file\n!unzip -p yiedl_historical.zip > yiedl_historical.parquet\n!rm yiedl_historical.zip\n## 1m12s","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:48:08.524339Z","iopub.execute_input":"2025-04-10T16:48:08.524714Z","iopub.status.idle":"2025-04-10T16:49:20.57446Z","shell.execute_reply.started":"2025-04-10T16:48:08.524666Z","shell.execute_reply":"2025-04-10T16:49:20.572491Z"}},"outputs":[{"name":"stdout","text":"CPU times: user 1.57 s, sys: 598 ms, total: 2.17 s\nWall time: 1min 12s\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"file_path = \"yiedl_historical.parquet\"\n\n# Optie 1: Chunk-gebaseerde verwerking met PyArrow\nprint(\"\\n=== PyArrow chunked processing ===\")\nyiedl_historical = read_parquet_with_pyarrow_chunked(file_path, chunksize=5000000)\n\n# Voorbeeld van verwerking per chunk\nfor i, chunk in enumerate(yiedl_historical):\n    if i < 3:  # Eerste 3 chunks als voorbeeld\n        print(f\"Chunk {i}: {len(chunk)} rijen\")\n        # Hier kun je verwerking per chunk doen\n        # Na verwerking wordt het geheugen van de chunk vrijgegeven\n    else:\n        break\n\n# Optie 2: Verwerken met Dask\nprint(\"\\n=== Dask processing ===\")\nyield_weekly_dask = process_with_dask(file_path, sample_freq='W', \n                                     output_path=\"yiedl_weekly_data_dask.parquet\")\n\n# Optie 3: Direct wekelijks samplen\nprint(\"\\n=== Direct weekly sampling ===\")\nyiedl_weekly_df = sample_weekly_direct(file_path)\nprint(f\"Wekelijkse data: {len(weekly_df)} rijen\")\nyiedl_weekly_df.to_parquet(\"yiedl_weekly_data_direct.parquet\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:50:30.760057Z","iopub.execute_input":"2025-04-10T16:50:30.760556Z","iopub.status.idle":"2025-04-10T16:50:30.870482Z","shell.execute_reply.started":"2025-04-10T16:50:30.760519Z","shell.execute_reply":"2025-04-10T16:50:30.869052Z"}},"outputs":[{"name":"stdout","text":"\n=== PyArrow chunked processing ===\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-289a609d0a63>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Voorbeeld van verwerking per chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myiedl_historical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Eerste 3 chunks als voorbeeld\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Chunk {i}: {len(chunk)} rijen\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-2fc435c298d4>\u001b[0m in \u001b[0;36mread_parquet_with_pyarrow_chunked\u001b[0;34m(file_path, chunksize, columns, filters)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Lees metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Schema: {schema}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Totaal aantal rijen: onbekend (chunked reading)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'verbose' is not defined"],"ename":"NameError","evalue":"name 'verbose' is not defined","output_type":"error"}],"execution_count":14},{"cell_type":"code","source":"%%time\n# Load and display the YIEDL historical crypto dataset\n## Faalt bij ram gebruik 30GB\n##df_yield_historical = rmd(pd.read_parquet(\"yiedl_historical.parquet\",\n##                                      engine = \"pyarrow\",\n                                      dtype_backend = \"numpy_nullable\"))                                    \n# Check dtypes\nyield_historical.dtypes\n# Display\n#display(df_yield_historical)\n## ms","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:09:50.868493Z","iopub.execute_input":"2025-04-10T16:09:50.868813Z","execution_failed":"2025-04-10T16:11:55.086Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n# Load and display the YIEDL latest crypto dataset\ndf_yield_latest = rmd(pd.read_parquet(\"yiedl_latest.parquet\", \n                                  engine = \"pyarrow\",\n                                  dtype_backend = \"numpy_nullable\"))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-10T16:11:55.091Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"''' numerai competitie dataset\n# Gebruik een van de nieuwste dataversies\nDATA_VERSION = \"v5.0\"\n\n# Maak een data directory\n!mkdir -p {DATA_VERSION}\n\n# Download data\nprint(\"Downloading training data...\")\nnapi.download_dataset(f\"{DATA_VERSION}/train.parquet\")\nnapi.download_dataset(f\"{DATA_VERSION}/features.json\")\n\n# Laad feature metadata\nfeature_metadata = json.load(open(f\"{DATA_VERSION}/features.json\"))\nprint(\"Available feature sets:\", list(feature_metadata[\"feature_sets\"].keys()))\nfeatures = feature_metadata[\"feature_sets\"][\"small\"]  # gebruik \"small\" voor sneller testen, \"medium\" of \"all\" voor betere prestaties\n'''","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-10T15:29:33.491Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\n# PyDrive implementation for Google Drive integration\nfrom pydrive2.auth import GoogleAuth\nfrom pydrive2.drive import GoogleDrive\nfrom google.colab import auth\nfrom oauth2client.client import GoogleCredentials\n\ndef setup_pydrive():\n    # Authenticate and create the PyDrive client\n    auth.authenticate_user()\n    gauth = GoogleAuth()\n    gauth.credentials = GoogleCredentials.get_application_default()\n    drive = GoogleDrive(gauth)\n    return drive\n\ndef create_folder_if_not_exists(drive, folder_name):\n    # Check if folder exists\n    file_list = drive.ListFile({\"q\": f\"title='{folder_name}' and mimeType='application/vnd.google-apps.folder' and trashed=false\"}).GetList()\n    \n    if len(file_list) > 0:\n        # Folder exists, return the folder ID\n        return file_list[0][\"id\"]\n    else:\n        # Create folder\n        folder = drive.CreateFile({\"title\": folder_name, \"mimeType\": \"application/vnd.google-apps.folder\"})\n        folder.Upload()\n        return folder[\"id\"]\n\ndef save_notebook_to_drive(drive, folder_id, notebook_name):\n    # Create a file in the folder\n    file = drive.CreateFile({\"title\": notebook_name, \"parents\": [{\"id\": folder_id}]})\n    \n    # Get the content of the current notebook\n    notebook_content = open(notebook_name, \"r\").read()\n    \n    # Set the content of the file\n    file.SetContentString(notebook_content)\n    file.Upload()\n    \n    return file[\"id\"]\n\ntry:\n    # Setup PyDrive\n    drive = setup_pydrive()\n    print(\"Successfully authenticated with Google Drive\")\n    \n    # Create Numer_crypto folder if it doesn't exist\n    folder_id = create_folder_if_not_exists(drive, \"Numer_crypto\")\n    print(f\"Numer_crypto folder ID: {folder_id}\")\n    \n    ## Save the current notebook to the folder\n    #notebook_name = \"numerai_sparkling_water_kaggle.ipynb\"\n    #file_id = save_notebook_to_drive(drive, folder_id, notebook_name)\n    #print(f\"Notebook saved to Google Drive with file ID: {file_id}\")\n    \n    ## List files in the folder\n    #file_list = drive.ListFile({\"q\": f\"'{folder_id}' in parents and trashed=false\"}).GetList()\n    #print(\"Files in Numer_crypto folder:\")\n    #for file in file_list:\n    #    print(f\"- {file['title']} (ID: {file['id']})\")\nexcept Exception as e:\n    print(f\"Error with PyDrive: {e}\")\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T15:32:58.464448Z","iopub.execute_input":"2025-04-10T15:32:58.464768Z","iopub.status.idle":"2025-04-10T15:33:39.964091Z","shell.execute_reply.started":"2025-04-10T15:32:58.464742Z","shell.execute_reply":"2025-04-10T15:33:39.963232Z"}},"outputs":[{"name":"stdout","text":"Successfully authenticated with Google Drive\nNumer_crypto folder ID: 1nLS8F4unm5wKYIgzTqk8tPyURpRx17w3\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Data laden met PySpark","metadata":{}},{"cell_type":"code","source":"'''\n# Laad trainingsdata met Spark\nprint(\"Loading training data with Spark...\")\ntrain_spark = spark.read.parquet(f\"{DATA_VERSION}/train.parquet\")\n\n# Selecteer alleen de benodigde kolommen\ncolumns_to_select = [\"era\"] + features + [\"target\"]\ntrain_spark = train_spark.select(*columns_to_select)\n\n# Downsampling voor snelheid (optioneel)\nprint(\"Preparing data for training...\")\n# Haal unieke era's op en sample 25% (elke 4e era)\nunique_eras = [row.era for row in train_spark.select(\"era\").distinct().collect()]\nsampled_eras = unique_eras[::4]\ntrain_spark = train_spark.filter(col(\"era\").isin(sampled_eras))\n\n# Bekijk de data\nprint(f\"Training data count: {train_spark.count()}\")\nprint(f\"Number of features: {len(features)}\")\nprint(f\"Number of eras: {len(sampled_eras)}\")\n\n# Toon schema\ntrain_spark.printSchema()\n'''","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-10T15:29:33.491Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data voorbereiden met PySpark","metadata":{}},{"cell_type":"code","source":"'''\n# Bereid data voor met Spark ML Pipeline\nprint(\"Preparing feature vector with Spark...\")\n\n# Maak een feature vector van alle features\nassembler = VectorAssembler(inputCols=features, outputCol=\"features\")\ntrain_spark = assembler.transform(train_spark)\n\n# Toon een voorbeeld van de getransformeerde data\ntrain_spark.select(\"era\", \"features\", \"target\").show(5, truncate=True)\n'''","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-10T15:29:33.492Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Converteren van Spark DataFrame naar H2O Frame","metadata":{}},{"cell_type":"code","source":"'''\n# Converteer Spark DataFrame naar H2O Frame\nprint(\"Converting Spark DataFrame to H2O Frame...\")\ntrain_h2o = h2o_context.asH2OFrame(train_spark)\n\n# Bekijk H2O Frame info\ntrain_h2o.describe()\n'''","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-10T15:29:33.492Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature engineering","metadata":{}},{"cell_type":"code","source":"# using standard Feature Engineering from here: https://www.kaggle.com/code/lucasmorin/crypto-forecasting-lgbm-feval-feature-importance\n# https://stackoverflow.com/questions/38641691/weighted-correlation-coefficient-with-pandas\ndef wmean(x, w):\n    return np.sum(x * w) / np.sum(w)\n\ndef wcov(x, y, w):\n    return np.sum(w * (x - wmean(x, w)) * (y - wmean(y, w))) / np.sum(w)\n\ndef wcorr(x, y, w):\n    return wcov(x, y, w) / np.sqrt(wcov(x, x, w) * wcov(y, y, w))\n\ndef eval_wcorr(preds, train_data):\n    w = train_data.add_w.values.flatten()\n    y_true = train_data.get_label()\n    return 'eval_wcorr', wcorr(preds, y_true, w), True\n\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-10T15:46:41.625Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Numerai crypto voorbeeld model","metadata":{}},{"cell_type":"code","source":"%%time\ndef generate_training_features(df: pd.DataFrame) -> List[str]:\n    # TODO: Get your data and create features\n    df['fake_feature_1'] = df.groupby([\"symbol\", \"date\"])['symbol'].transform(lambda x: random.uniform(0, 1))\n    return ['fake_feature_1']\n\n# Historical targets file contains [\"symbol\", \"date\", \"target\"] columns\n#train_df\n\n# Add training features for each (symbol, date)\nfeature_cols = generate_training_features(train_df)\n\nmodel = lgb.LGBMRegressor(\n    n_estimators=2000,\n    learning_rate=0.01,\n    max_depth=5,\n    num_leaves=2 ** 5,\n    colsample_bytree=0.1\n)\n\nmodel.fit(\n    train_df[feature_cols],\n    train_df[\"target\"]\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_features(df: pd.DataFrame):\n    # TODO: Get your data and create features for live universe\n    df['fake_feature_1'] = df['symbol'].transform(lambda x: random.uniform(0, 1))\n\n# Use API keys to authenticate\nnapi = NumerAPI(\"[your api public id]\", \"[your api secret key]\")\n\n# Generate features for the live universe\ngenerate_features(live)\n\n# Get live predictions\nlive[\"signal\"] = model.predict(live[feature_cols])\n\n# Predictions must be between 0 and 1\nlive[\"signal\"] = live[\"signal\"].rank(pct=True)\n\n# Format and save submission\nlive[['symbol', 'signal']].to_parquet(\"submission.parquet\")\n\n# Get model ids and submit models\nmodels = napi.get_models(tournament=12)\nfor model_name, model_id in models.items():\n    print(f'submitting {model_name}...')\n    napi.upload_predictions(\"submission.parquet\", model_id=model_id, tournament=12)\n\nprint('done!')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Claude.ai versie van model met behulp van GPU","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nimport random\nfrom typing import List\nimport time\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\n\n# Controleer of GPU beschikbaar is\ndef check_gpu_availability():\n    try:\n        # Voor Kaggle P100\n        !nvidia-smi\n        print(\"GPU is beschikbaar\")\n        return True\n    except:\n        print(\"GPU niet gevonden, fall back naar CPU\")\n        return False\n\n# Verbeterde versie van feature generatie functie\ndef generate_training_features(df: pd.DataFrame) -> List[str]:\n    \"\"\"\n    Genereert features voor het trainingsmodel met betere prestaties.\n    \n    Args:\n        df: DataFrame met minimaal 'symbol' en 'date' kolommen\n    \n    Returns:\n        List met namen van gegenereerde feature kolommen\n    \"\"\"\n    # Start timer voor benchmarking\n    start_time = time.time()\n    \n    # Lijst om feature namen bij te houden\n    feature_cols = []\n    \n    # Sorteer de data op symbol en date - belangrijk voor tijdreekseigenschappen\n    df = df.sort_values(['symbol', 'date'])\n    \n    # Basis statistieken per symbol\n    print(\"Berekenen groepsstatistieken...\")\n    \n    # Meer betekenisvolle features genereren (voorbeeld)\n    # Voor een crypto competitie zouden we features toe kunnen voegen zoals:\n    \n    # 1. Mean encoding van symbol om rekening te houden met crypto-specifieke eigenschappen\n    df['symbol_mean_target'] = df.groupby('symbol')['target'].transform('mean')\n    feature_cols.append('symbol_mean_target')\n    \n    # 2. Tijdsdimensie features\n    if 'date' in df.columns:\n        df['date'] = pd.to_datetime(df['date'])\n        df['day_of_week'] = df['date'].dt.dayofweek\n        df['month'] = df['date'].dt.month\n        df['quarter'] = df['date'].dt.quarter\n        feature_cols.extend(['day_of_week', 'month', 'quarter'])\n    \n    # 3. Rolling statistieken (als we aanvullende prijs/volume data zouden hebben)\n    # Als voorbeeld, simuleren we hier wat prijsdata\n    if 'fake_price' not in df.columns:\n        df['fake_price'] = np.random.normal(100, 10, size=len(df))\n    \n    # Bereken rolling statistieken met window size 7\n    for window in [7, 14, 30]:\n        # Rolling mean\n        df[f'price_rolling_mean_{window}'] = df.groupby('symbol')['fake_price'].transform(\n            lambda x: x.rolling(window=window, min_periods=1).mean())\n        \n        # Rolling volatility (std)\n        df[f'price_rolling_std_{window}'] = df.groupby('symbol')['fake_price'].transform(\n            lambda x: x.rolling(window=window, min_periods=1).std())\n        \n        # Momentum (% verandering)\n        df[f'price_momentum_{window}'] = df.groupby('symbol')['fake_price'].transform(\n            lambda x: x.pct_change(periods=window).fillna(0))\n        \n        feature_cols.extend([\n            f'price_rolling_mean_{window}',\n            f'price_rolling_std_{window}',\n            f'price_momentum_{window}'\n        ])\n    \n    # 4. Kruisende moving averages (technische indicators)\n    df['sma_short'] = df.groupby('symbol')['fake_price'].transform(\n        lambda x: x.rolling(window=7, min_periods=1).mean())\n    df['sma_long'] = df.groupby('symbol')['fake_price'].transform(\n        lambda x: x.rolling(window=21, min_periods=1).mean())\n    df['sma_cross'] = (df['sma_short'] > df['sma_long']).astype(int)\n    feature_cols.append('sma_cross')\n    \n    # Random noise feature (als placeholder)\n    df['random_feature'] = np.random.normal(0, 1, size=len(df))\n    feature_cols.append('random_feature')\n    \n    # Log execution time\n    end_time = time.time()\n    print(f\"Feature generatie voltooid in {end_time - start_time:.2f} seconden\")\n    print(f\"Gegenereerde features: {len(feature_cols)}\")\n    \n    return feature_cols\n\n# Train een GPU-versneld LightGBM model\ndef train_lgbm_model(train_df, feature_cols, target_col=\"target\", use_gpu=False):\n    \"\"\"\n    Traint een LightGBM model met GPU acceleratie indien beschikbaar\n    \n    Args:\n        train_df: DataFrame met trainingsdata\n        feature_cols: Lijst met feature kolommen\n        target_col: Naam van de target kolom\n        use_gpu: Boolean om GPU te gebruiken\n    \n    Returns:\n        Getraind LightGBM model\n    \"\"\"\n    print(f\"Training model op {'GPU' if use_gpu else 'CPU'}...\")\n    start_time = time.time()\n    \n    # Bereken optimale parameters op basis van dataset grootte\n    num_samples = len(train_df)\n    num_features = len(feature_cols)\n    \n    # Pas hyperparameters aan voor GPU training\n    params = {\n        'objective': 'regression',\n        'metric': 'rmse',\n        'n_estimators': 2000,\n        'learning_rate': 0.01,\n        'max_depth': 5,\n        'num_leaves': 2**5,\n        'colsample_bytree': 0.1,\n        'verbosity': -1,\n        'early_stopping_rounds': 50\n    }\n    \n    # GPU-specifieke parameters toevoegen indien nodig\n    if use_gpu:\n        params.update({\n            'device': 'gpu',\n            'gpu_platform_id': 0,\n            'gpu_device_id': 0,\n            'use_gpu_hist': True,\n            'gpu_use_dp': True  # Gebruik dubbele precisie voor betere nauwkeurigheid\n        })\n    \n    # Split data voor early stopping\n    from sklearn.model_selection import train_test_split\n    X_train, X_val, y_train, y_val = train_test_split(\n        train_df[feature_cols], \n        train_df[target_col],\n        test_size=0.2,\n        random_state=42\n    )\n    \n    # Maak LGBMRegressor met aangepaste parameters\n    model = lgb.LGBMRegressor(**params)\n    \n    # Fit het model met evaluation set\n    model.fit(\n        X_train, y_train,\n        eval_set=[(X_val, y_val)],\n        verbose=100  # Toon training voortgang elke 100 iteraties\n    )\n    \n    # Evalueer het model\n    val_preds = model.predict(X_val)\n    rmse = mean_squared_error(y_val, val_preds, squared=False)\n    print(f\"Validatie RMSE: {rmse:.6f}\")\n    \n    # Check vroeg stoppen\n    print(f\"Model stopte na {model.best_iteration_} iteraties\")\n    \n    # Print feature importance\n    feature_importance = pd.DataFrame({\n        'Feature': feature_cols,\n        'Importance': model.feature_importances_\n    }).sort_values('Importance', ascending=False)\n    \n    print(\"\\nTop 10 belangrijkste features:\")\n    print(feature_importance.head(10))\n    \n    # Plot feature importance\n    plt.figure(figsize=(10, 6))\n    plt.barh(feature_importance['Feature'][:15], feature_importance['Importance'][:15])\n    plt.xlabel('Importance')\n    plt.title('Feature Importance (Top 15)')\n    plt.gca().invert_yaxis()\n    plt.show()\n    \n    end_time = time.time()\n    print(f\"Model training voltooid in {end_time - start_time:.2f} seconden\")\n    \n    return model\n\n# Hoofdprogramma\nif __name__ == \"__main__\":\n    # Controleer GPU beschikbaarheid\n    use_gpu = check_gpu_availability()\n    \n    # Laad trainingsdata (vervang dit met je echte data loading logica)\n    print(\"Laden van trainingsdata...\")\n    \n    # Voorbeeld: als je trainingsdata uit een lokaal bestand laadt\n    # train_df = pd.read_csv('/path/to/train.csv')\n    \n    # Voor demonstratie, maken we synthetische data\n    symbols = ['BTC', 'ETH', 'XRP', 'ADA', 'SOL', 'DOT', 'AVAX', 'MATIC']\n    dates = pd.date_range(start='2020-01-01', end='2023-01-01', freq='D')\n    \n    data = []\n    for symbol in symbols:\n        for date in dates:\n            data.append({\n                'symbol': symbol,\n                'date': date,\n                'target': np.random.normal(0, 1)  # random target waarde\n            })\n    \n    train_df = pd.DataFrame(data)\n    print(f\"Trainingsdata geladen: {train_df.shape}\")\n    \n    # Genereer features\n    feature_cols = generate_training_features(train_df)\n    \n    # Train model met GPU indien beschikbaar\n    model = train_lgbm_model(train_df, feature_cols, use_gpu=use_gpu)\n    \n    # Sla model op\n    import pickle\n    with open('numerai_crypto_model.pkl', 'wb') as f:\n        pickle.dump(model, f)\n    \n    print(\"Model opgeslagen als 'numerai_crypto_model.pkl'\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluation and model flow nfold lgbm","metadata":{}},{"cell_type":"code","source":"n_fold = 5\n\nimportances = []\n\nfor fold in range(n_fold):\n    print('Fold: '+str(fold))\n\n    train = pd.read_parquet('../input/crypto-forecasting-static-feature-engineering/train_fold_'+str(fold)+'.parquet')\n    test = pd.read_parquet('../input/crypto-forecasting-static-feature-engineering/test_fold_'+str(fold)+'.parquet')\n    \n    if DEBUG:\n        timestamp_sample_train = train.timestamp.unique()[:np.int(len(train.timestamp.unique())*0.05)]\n        timestamp_sample_test = test.timestamp.unique()[:np.int(len(test.timestamp.unique())*0.05)]\n        train = train[train.timestamp.isin(timestamp_sample_train)]\n        test = test[test.timestamp.isin(timestamp_sample_test)]\n\n    y_train = train['Target']\n    y_test = test['Target']\n\n    features = [col for col in train.columns if col not in {'timestamp', 'Target', 'Target_M','weights'}]\n\n    weights_train = train[['weights']]\n    weights_test = test[['weights']]\n\n    train = train[features]\n    test = test[features]\n    \n    train_dataset = lgb.Dataset(train, y_train, feature_name = features, categorical_feature= ['Asset_ID'])\n    val_dataset = lgb.Dataset(test, y_test, feature_name = features, categorical_feature= ['Asset_ID'])\n\n    train_dataset.add_w = weights_train\n    val_dataset.add_w = weights_test\n\n    val_data = test\n    val_y = y_test\n\n    del train\n    \n    evals_result = {}\n    \n    # parameters\n    params = {'n_estimators': 2000,\n            'objective': 'regression',\n            'metric': 'None',\n            'boosting_type': 'gbdt',\n            'max_depth': -1,\n            'learning_rate': 0.05,\n            'subsample': 0.72,\n            'subsample_freq': 4,\n            'feature_fraction': 0.4,\n            'lambda_l1': 1,\n            'lambda_l2': 1,\n            'seed': 46,\n            'verbose': -1,\n            }\n\n    model = lgb.train(params = params,\n                      train_set = train_dataset, \n                      valid_sets = [val_dataset],\n                      #early_stopping_rounds=1000,\n                      verbose_eval = 100,\n                      feval=eval_wcorr,\n                      evals_result = evals_result \n                     )\n    \n    importances.append(model.feature_importance(importance_type='gain'))\n    \n    plt.plot(np.array(evals_result['valid_0']['eval_wcorr']), label='fold '+str(fold))\n    \nplt.legend(loc=\"upper left\")\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-10T15:29:33.492Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model trainen met H2O XGBoost via Sparkling Water","metadata":{}},{"cell_type":"code","source":"# Train model met H2O XGBoost via Sparkling Water\nprint(\"Training H2O XGBoost model via Sparkling Water...\")\nstart_time = time.time()\n\n# Configureer XGBoost model\nfrom h2o.estimators.xgboost import H2OXGBoostEstimator\n\nxgb_model = H2OXGBoostEstimator(\n    ntrees=2000,\n    max_depth=5,\n    learn_rate=0.01,\n    sample_rate=0.8,\n    col_sample_rate=0.8,\n    tree_method=\"auto\",  # auto selecteert GPU indien beschikbaar\n    booster=\"gbtree\",\n    seed=42\n)\n\n# Train het model\nxgb_model.train(x=features, y=\"target\", training_frame=train_h2o)\n\ntraining_time = time.time() - start_time\nprint(f\"Training completed in {training_time:.2f} seconds\")\n\n# Toon model informatie\nprint(xgb_model)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-10T15:29:33.492Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature importance visualiseren","metadata":{}},{"cell_type":"code","source":"# Feature importance visualiseren\nfeature_importance = xgb_model.varimp(use_pandas=True)\nif feature_importance is not None:\n    plt.figure(figsize=(10, 8))\n    plt.barh(range(len(feature_importance[:20])), feature_importance[:20]['relative_importance'])\n    plt.yticks(range(len(feature_importance[:20])), feature_importance[:20]['variable'])\n    plt.title('H2O XGBoost Feature Importance (top 20)')\n    plt.xlabel('Relative Importance')\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-10T15:29:33.493Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model opslaan als MOJO","metadata":{}},{"cell_type":"code","source":"# Sla het model op als MOJO (Model Object, Optimized)\nmojo_path = xgb_model.download_mojo(path=\"./\", get_genmodel_jar=True)\nprint(f\"Model saved as MOJO: {mojo_path}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-10T15:29:33.493Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Validatiedata laden en voorbereiden met PySpark","metadata":{}},{"cell_type":"code","source":"# Download validatiedata voor testen\nprint(\"Downloading validation data for testing...\")\nnapi.download_dataset(f\"{DATA_VERSION}/validation.parquet\")\n\n# Laad validatiedata met Spark\nprint(\"Loading validation data with Spark...\")\nvalidation_spark = spark.read.parquet(f\"{DATA_VERSION}/validation.parquet\")\n\n# Selecteer alleen de benodigde kolommen\ncolumns_to_select = [\"era\", \"data_type\"] + features\nvalidation_spark = validation_spark.select(*columns_to_select)\n\n# Filter alleen validatie data\nvalidation_spark = validation_spark.filter(col(\"data_type\") == \"validation\")\n\n# Neem een kleine subset voor geheugenefficiëntie\nvalidation_spark = validation_spark.limit(1000)\n\n# Maak een feature vector van alle features\nvalidation_spark = assembler.transform(validation_spark)\n\n# Converteer Spark DataFrame naar H2O Frame\nvalidation_h2o = h2o_context.asH2OFrame(validation_spark)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-10T15:29:33.493Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Voorspellingen maken met het model","metadata":{}},{"cell_type":"code","source":"# Maak voorspellingen met het model\nprint(\"Making predictions...\")\npredictions_h2o = xgb_model.predict(validation_h2o)\n\n# Converteer H2O Frame terug naar Spark DataFrame\npredictions_spark = h2o_context.asSparkFrame(predictions_h2o)\n\n# Toon voorspellingen\nprint(\"Sample predictions:\")\npredictions_spark.show(5)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-10T15:29:33.494Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Voorspellingsfunctie definiëren","metadata":{}},{"cell_type":"code","source":"# Definieer voorspellingsfunctie die werkt met H2O model\ndef predict(\n    live_features: pd.DataFrame,\n    live_benchmark_models: pd.DataFrame\n) -> pd.DataFrame:\n    # Converteer pandas DataFrame naar Spark DataFrame\n    live_features_spark = spark.createDataFrame(live_features[features])\n    \n    # Maak een feature vector van alle features\n    live_features_spark = assembler.transform(live_features_spark)\n    \n    # Converteer Spark DataFrame naar H2O Frame\n    live_features_h2o = h2o_context.asH2OFrame(live_features_spark)\n    \n    # Maak voorspellingen met het H2O model\n    preds = xgb_model.predict(live_features_h2o)\n    \n    # Converteer H2O voorspellingen terug naar pandas\n    predictions = h2o.as_list(preds)[\"predict\"].values\n    \n    # Maak submission DataFrame\n    submission = pd.Series(predictions, index=live_features.index)\n    return submission.to_frame(\"prediction\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-10T15:29:33.495Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Voorspellingsfunctie testen","metadata":{}},{"cell_type":"code","source":"# Converteer Spark DataFrame terug naar pandas voor testen\nvalidation_pd = validation_spark.toPandas()\n\n# Test voorspellingsfunctie\nprint(\"Testing prediction function...\")\n# Maak een lege DataFrame voor benchmark_models (niet gebruikt in onze voorspellingsfunctie)\nempty_benchmark = pd.DataFrame(index=validation_pd.index)\npredictions = predict(validation_pd, empty_benchmark)\n\nprint(f\"Predictions shape: {predictions.shape}\")\nprint(\"\\nSample predictions:\")\nprint(predictions.head())","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-10T15:29:33.495Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Voorspellingsfunctie opslaan met cloudpickle","metadata":{}},{"cell_type":"code","source":"# Pickle voorspellingsfunctie\nprint(\"Saving prediction function with cloudpickle...\")\np = cloudpickle.dumps(predict)\nwith open(\"numerai_sparkling_water_model.pkl\", \"wb\") as f:\n    f.write(p)\n\nprint(\"Prediction function saved as 'numerai_sparkling_water_model.pkl'\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-10T15:29:33.496Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Kaggle specifieke functies voor het opslaan van resultaten","metadata":{}},{"cell_type":"code","source":"# Opslaan van resultaten in Kaggle output\n# Dit maakt het mogelijk om de resultaten te downloaden of als dataset te gebruiken\ntry:\n    # Maak een output directory\n    !mkdir -p /kaggle/working/output\n    \n    # Kopieer de belangrijke bestanden\n    !cp numerai_sparkling_water_model.pkl /kaggle/working/output/\n    !cp {mojo_path} /kaggle/working/output/\n    \n    print(\"Model bestanden opgeslagen in Kaggle output directory\")\nexcept Exception as e:\n    print(f\"Fout bij opslaan in Kaggle output: {e}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-10T15:29:33.496Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Voordelen van Sparkling Water","metadata":{}},{"cell_type":"code","source":"# Hier zou je een vergelijking kunnen maken tussen standaard H2O en Sparkling Water\nprint(\"Sparkling Water Voordelen:\")\nprint(\"1. Gedistribueerde verwerking met Spark voor grote datasets\")\nprint(\"2. Combinatie van Spark's data processing met H2O's machine learning algoritmes\")\nprint(\"3. Betere schaalbaarheid voor complexe modellen en grote datasets\")\nprint(\"4. Mogelijkheid om Spark ML Pipeline te integreren met H2O modellen\")\nprint(f\"5. Onze training duurde {training_time:.2f} seconden met Sparkling Water\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-10T15:29:33.496Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Sluit H2O cluster af\nh2o.cluster().shutdown()\n\n# Sluit Spark sessie af\nspark.stop()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-10T15:29:33.496Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Financial Modeling Prep API Integration\nimport requests\nimport pandas as pd\n\nFMP_API_KEY = \"aDFEO9rxgvGL3VQgPcBxXblSZ3laRLap\"\nDEEPSEEK_API_KEY = \"sk-6a3502649b0048259e0009a328c71960\"\n\n# Function to get economic indicators from Financial Modeling Prep\ndef get_economic_indicators():\n    url = f\"https://financialmodelingprep.com/api/v3/economic/economic_indicators?apikey={FMP_API_KEY}\"\n    response = requests.get(url)\n    data = response.json()\n    return pd.DataFrame(data)\n\n# Get country and currency data\ndef get_country_currency_data():\n    url = f\"https://financialmodelingprep.com/api/v3/fx?apikey={FMP_API_KEY}\"\n    response = requests.get(url)\n    fx_data = response.json()\n    \n    # Get country profiles for ISO codes\n    url = f\"https://financialmodelingprep.com/api/v4/country_list?apikey={FMP_API_KEY}\"\n    response = requests.get(url)\n    country_data = response.json()\n    \n    # Create comprehensive country-currency mapping\n    country_df = pd.DataFrame(country_data)\n    fx_df = pd.DataFrame(fx_data)\n    \n    # Extract currency codes from FX pairs\n    currency_codes = set()\n    for pair in fx_df[\"ticker\"].values:\n        if \"/\" in pair:\n            base, quote = pair.split(\"/\")\n            currency_codes.add(base)\n            currency_codes.add(quote)\n    \n    # Create final mapping dataframe\n    mapping_data = []\n    for country in country_df.to_dict(\"records\"):\n        country_name = country.get(\"name\", \"\")\n        country_code = country.get(\"code\", \"\")\n        currency_name = country.get(\"currency\", \"\")\n        currency_code = \"\"\n        \n        # Try to find currency code\n        for code in currency_codes:\n            if len(code) == 3 and code.upper() in currency_name.upper():\n                currency_code = code\n                break\n        \n        mapping_data.append({\n            \"country_name\": country_name,\n            \"country_code\": country_code,\n            \"currency_name\": currency_name,\n            \"currency_code\": currency_code\n        })\n    \n    return pd.DataFrame(mapping_data)\n\n# Get economic indicators\ntry:\n    economic_indicators = get_economic_indicators()\n    print(\"Economic Indicators:\")\n    print(economic_indicators.head())\nexcept Exception as e:\n    print(f\"Error fetching economic indicators: {e}\")\n\n# Get country-currency mapping\ntry:\n    country_currency_mapping = get_country_currency_data()\n    print(\"\nCountry-Currency Mapping:\")\n    print(country_currency_mapping.head(20))\n    \n    # Save the mapping to CSV\n    country_currency_mapping.to_csv(\"country_currency_mapping.csv\", index=False)\n    print(\"\nSaved country-currency mapping to CSV file\")\nexcept Exception as e:\n    print(f\"Error creating country-currency mapping: {e}\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-10T15:29:33.496Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# DeepSeek API Integration for Crypto-Country Association\nimport requests\nimport json\n\ndef get_crypto_country_associations(cryptocurrencies):\n    url = \"https://api.deepseek.com/v1/chat/completions\"\n    \n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"Authorization\": f\"Bearer {DEEPSEEK_API_KEY}\"\n    }\n    \n    crypto_list = \", \".join(cryptocurrencies)\n    \n    data = {\n        \"model\": \"deepseek-chat\",\n        \"messages\": [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful assistant that provides accurate information about cryptocurrencies.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"For each of these cryptocurrencies: {crypto_list}, provide the country where they have their entity registered or where they primarily report taxes. Return the data in JSON format with cryptocurrency name, country name, and ISO country code.\"\n            }\n        ],\n        \"temperature\": 0.1,\n        \"max_tokens\": 2000\n    }\n    \n    try:\n        response = requests.post(url, headers=headers, json=data)\n        response_data = response.json()\n        \n        if \"choices\" in response_data and len(response_data[\"choices\"]) > 0:\n            content = response_data[\"choices\"][0][\"message\"][\"content\"]\n            \n            # Extract JSON from the response\n            try:\n                # Try to find JSON in the response\n                start_idx = content.find(\"{\")\n                end_idx = content.rfind(\"}\")\n                \n                if start_idx != -1 and end_idx != -1:\n                    json_str = content[start_idx:end_idx+1]\n                    return json.loads(json_str)\n                else:\n                    return {\"error\": \"No JSON found in response\", \"raw_response\": content}\n            except json.JSONDecodeError:\n                return {\"error\": \"Failed to parse JSON\", \"raw_response\": content}\n        else:\n            return {\"error\": \"No response from DeepSeek API\"}\n    except Exception as e:\n        return {\"error\": str(e)}\n\n# Example usage\ncryptocurrencies = [\"Bitcoin\", \"Ethereum\", \"Ripple\", \"Cardano\", \"Solana\"]\ntry:\n    crypto_country_data = get_crypto_country_associations(cryptocurrencies)\n    print(\"Cryptocurrency Country Associations:\")\n    print(json.dumps(crypto_country_data, indent=2))\n    \n    # Convert to DataFrame and save\n    if not isinstance(crypto_country_data, dict) or not crypto_country_data.get(\"error\"):\n        crypto_df = pd.DataFrame(crypto_country_data)\n        crypto_df.to_csv(\"crypto_country_associations.csv\", index=False)\n        print(\"\nSaved cryptocurrency country associations to CSV file\")\nexcept Exception as e:\n    print(f\"Error getting cryptocurrency country associations: {e}\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-10T15:29:33.497Z"}},"outputs":[],"execution_count":null}]}