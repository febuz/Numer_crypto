{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Kaggle-compatible Google Drive integration\n",
    "!pip install -q pydrive2\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pydrive2.auth import GoogleAuth\n",
    "from pydrive2.drive import GoogleDrive\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# Note: This is a simplified version for Kaggle environment\n",
    "# Full Google Drive integration requires authentication\n",
    "# which is handled differently in Kaggle vs Colab\n",
    "\n",
    "print(\"Google Drive integration is disabled in Kaggle environment\")\n",
    "print(\"To use Google Drive with this notebook:\")\n",
    "print(\"1. Run this notebook in Google Colab instead\")\n",
    "print(\"2. Or use Kaggle Datasets for data storage\")\n",
    "\n",
    "# Create placeholder functions to avoid errors\n",
    "def setup_pydrive():\n",
    "    print(\"Drive setup is disabled in Kaggle environment\")\n",
    "    return None\n",
    "\n",
    "def create_folder_if_not_exists(drive, folder_name):\n",
    "    print(f\"Would create folder: {folder_name} (disabled in Kaggle)\")\n",
    "    return \"placeholder-folder-id\"\n",
    "\n",
    "def save_notebook_to_drive(drive, folder_id, file_name):\n",
    "    print(f\"Would save file: {file_name} (disabled in Kaggle)\")\n",
    "    return \"placeholder-file-id\"\n",
    "\n",
    "# Use Kaggle's built-in output directory for saving files\n",
    "output_dir = \"/kaggle/working\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Files will be saved to: {output_dir}\")\n",
    "\n",
    "try:\n",
    "    # Setup PyDrive (placeholder in Kaggle)\n",
    "    drive = setup_pydrive()\n",
    "    \n",
    "    # Create Numer_crypto folder if it doesn't exist (placeholder)\n",
    "    folder_id = create_folder_if_not_exists(drive, \"Numer_crypto\")\n",
    "    \n",
    "    # Save the current notebook to the folder (placeholder)\n",
    "    notebook_name = \"numerai_sparkling_water_kaggle.ipynb\"\n",
    "    file_id = save_notebook_to_drive(drive, folder_id, notebook_name)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error with PyDrive: {e}\")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Financial Modeling Prep API Integration\n",
    "!pip install -q requests pandas\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Financial Modeling Prep API key\n",
    "FMP_API_KEY = \"aDFEO9rxgvGL3VQgPcBxXblSZ3laRLap\"\n",
    "\n",
    "def get_economic_indicators():\n",
    "    \"\"\"Get economic indicators from Financial Modeling Prep API\"\"\"\n",
    "    url = f\"https://financialmodelingprep.com/api/v3/economic/country_indicator?apikey={FMP_API_KEY}\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return pd.DataFrame(data)\n",
    "    else:\n",
    "        print(f\"Error fetching economic indicators: {response.status_code}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def get_country_currency_mapping():\n",
    "    \"\"\"Get country to currency mapping from Financial Modeling Prep API\"\"\"\n",
    "    url = f\"https://financialmodelingprep.com/api/v3/fx?apikey={FMP_API_KEY}\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        \n",
    "        # Extract unique currencies and their countries\n",
    "        currency_map = {}\n",
    "        for item in data:\n",
    "            if 'name' in item and '/' in item['name']:\n",
    "                currencies = item['name'].split('/')\n",
    "                if len(currencies) == 2:\n",
    "                    currency_map[currencies[0].strip()] = currencies[1].strip()\n",
    "        \n",
    "        return currency_map\n",
    "    else:\n",
    "        print(f\"Error fetching currency mapping: {response.status_code}\")\n",
    "        return {}\n",
    "\n",
    "def get_crypto_country_associations(cryptocurrencies):\n",
    "    \"\"\"Associate cryptocurrencies with countries based on currency usage\"\"\"\n",
    "    # Get country-currency mapping\n",
    "    currency_map = get_country_currency_mapping()\n",
    "    \n",
    "    # Get economic indicators for additional country data\n",
    "    economic_df = get_economic_indicators()\n",
    "    \n",
    "    # Create mapping data\n",
    "    mapping_data = []\n",
    "    \n",
    "    # For demonstration, we'll create some sample associations\n",
    "    # In a real implementation, this would use more sophisticated analysis\n",
    "    country_crypto_affinity = {\n",
    "        \"USD\": [\"Bitcoin\", \"Ethereum\", \"Ripple\"],\n",
    "        \"EUR\": [\"Ethereum\", \"Cardano\"],\n",
    "        \"JPY\": [\"Ripple\", \"Cardano\"],\n",
    "        \"GBP\": [\"Bitcoin\", \"Ethereum\"],\n",
    "        \"CHF\": [\"Bitcoin\", \"Solana\"],\n",
    "        \"CAD\": [\"Ethereum\", \"Cardano\"],\n",
    "        \"AUD\": [\"Ripple\", \"Solana\"],\n",
    "        \"CNY\": [\"Bitcoin\", \"Ethereum\"]\n",
    "    }\n",
    "    \n",
    "    for country, currency_code in currency_map.items():\n",
    "        for crypto in cryptocurrencies:\n",
    "            if currency_code in country_crypto_affinity and crypto in country_crypto_affinity[currency_code]:\n",
    "                mapping_data.append({\n",
    "                    \"country\": country,\n",
    "                    \"cryptocurrency\": crypto,\n",
    "                    \"currency_code\": currency_code\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(mapping_data)\n",
    "\n",
    "# Test the Financial Modeling Prep API integration\n",
    "try:\n",
    "    print(\"Testing Financial Modeling Prep API integration...\")\n",
    "    economic_indicators = get_economic_indicators()\n",
    "    if not economic_indicators.empty:\n",
    "        print(f\"Successfully retrieved {len(economic_indicators)} economic indicators\")\n",
    "        print(economic_indicators.head(3))\n",
    "    else:\n",
    "        print(\"No economic indicators retrieved\")\n",
    "        \n",
    "    # Test cryptocurrency country associations\n",
    "    cryptocurrencies = [\"Bitcoin\", \"Ethereum\", \"Ripple\", \"Cardano\", \"Solana\"]\n",
    "    crypto_country_data = get_crypto_country_associations(cryptocurrencies)\n",
    "    if not crypto_country_data.empty:\n",
    "        print(f\"\\nSuccessfully created {len(crypto_country_data)} cryptocurrency-country associations\")\n",
    "        print(crypto_country_data.head(3))\n",
    "        \n",
    "        # Save to CSV in Kaggle working directory\n",
    "        output_path = \"/kaggle/working/crypto_country_associations.csv\"\n",
    "        crypto_country_data.to_csv(output_path, index=False)\n",
    "        print(f\"Saved associations to {output_path}\")\n",
    "    else:\n",
    "        print(\"No cryptocurrency-country associations created\")\n",
    "except Exception as e:\n",
    "    print(f\"Error testing Financial Modeling Prep API: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerai Crypto Competitie Voorspellingsmodel met H2O Sparkling Water\n",
    "\n",
    "Dit notebook implementeert een voorspellingsmodel voor de Numerai/Numerai Crypto competitie met behulp van H2O Sparkling Water, wat H2O integreert met Apache Spark voor gedistribueerde verwerking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installatie van benodigde packages\n",
    "\n",
    "Eerst moeten we Java, Spark en H2O Sparkling Water installeren. Dit kan enige tijd duren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Installeer Java (vereist voor H2O en Spark)\n",
    "!apt-get update -qq\n",
    "!apt-get install -y default-jre > /dev/null\n",
    "!java -version\n",
    "\n",
    "# Installeer Spark en PySpark\n",
    "!pip install -q pyspark==3.1.2\n",
    "\n",
    "# Installeer H2O Sparkling Water\n",
    "!pip install -q h2o-pysparkling-3.1\n",
    "\n",
    "# Installeer andere benodigde packages\n",
    "!pip install -q numerapi pandas h2o cloudpickle==2.2.1 pyarrow scikit-learn scipy==1.10.1 matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importeren van benodigde libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from numerapi import NumerAPI\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Spark imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# H2O Sparkling Water imports\n",
    "from pysparkling import H2OContext\n",
    "# Original import that causes error: from pysparkling.ml import H2OXGBoostEstimator\n",
    "# Using the correct import for H2OXGBoostEstimator\n",
    "from h2o.estimators.xgboost import H2OXGBoostEstimator\n",
    "\n",
    "import h2o\n",
    "import cloudpickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialiseren van Spark en H2O Sparkling Water"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialiseer Spark sessie\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NumeraiSparklingWater\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Initialiseer H2O Sparkling Water context\n",
    "h2o_context = H2OContext.getOrCreate()\n",
    "\n",
    "# Print Spark en H2O versie informatie\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"H2O cluster version: {h2o.version()}\")\n",
    "print(f\"Sparkling Water version: {h2o_context.getSparklingWaterVersion()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialiseren van de Numerai API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialiseer de Numerai API client\n",
    "# Voor het indienen van voorspellingen zijn API keys nodig\n",
    "# napi = NumerAPI(public_id=\"UW_PUBLIC_ID\", secret_key=\"UW_SECRET_KEY\")\n",
    "napi = NumerAPI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data downloaden en laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Gebruik een van de nieuwste dataversies\n",
    "DATA_VERSION = \"v5.0\"\n",
    "\n",
    "# Maak een data directory\n",
    "!mkdir -p {DATA_VERSION}\n",
    "\n",
    "# Download data\n",
    "print(\"Downloading training data...\")\n",
    "napi.download_dataset(f\"{DATA_VERSION}/train.parquet\")\n",
    "napi.download_dataset(f\"{DATA_VERSION}/features.json\")\n",
    "\n",
    "# Laad feature metadata\n",
    "feature_metadata = json.load(open(f\"{DATA_VERSION}/features.json\"))\n",
    "print(\"Available feature sets:\", list(feature_metadata[\"feature_sets\"].keys()))\n",
    "features = feature_metadata[\"feature_sets\"][\"small\"]  # gebruik \"small\" voor sneller testen, \"medium\" of \"all\" voor betere prestaties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data laden met PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Laad trainingsdata met Spark\n",
    "print(\"Loading training data with Spark...\")\n",
    "train_spark = spark.read.parquet(f\"{DATA_VERSION}/train.parquet\")\n",
    "\n",
    "# Selecteer alleen de benodigde kolommen\n",
    "columns_to_select = [\"era\"] + features + [\"target\"]\n",
    "train_spark = train_spark.select(*columns_to_select)\n",
    "\n",
    "# Downsampling voor snelheid (optioneel)\n",
    "print(\"Preparing data for training...\")\n",
    "# Haal unieke era's op en sample 25% (elke 4e era)\n",
    "unique_eras = [row.era for row in train_spark.select(\"era\").distinct().collect()]\n",
    "sampled_eras = unique_eras[::4]\n",
    "train_spark = train_spark.filter(col(\"era\").isin(sampled_eras))\n",
    "\n",
    "# Bekijk de data\n",
    "print(f\"Training data count: {train_spark.count()}\")\n",
    "print(f\"Number of features: {len(features)}\")\n",
    "print(f\"Number of eras: {len(sampled_eras)}\")\n",
    "\n",
    "# Toon schema\n",
    "train_spark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data voorbereiden met PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Bereid data voor met Spark ML Pipeline\n",
    "print(\"Preparing feature vector with Spark...\")\n",
    "\n",
    "# Maak een feature vector van alle features\n",
    "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "train_spark = assembler.transform(train_spark)\n",
    "\n",
    "# Toon een voorbeeld van de getransformeerde data\n",
    "train_spark.select(\"era\", \"features\", \"target\").show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converteren van Spark DataFrame naar H2O Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Converteer Spark DataFrame naar H2O Frame\n",
    "print(\"Converting Spark DataFrame to H2O Frame...\")\n",
    "train_h2o = h2o_context.asH2OFrame(train_spark)\n",
    "\n",
    "# Bekijk H2O Frame info\n",
    "train_h2o.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model trainen met H2O XGBoost via Sparkling Water"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train model met H2O XGBoost via Sparkling Water\n",
    "print(\"Training H2O XGBoost model via Sparkling Water...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Configureer XGBoost model\n",
    "from h2o.estimators.xgboost import H2OXGBoostEstimator\n",
    "\n",
    "xgb_model = H2OXGBoostEstimator(\n",
    "    ntrees=2000,\n",
    "    max_depth=5,\n",
    "    learn_rate=0.01,\n",
    "    sample_rate=0.8,\n",
    "    col_sample_rate=0.8,\n",
    "    tree_method=\"auto\",  # auto selecteert GPU indien beschikbaar\n",
    "    booster=\"gbtree\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Train het model\n",
    "xgb_model.train(x=features, y=\"target\", training_frame=train_h2o)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "# Toon model informatie\n",
    "print(xgb_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance visualiseren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Feature importance visualiseren\n",
    "feature_importance = xgb_model.varimp(use_pandas=True)\n",
    "if feature_importance is not None:\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.barh(range(len(feature_importance[:20])), feature_importance[:20]['relative_importance'])\n",
    "    plt.yticks(range(len(feature_importance[:20])), feature_importance[:20]['variable'])\n",
    "    plt.title('H2O XGBoost Feature Importance (top 20)')\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model opslaan als MOJO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Sla het model op als MOJO (Model Object, Optimized)\n",
    "mojo_path = xgb_model.download_mojo(path=\"./\", get_genmodel_jar=True)\n",
    "print(f\"Model saved as MOJO: {mojo_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validatiedata laden en voorbereiden met PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Download validatiedata voor testen\n",
    "print(\"Downloading validation data for testing...\")\n",
    "napi.download_dataset(f\"{DATA_VERSION}/validation.parquet\")\n",
    "\n",
    "# Laad validatiedata met Spark\n",
    "print(\"Loading validation data with Spark...\")\n",
    "validation_spark = spark.read.parquet(f\"{DATA_VERSION}/validation.parquet\")\n",
    "\n",
    "# Selecteer alleen de benodigde kolommen\n",
    "columns_to_select = [\"era\", \"data_type\"] + features\n",
    "validation_spark = validation_spark.select(*columns_to_select)\n",
    "\n",
    "# Filter alleen validatie data\n",
    "validation_spark = validation_spark.filter(col(\"data_type\") == \"validation\")\n",
    "\n",
    "# Neem een kleine subset voor geheugeneffici\u00ebntie\n",
    "validation_spark = validation_spark.limit(1000)\n",
    "\n",
    "# Maak een feature vector van alle features\n",
    "validation_spark = assembler.transform(validation_spark)\n",
    "\n",
    "# Converteer Spark DataFrame naar H2O Frame\n",
    "validation_h2o = h2o_context.asH2OFrame(validation_spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voorspellingen maken met het model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Maak voorspellingen met het model\n",
    "print(\"Making predictions...\")\n",
    "predictions_h2o = xgb_model.predict(validation_h2o)\n",
    "\n",
    "# Converteer H2O Frame terug naar Spark DataFrame\n",
    "predictions_spark = h2o_context.asSparkFrame(predictions_h2o)\n",
    "\n",
    "# Toon voorspellingen\n",
    "print(\"Sample predictions:\")\n",
    "predictions_spark.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voorspellingsfunctie defini\u00ebren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Definieer voorspellingsfunctie die werkt met H2O model\n",
    "def predict(\n",
    "    live_features: pd.DataFrame,\n",
    "    live_benchmark_models: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    # Converteer pandas DataFrame naar Spark DataFrame\n",
    "    live_features_spark = spark.createDataFrame(live_features[features])\n",
    "    \n",
    "    # Maak een feature vector van alle features\n",
    "    live_features_spark = assembler.transform(live_features_spark)\n",
    "    \n",
    "    # Converteer Spark DataFrame naar H2O Frame\n",
    "    live_features_h2o = h2o_context.asH2OFrame(live_features_spark)\n",
    "    \n",
    "    # Maak voorspellingen met het H2O model\n",
    "    preds = xgb_model.predict(live_features_h2o)\n",
    "    \n",
    "    # Converteer H2O voorspellingen terug naar pandas\n",
    "    predictions = h2o.as_list(preds)[\"predict\"].values\n",
    "    \n",
    "    # Maak submission DataFrame\n",
    "    submission = pd.Series(predictions, index=live_features.index)\n",
    "    return submission.to_frame(\"prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voorspellingsfunctie testen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Converteer Spark DataFrame terug naar pandas voor testen\n",
    "validation_pd = validation_spark.toPandas()\n",
    "\n",
    "# Test voorspellingsfunctie\n",
    "print(\"Testing prediction function...\")\n",
    "# Maak een lege DataFrame voor benchmark_models (niet gebruikt in onze voorspellingsfunctie)\n",
    "empty_benchmark = pd.DataFrame(index=validation_pd.index)\n",
    "predictions = predict(validation_pd, empty_benchmark)\n",
    "\n",
    "print(f\"Predictions shape: {predictions.shape}\")\n",
    "print(\"\\nSample predictions:\")\n",
    "print(predictions.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voorspellingsfunctie opslaan met cloudpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Pickle voorspellingsfunctie\n",
    "print(\"Saving prediction function with cloudpickle...\")\n",
    "p = cloudpickle.dumps(predict)\n",
    "with open(\"numerai_sparkling_water_model.pkl\", \"wb\") as f:\n",
    "    f.write(p)\n",
    "\n",
    "print(\"Prediction function saved as 'numerai_sparkling_water_model.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle specifieke functies voor het opslaan van resultaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Opslaan van resultaten in Kaggle output\n",
    "# Dit maakt het mogelijk om de resultaten te downloaden of als dataset te gebruiken\n",
    "try:\n",
    "    # Maak een output directory\n",
    "    !mkdir -p /kaggle/working/output\n",
    "    \n",
    "    # Kopieer de belangrijke bestanden\n",
    "    !cp numerai_sparkling_water_model.pkl /kaggle/working/output/\n",
    "    !cp {mojo_path} /kaggle/working/output/\n",
    "    \n",
    "    print(\"Model bestanden opgeslagen in Kaggle output directory\")\n",
    "except Exception as e:\n",
    "    print(f\"Fout bij opslaan in Kaggle output: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voordelen van Sparkling Water"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Hier zou je een vergelijking kunnen maken tussen standaard H2O en Sparkling Water\n",
    "print(\"Sparkling Water Voordelen:\")\n",
    "print(\"1. Gedistribueerde verwerking met Spark voor grote datasets\")\n",
    "print(\"2. Combinatie van Spark's data processing met H2O's machine learning algoritmes\")\n",
    "print(\"3. Betere schaalbaarheid voor complexe modellen en grote datasets\")\n",
    "print(\"4. Mogelijkheid om Spark ML Pipeline te integreren met H2O modellen\")\n",
    "print(f\"5. Onze training duurde {training_time:.2f} seconden met Sparkling Water\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Afsluiten van Spark en H2O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Sluit H2O cluster af\n",
    "h2o.cluster().shutdown()\n",
    "\n",
    "# Sluit Spark sessie af\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Financial Modeling Prep API Integration\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "FMP_API_KEY = \"aDFEO9rxgvGL3VQgPcBxXblSZ3laRLap\"\n",
    "DEEPSEEK_API_KEY = \"sk-6a3502649b0048259e0009a328c71960\"\n",
    "\n",
    "# Function to get economic indicators from Financial Modeling Prep\n",
    "def get_economic_indicators():\n",
    "    url = f\"https://financialmodelingprep.com/api/v3/economic/economic_indicators?apikey={FMP_API_KEY}\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Get country and currency data\n",
    "def get_country_currency_data():\n",
    "    url = f\"https://financialmodelingprep.com/api/v3/fx?apikey={FMP_API_KEY}\"\n",
    "    response = requests.get(url)\n",
    "    fx_data = response.json()\n",
    "    \n",
    "    # Get country profiles for ISO codes\n",
    "    url = f\"https://financialmodelingprep.com/api/v4/country_list?apikey={FMP_API_KEY}\"\n",
    "    response = requests.get(url)\n",
    "    country_data = response.json()\n",
    "    \n",
    "    # Create comprehensive country-currency mapping\n",
    "    country_df = pd.DataFrame(country_data)\n",
    "    fx_df = pd.DataFrame(fx_data)\n",
    "    \n",
    "    # Extract currency codes from FX pairs\n",
    "    currency_codes = set()\n",
    "    for pair in fx_df[\"ticker\"].values:\n",
    "        if \"/\" in pair:\n",
    "            base, quote = pair.split(\"/\")\n",
    "            currency_codes.add(base)\n",
    "            currency_codes.add(quote)\n",
    "    \n",
    "    # Create final mapping dataframe\n",
    "    mapping_data = []\n",
    "    for country in country_df.to_dict(\"records\"):\n",
    "        country_name = country.get(\"name\", \"\")\n",
    "        country_code = country.get(\"code\", \"\")\n",
    "        currency_name = country.get(\"currency\", \"\")\n",
    "        currency_code = \"\"\n",
    "        \n",
    "        # Try to find currency code\n",
    "        for code in currency_codes:\n",
    "            if len(code) == 3 and code.upper() in currency_name.upper():\n",
    "                currency_code = code\n",
    "                break\n",
    "        \n",
    "        mapping_data.append({\n",
    "            \"country_name\": country_name,\n",
    "            \"country_code\": country_code,\n",
    "            \"currency_name\": currency_name,\n",
    "            \"currency_code\": currency_code\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(mapping_data)\n",
    "\n",
    "# Get economic indicators\n",
    "try:\n",
    "    economic_indicators = get_economic_indicators()\n",
    "    print(\"Economic Indicators:\")\n",
    "    print(economic_indicators.head())\n",
    "except Exception as e:\n",
    "    print(f\"Error fetching economic indicators: {e}\")\n",
    "\n",
    "# Get country-currency mapping\n",
    "try:\n",
    "    country_currency_mapping = get_country_currency_data()\n",
    "    print(\"\nCountry-Currency Mapping:\")\n",
    "    print(country_currency_mapping.head(20))\n",
    "    \n",
    "    # Save the mapping to CSV\n",
    "    country_currency_mapping.to_csv(\"country_currency_mapping.csv\", index=False)\n",
    "    print(\"\nSaved country-currency mapping to CSV file\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating country-currency mapping: {e}\")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# DeepSeek API Integration for Crypto-Country Association\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def get_crypto_country_associations(cryptocurrencies):\n",
    "    url = \"https://api.deepseek.com/v1/chat/completions\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {DEEPSEEK_API_KEY}\"\n",
    "    }\n",
    "    \n",
    "    crypto_list = \", \".join(cryptocurrencies)\n",
    "    \n",
    "    data = {\n",
    "        \"model\": \"deepseek-chat\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant that provides accurate information about cryptocurrencies.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"For each of these cryptocurrencies: {crypto_list}, provide the country where they have their entity registered or where they primarily report taxes. Return the data in JSON format with cryptocurrency name, country name, and ISO country code.\"\n",
    "            }\n",
    "        ],\n",
    "        \"temperature\": 0.1,\n",
    "        \"max_tokens\": 2000\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=data)\n",
    "        response_data = response.json()\n",
    "        \n",
    "        if \"choices\" in response_data and len(response_data[\"choices\"]) > 0:\n",
    "            content = response_data[\"choices\"][0][\"message\"][\"content\"]\n",
    "            \n",
    "            # Extract JSON from the response\n",
    "            try:\n",
    "                # Try to find JSON in the response\n",
    "                start_idx = content.find(\"{\")\n",
    "                end_idx = content.rfind(\"}\")\n",
    "                \n",
    "                if start_idx != -1 and end_idx != -1:\n",
    "                    json_str = content[start_idx:end_idx+1]\n",
    "                    return json.loads(json_str)\n",
    "                else:\n",
    "                    return {\"error\": \"No JSON found in response\", \"raw_response\": content}\n",
    "            except json.JSONDecodeError:\n",
    "                return {\"error\": \"Failed to parse JSON\", \"raw_response\": content}\n",
    "        else:\n",
    "            return {\"error\": \"No response from DeepSeek API\"}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Example usage\n",
    "cryptocurrencies = [\"Bitcoin\", \"Ethereum\", \"Ripple\", \"Cardano\", \"Solana\"]\n",
    "try:\n",
    "    crypto_country_data = get_crypto_country_associations(cryptocurrencies)\n",
    "    print(\"Cryptocurrency Country Associations:\")\n",
    "    print(json.dumps(crypto_country_data, indent=2))\n",
    "    \n",
    "    # Convert to DataFrame and save\n",
    "    if not isinstance(crypto_country_data, dict) or not crypto_country_data.get(\"error\"):\n",
    "        crypto_df = pd.DataFrame(crypto_country_data)\n",
    "        crypto_df.to_csv(\"crypto_country_associations.csv\", index=False)\n",
    "        print(\"\nSaved cryptocurrency country associations to CSV file\")\n",
    "except Exception as e:\n",
    "    print(f\"Error getting cryptocurrency country associations: {e}\")\n"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}