{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yiedl Crypto Data Analysis and Modeling\n",
    "\n",
    "This notebook demonstrates how to download and analyze Yiedl crypto data, and build models with polynomial feature combinations using XGBoost and LightGBM."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Install required packages\n!pip install -q requests pandas numpy matplotlib sklearn xgboost lightgbm h2o scikit-learn polars numerapi pyarrow",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Yiedl Data\n",
    "\n",
    "We'll download both the latest and historical Yiedl crypto datasets."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import requests\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport zipfile\nfrom sklearn.model_selection import train_test_split\nimport polars as pl\nfrom pathlib import Path\nimport time\n\n# Helper function to download files with retry mechanism\ndef download_file(url, output_filename, max_retries=3, timeout=300):\n    for attempt in range(max_retries):\n        try:\n            print(f\"Download attempt {attempt + 1}/{max_retries}\")\n            response = requests.get(url, stream=True, timeout=timeout)\n            response.raise_for_status()  # Raise an exception for HTTP errors\n            \n            # Get total file size if available\n            total_size = int(response.headers.get('content-length', 0))\n            \n            # Download with progress tracking for large files\n            if total_size > 10 * 1024 * 1024:  # If file is larger than 10MB\n                print(f\"Downloading {output_filename} ({total_size / (1024 * 1024):.1f} MB)\")\n                \n                with open(output_filename, 'wb') as file:\n                    downloaded = 0\n                    start_time = time.time()\n                    last_print_time = start_time\n                    \n                    for chunk in response.iter_content(chunk_size=1024*1024):  # 1MB chunks\n                        if chunk:\n                            file.write(chunk)\n                            downloaded += len(chunk)\n                            \n                            # Update progress every 5 seconds\n                            current_time = time.time()\n                            if current_time - last_print_time > 5:\n                                speed = downloaded / (current_time - start_time) / (1024 * 1024)  # MB/s\n                                percent = 100 * downloaded / total_size if total_size > 0 else 0\n                                print(f\"Progress: {percent:.1f}% ({downloaded / (1024 * 1024):.1f}/{total_size / (1024 * 1024):.1f} MB) - {speed:.1f} MB/s\")\n                                last_print_time = current_time\n            else:\n                # Small file, download without progress tracking\n                with open(output_filename, 'wb') as file:\n                    file.write(response.content)\n            \n            print(f\"File downloaded successfully as {output_filename}\")\n            return True\n            \n        except requests.exceptions.RequestException as e:\n            print(f\"Download failed (attempt {attempt + 1}/{max_retries}): {e}\")\n            if attempt < max_retries - 1:\n                wait_time = 2 ** attempt  # Exponential backoff\n                print(f\"Retrying in {wait_time} seconds...\")\n                time.sleep(wait_time)\n            else:\n                print(\"Max retries exceeded.\")\n                return False\n\n# Create data directory if it doesn't exist\ndata_dir = Path(\"../data/yiedl\")\ndata_dir.mkdir(exist_ok=True, parents=True)\n\n# Download Yiedl latest dataset\nprint(\"Downloading Yiedl latest dataset...\")\nlatest_url = 'https://api.yiedl.ai/yiedl/v1/downloadDataset?type=latest'\nlatest_path = data_dir / \"yiedl_latest.parquet\"\nif not latest_path.exists():\n    download_file(latest_url, latest_path)\nelse:\n    print(f\"File already exists: {latest_path}\")\n\n# Download Yiedl historical dataset (which is a zip file)\nprint(\"\\nDownloading Yiedl historical dataset...\")\nhistorical_url = 'https://api.yiedl.ai/yiedl/v1/downloadDataset?type=historical'\nhistorical_zip_path = data_dir / \"yiedl_historical.zip\"\nhistorical_path = data_dir / \"yiedl_historical.parquet\"\n\nif not historical_path.exists():\n    if not historical_zip_path.exists():\n        download_file(historical_url, historical_zip_path)\n    \n    # Extract the zip file\n    print(\"Extracting historical dataset...\")\n    with zipfile.ZipFile(historical_zip_path, 'r') as zip_ref:\n        zip_files = zip_ref.namelist()\n        if len(zip_files) == 1:\n            # Extract the file with its original name\n            zip_ref.extract(zip_files[0], path=data_dir)\n            # Rename the file to yiedl_historical.parquet\n            extracted_file = data_dir / zip_files[0]\n            os.rename(extracted_file, historical_path)\n        else:\n            print(f\"Unexpected files in zip: {zip_files}\")\n    print(f\"Extracted to {historical_path}\")\nelse:\n    print(f\"File already exists: {historical_path}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Explore the Data\n",
    "\n",
    "Let's load and explore both datasets to understand their structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load datasets using polars for better memory management\n",
    "print(\"Loading latest dataset...\")\n",
    "df_latest = pl.read_parquet(latest_path)\n",
    "\n",
    "# Print basic info about the latest dataset\n",
    "print(f\"\\nLatest dataset shape: {df_latest.shape}\")\n",
    "print(\"\\nLatest dataset columns preview:\")\n",
    "print(df_latest.columns[:10])\n",
    "print(f\"Total columns: {len(df_latest.columns)}\")\n",
    "\n",
    "# Print the first few rows of the latest dataset\n",
    "print(\"\\nLatest dataset sample:\")\n",
    "display(df_latest.head().to_pandas())\n",
    "\n",
    "# For historical data, we'll check if it exists but not load it completely to avoid memory issues\n",
    "if historical_path.exists():\n",
    "    print(\"\\nLoading historical dataset (only metadata)...\")\n",
    "    schema = pl.read_parquet_schema(historical_path)\n",
    "    num_rows = pl.scan_parquet(historical_path).select(pl.count()).collect().item()\n",
    "    print(f\"Historical dataset has approximately {num_rows} rows\")\n",
    "    print(f\"Historical dataset has {len(schema)} columns\")\n",
    "    print(\"\\nHistorical dataset columns preview:\")\n",
    "    print(list(schema.keys())[:10])\n",
    "else:\n",
    "    print(\"Historical dataset not found. Please extract it first.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Let's preprocess the data before creating models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to identify column types based on naming patterns\n",
    "def identify_column_groups(columns):\n",
    "    pvm_cols = [col for col in columns if col.startswith('pvm_')]\n",
    "    sentiment_cols = [col for col in columns if col.startswith('sentiment_')]\n",
    "    onchain_cols = [col for col in columns if col.startswith('onchain_')]\n",
    "    date_symbol_cols = ['date', 'symbol']\n",
    "    other_cols = [col for col in columns if col not in pvm_cols + sentiment_cols + onchain_cols + date_symbol_cols]\n",
    "    \n",
    "    return {\n",
    "        'pvm': pvm_cols,\n",
    "        'sentiment': sentiment_cols,\n",
    "        'onchain': onchain_cols,\n",
    "        'date_symbol': date_symbol_cols,\n",
    "        'other': other_cols\n",
    "    }\n",
    "\n",
    "# Identify column groups\n",
    "column_groups = identify_column_groups(df_latest.columns)\n",
    "\n",
    "# Print summary of column groups\n",
    "print(\"Column Group Summary:\")\n",
    "for group, cols in column_groups.items():\n",
    "    print(f\"{group}: {len(cols)} columns\")\n",
    "\n",
    "# Sample a few columns from each group\n",
    "for group, cols in column_groups.items():\n",
    "    if cols:\n",
    "        print(f\"\\nSample {group} columns: {cols[:5]}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Target Variable\n",
    "\n",
    "Since we do not have a specific target variable for this exercise, we'll create a synthetic one based on the data features. This will allow us to demonstrate the modeling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to create a synthetic target variable for demonstration purposes\n",
    "def create_synthetic_target(df, method='pvm_based'):\n",
    "    if method == 'pvm_based':\n",
    "        # Get a PVM column if available\n",
    "        pvm_cols = column_groups['pvm']\n",
    "        if pvm_cols:\n",
    "            # Use the first PVM column as a basis\n",
    "            pvm_col = pvm_cols[0]\n",
    "            # Convert to pandas for easier manipulation\n",
    "            pvm_series = df.select(pvm_col).to_pandas()[pvm_col]\n",
    "            # Create target: 1 if value > median, 0 otherwise\n",
    "            target = (pvm_series > pvm_series.median()).astype(int)\n",
    "            return target\n",
    "    \n",
    "    # Fallback: create random target\n",
    "    print(\"Using random target as fallback\")\n",
    "    return np.random.randint(0, 2, size=len(df))\n",
    "\n",
    "# Convert to pandas for further processing (using a subset of columns to manage memory)\n",
    "columns_to_use = [\n",
    "    *column_groups['date_symbol'], \n",
    "    *column_groups['pvm'][:50], \n",
    "    *column_groups['sentiment'][:50], \n",
    "    *column_groups['onchain'][:50]\n",
    "]\n",
    "\n",
    "print(f\"Using {len(columns_to_use)} columns for analysis\")\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df_pandas = df_latest.select(columns_to_use).to_pandas()\n",
    "\n",
    "# Create synthetic target\n",
    "df_pandas['target'] = create_synthetic_target(df_latest)\n",
    "\n",
    "# Check class distribution\n",
    "print(\"\\nTarget distribution:\")\n",
    "print(df_pandas['target'].value_counts())\n",
    "\n",
    "# Count of unique symbols\n",
    "print(f\"\\nNumber of unique symbols: {df_pandas['symbol'].nunique()}\")\n",
    "\n",
    "# Display a few rows with target\n",
    "print(\"\\nSample rows with target:\")\n",
    "display(df_pandas[['date', 'symbol', 'target']].head(10))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering with Polynomial Combinations\n",
    "\n",
    "Let's create polynomial features and interactions between the different data types"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from sklearn.preprocessing import PolynomialFeatures\nimport warnings\nimport gc  # For garbage collection\nwarnings.filterwarnings('ignore')\n\n# Function to create polynomial features for specific column groups\ndef create_polynomial_features(df, feature_cols, degree=2, interaction_only=False, max_features=100):\n    # Fill NaN values with 0 for the feature columns\n    feature_df = df[feature_cols].copy().fillna(0)\n    \n    # Limit the number of features to avoid memory issues\n    if len(feature_cols) > max_features:\n        print(f\"Limiting features from {len(feature_cols)} to {max_features}\")\n        feature_cols = feature_cols[:max_features]\n        feature_df = feature_df[feature_cols]\n    \n    # Create polynomial features\n    poly = PolynomialFeatures(degree=degree, interaction_only=interaction_only, include_bias=False)\n    poly_features = poly.fit_transform(feature_df)\n    \n    # Create feature names\n    feature_names = poly.get_feature_names_out(feature_cols)\n    \n    # Convert to DataFrame\n    poly_df = pd.DataFrame(poly_features, columns=feature_names, index=df.index)\n    \n    # Clean up to free memory\n    del feature_df, poly_features\n    gc.collect()\n    \n    return poly_df\n\n# Function to process features in batches to manage memory\ndef process_features_in_batches(df, feature_cols, batch_size=20, degree=2, interaction_only=True):\n    all_batches = []\n    # Process features in batches\n    for i in range(0, len(feature_cols), batch_size):\n        batch_end = min(i + batch_size, len(feature_cols))\n        print(f\"Processing batch {i//batch_size + 1}: features {i+1}-{batch_end}\")\n        batch_features = feature_cols[i:batch_end]\n        \n        batch_poly = create_polynomial_features(\n            df, batch_features, degree=degree, \n            interaction_only=interaction_only, max_features=batch_size\n        )\n        \n        all_batches.append(batch_poly)\n        \n        # Force garbage collection to free memory\n        gc.collect()\n    \n    # Combine all batches\n    if all_batches:\n        combined_df = pd.concat(all_batches, axis=1)\n        # Clean up to free memory\n        del all_batches\n        gc.collect()\n        return combined_df\n    else:\n        return pd.DataFrame(index=df.index)\n\n# Create polynomial features for each group\nprint(\"Creating polynomial features...\")\n\n# For PVM features\npvm_features = df_pandas.columns[df_pandas.columns.str.startswith('pvm_')].tolist()\nif pvm_features:\n    print(f\"Creating polynomial features for {len(pvm_features)} PVM features\")\n    pvm_poly_df = process_features_in_batches(df_pandas, pvm_features, batch_size=20, degree=2, interaction_only=True)\n    print(f\"Created {pvm_poly_df.shape[1]} polynomial PVM features\")\nelse:\n    pvm_poly_df = pd.DataFrame(index=df_pandas.index)\n    print(\"No PVM features found\")\n\n# For sentiment features\nsentiment_features = df_pandas.columns[df_pandas.columns.str.startswith('sentiment_')].tolist()\nif sentiment_features:\n    print(f\"Creating polynomial features for {len(sentiment_features)} sentiment features\")\n    sentiment_poly_df = process_features_in_batches(df_pandas, sentiment_features, batch_size=20, degree=2, interaction_only=True)\n    print(f\"Created {sentiment_poly_df.shape[1]} polynomial sentiment features\")\nelse:\n    sentiment_poly_df = pd.DataFrame(index=df_pandas.index)\n    print(\"No sentiment features found\")\n\n# For onchain features\nonchain_features = df_pandas.columns[df_pandas.columns.str.startswith('onchain_')].tolist()\nif onchain_features:\n    print(f\"Creating polynomial features for {len(onchain_features)} onchain features\")\n    onchain_poly_df = process_features_in_batches(df_pandas, onchain_features, batch_size=20, degree=2, interaction_only=True)\n    print(f\"Created {onchain_poly_df.shape[1]} polynomial onchain features\")\nelse:\n    onchain_poly_df = pd.DataFrame(index=df_pandas.index)\n    print(\"No onchain features found\")\n\n# Create cross-category interaction features\nprint(\"\\nCreating cross-category interaction features...\")\ncross_features = []\nif pvm_features and sentiment_features:\n    # Select a subset of features from each category\n    pvm_subset = pvm_features[:10] if len(pvm_features) > 10 else pvm_features\n    sentiment_subset = sentiment_features[:10] if len(sentiment_features) > 10 else sentiment_features\n    \n    # Combine the subsets and create cross features\n    combined_features = pvm_subset + sentiment_subset\n    cross_pvm_sentiment = create_polynomial_features(\n        df_pandas, combined_features, degree=2, interaction_only=True, max_features=40\n    )\n    \n    # Only keep interactions between different categories\n    cross_cols = [col for col in cross_pvm_sentiment.columns \n                 if any(p in col for p in pvm_subset) and any(s in col for s in sentiment_subset)]\n    \n    if cross_cols:\n        cross_features.append(cross_pvm_sentiment[cross_cols])\n        print(f\"Created {len(cross_cols)} PVM-Sentiment interaction features\")\n    \n    # Clean up to free memory\n    del cross_pvm_sentiment\n    gc.collect()\n\n# Combine all feature sets\nfeature_dfs = [pvm_poly_df, sentiment_poly_df, onchain_poly_df] + cross_features\nall_poly_features = pd.concat(feature_dfs, axis=1)\n\nprint(f\"\\nTotal polynomial features created: {all_poly_features.shape[1]}\")\n\n# Clean up individual feature dataframes to free memory\ndel pvm_poly_df, sentiment_poly_df, onchain_poly_df, cross_features\ngc.collect()\n\n# Combine with original features for modeling\nmodeling_df = pd.concat([df_pandas.drop(['date', 'symbol'], axis=1), all_poly_features], axis=1)\n\n# Remove any constant columns to reduce dimensionality\nmodeling_df = modeling_df.loc[:, (modeling_df != modeling_df.iloc[0]).any()]\n\nprint(f\"\\nFinal dataset shape for modeling: {modeling_df.shape}\")\nprint(f\"Number of features: {modeling_df.shape[1] - 1}\")\n\n# Free up memory\ndel all_poly_features\ngc.collect()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split\n",
    "\n",
    "Let's split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Split the data\n",
    "X = modeling_df.drop('target', axis=1)\n",
    "y = modeling_df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Testing set: {X_test.shape}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train XGBoost Model\n",
    "\n",
    "Let's train an XGBoost model using the polynomial features."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import xgboost as xgb\nfrom sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n\n# Create DMatrix objects for XGBoost\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_test, label=y_test)\n\n# Set XGBoost parameters with GPU acceleration\nparams = {\n    'objective': 'binary:logistic',\n    'eval_metric': 'auc',\n    'max_depth': 6,\n    'eta': 0.1,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'min_child_weight': 1,\n    'tree_method': 'gpu_hist',  # Use GPU acceleration\n    'gpu_id': 0,  # Use first GPU\n    'seed': 42\n}\n\n# Check if GPU is available\ntry:\n    # Train XGBoost model with early stopping\n    print(\"Training XGBoost model with GPU acceleration...\")\n    watchlist = [(dtrain, 'train'), (dtest, 'test')]\n    xgb_model = xgb.train(\n        params, \n        dtrain, \n        num_boost_round=1000,\n        evals=watchlist,\n        early_stopping_rounds=50,\n        verbose_eval=100\n    )\nexcept Exception as e:\n    print(f\"GPU training failed with error: {e}\")\n    print(\"Falling back to CPU training...\")\n    params['tree_method'] = 'hist'  # Fall back to CPU\n    params.pop('gpu_id', None)  # Remove GPU parameter\n    xgb_model = xgb.train(\n        params, \n        dtrain, \n        num_boost_round=1000,\n        evals=watchlist,\n        early_stopping_rounds=50,\n        verbose_eval=100\n    )\n\n# Make predictions\ny_pred_proba = xgb_model.predict(dtest)\ny_pred = (y_pred_proba > 0.5).astype(int)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nauc = roc_auc_score(y_test, y_pred_proba)\n\nprint(f\"\\nXGBoost Model Performance:\")\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"AUC: {auc:.4f}\")\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred))\n\n# Get feature importance\nimportance = xgb_model.get_score(importance_type='gain')\nimportance_df = pd.DataFrame({'Feature': list(importance.keys()), 'Importance': list(importance.values())})\nimportance_df = importance_df.sort_values('Importance', ascending=False).reset_index(drop=True)\n\n# Plot feature importance\nplt.figure(figsize=(14, 8))\nplt.barh(importance_df['Feature'][:20], importance_df['Importance'][:20])\nplt.title('XGBoost Feature Importance (Top 20)')\nplt.xlabel('Importance')\nplt.gca().invert_yaxis()\nplt.tight_layout()\nplt.show()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LightGBM Model\n",
    "\n",
    "Let's also train a LightGBM model and compare performance."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import lightgbm as lgb\n\n# Create LightGBM datasets\ntrain_data = lgb.Dataset(X_train, label=y_train)\ntest_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n\n# Set LightGBM parameters with GPU acceleration\nparams = {\n    'objective': 'binary',\n    'metric': 'auc',\n    'boosting_type': 'gbdt',\n    'num_leaves': 31,\n    'learning_rate': 0.05,\n    'feature_fraction': 0.9,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 5,\n    'verbose': -1,\n    'device': 'gpu',  # Use GPU\n    'gpu_platform_id': 0,\n    'gpu_device_id': 0,\n    'seed': 42\n}\n\n# Try training with GPU, fall back to CPU if not available\ntry:\n    # Train LightGBM model with early stopping\n    print(\"Training LightGBM model with GPU acceleration...\")\n    lgb_model = lgb.train(\n        params,\n        train_data,\n        num_boost_round=1000,\n        valid_sets=[train_data, test_data],\n        valid_names=['train', 'test'],\n        early_stopping_rounds=50,\n        verbose_eval=100\n    )\nexcept Exception as e:\n    print(f\"GPU training failed with error: {e}\")\n    print(\"Falling back to CPU training...\")\n    # Remove GPU parameters\n    params['device'] = 'cpu'\n    params.pop('gpu_platform_id', None)\n    params.pop('gpu_device_id', None)\n    \n    lgb_model = lgb.train(\n        params,\n        train_data,\n        num_boost_round=1000,\n        valid_sets=[train_data, test_data],\n        valid_names=['train', 'test'],\n        early_stopping_rounds=50,\n        verbose_eval=100\n    )\n\n# Make predictions\ny_pred_proba_lgb = lgb_model.predict(X_test, num_iteration=lgb_model.best_iteration)\ny_pred_lgb = (y_pred_proba_lgb > 0.5).astype(int)\n\n# Evaluate the model\naccuracy_lgb = accuracy_score(y_test, y_pred_lgb)\nauc_lgb = roc_auc_score(y_test, y_pred_proba_lgb)\n\nprint(f\"\\nLightGBM Model Performance:\")\nprint(f\"Accuracy: {accuracy_lgb:.4f}\")\nprint(f\"AUC: {auc_lgb:.4f}\")\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred_lgb))\n\n# Plot feature importance\nplt.figure(figsize=(14, 8))\nlgb.plot_importance(lgb_model, max_num_features=20, importance_type='gain')\nplt.title('LightGBM Feature Importance (Top 20)')\nplt.tight_layout()\nplt.show()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Model Performance\n",
    "\n",
    "Let's compare the performance of XGBoost and LightGBM models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a comparison table\n",
    "model_comparison = pd.DataFrame({\n",
    "    'Model': ['XGBoost', 'LightGBM'],\n",
    "    'Accuracy': [accuracy, accuracy_lgb],\n",
    "    'AUC': [auc, auc_lgb]\n",
    "})\n",
    "\n",
    "print(\"Model Performance Comparison:\")\n",
    "display(model_comparison)\n",
    "\n",
    "# Plot ROC curves\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Get ROC curve data\n",
    "fpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_pred_proba)\n",
    "fpr_lgb, tpr_lgb, _ = roc_curve(y_test, y_pred_proba_lgb)\n",
    "\n",
    "# Plot ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr_xgb, tpr_xgb, label=f'XGBoost (AUC = {auc:.4f})')\n",
    "plt.plot(fpr_lgb, tpr_lgb, label=f'LightGBM (AUC = {auc_lgb:.4f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Comparison')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis\n",
    "\n",
    "Let's analyze the most important features from both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get LightGBM feature importance\n",
    "lgb_importance = pd.DataFrame({\n",
    "    'Feature': lgb_model.feature_name(),\n",
    "    'Importance': lgb_model.feature_importance(importance_type='gain')\n",
    "})\n",
    "lgb_importance = lgb_importance.sort_values('Importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Compare top features from both models\n",
    "top_features_xgb = set(importance_df['Feature'][:10])\n",
    "top_features_lgb = set(lgb_importance['Feature'][:10])\n",
    "common_features = top_features_xgb.intersection(top_features_lgb)\n",
    "\n",
    "print(f\"Number of common top features: {len(common_features)}\")\n",
    "print(\"Common top features:\")\n",
    "for feature in common_features:\n",
    "    print(f\"- {feature}\")\n",
    "\n",
    "# Analyze feature types in top features\n",
    "def categorize_feature(feature):\n",
    "    if any(feature.startswith(prefix) for prefix in ['pvm_', 'pvm']):\n",
    "        return 'PVM'\n",
    "    elif any(feature.startswith(prefix) for prefix in ['sentiment_', 'sentiment']):\n",
    "        return 'Sentiment'\n",
    "    elif any(feature.startswith(prefix) for prefix in ['onchain_', 'onchain']):\n",
    "        return 'Onchain'\n",
    "    elif ' ' in feature:  # Interaction feature\n",
    "        return 'Interaction'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Categorize top features\n",
    "xgb_top_categorized = importance_df.head(20).copy()\n",
    "xgb_top_categorized['Category'] = xgb_top_categorized['Feature'].apply(categorize_feature)\n",
    "\n",
    "lgb_top_categorized = lgb_importance.head(20).copy()\n",
    "lgb_top_categorized['Category'] = lgb_top_categorized['Feature'].apply(categorize_feature)\n",
    "\n",
    "# Count feature types\n",
    "print(\"\\nXGBoost top feature categories:\")\n",
    "print(xgb_top_categorized['Category'].value_counts())\n",
    "\n",
    "print(\"\\nLightGBM top feature categories:\")\n",
    "print(lgb_top_categorized['Category'].value_counts())"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Models for Future Use\n",
    "\n",
    "Let's save the trained models for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "models_dir = Path(\"../models/yiedl\")\n",
    "models_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Save XGBoost model\n",
    "xgb_model.save_model(models_dir / \"xgboost_model.json\")\n",
    "print(f\"XGBoost model saved to {models_dir / 'xgboost_model.json'}\")\n",
    "\n",
    "# Save LightGBM model\n",
    "lgb_model.save_model(str(models_dir / \"lightgbm_model.txt\"))\n",
    "print(f\"LightGBM model saved to {models_dir / 'lightgbm_model.txt'}\")\n",
    "\n",
    "# Save feature columns\n",
    "joblib.dump(X.columns.tolist(), models_dir / \"feature_columns.joblib\")\n",
    "print(f\"Feature columns saved to {models_dir / 'feature_columns.joblib'}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "1. Downloaded and explored the Yiedl crypto datasets\n",
    "2. Created polynomial feature combinations\n",
    "3. Built XGBoost and LightGBM models\n",
    "4. Compared model performance\n",
    "5. Analyzed feature importance\n",
    "\n",
    "The models performed well on our synthetic target variable, and we identified important features for predicting the target."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}