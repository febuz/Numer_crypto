{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fc5ed14b-c595-4d3a-86e6-e5937bbeb6dd",
      "metadata": {
        "id": "fc5ed14b-c595-4d3a-86e6-e5937bbeb6dd"
      },
      "source": [
        "## Download the Numerai Crypto Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sNmpaUCohLZv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sNmpaUCohLZv",
        "outputId": "c25385c9-3694-4bdb-a2ca-64b84f39fb93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: h2o in /home/knight1/numerai_EH/lib/python3.10/site-packages (3.46.0.6)\n",
            "Requirement already satisfied: requests in /home/knight1/numerai_EH/lib/python3.10/site-packages (from h2o) (2.32.3)\n",
            "Requirement already satisfied: tabulate in /home/knight1/numerai_EH/lib/python3.10/site-packages (from h2o) (0.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from requests->h2o) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from requests->h2o) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from requests->h2o) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from requests->h2o) (2024.12.14)\n",
            "Requirement already satisfied: pyspark==3.5.0 in /home/knight1/numerai_EH/lib/python3.10/site-packages (3.5.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from pyspark==3.5.0) (0.10.9.7)\n",
            "Requirement already satisfied: numerapi in /home/knight1/numerai_EH/lib/python3.10/site-packages (2.18.0)\n",
            "Requirement already satisfied: requests in /home/knight1/numerai_EH/lib/python3.10/site-packages (2.32.3)\n",
            "Requirement already satisfied: pyarrow in /home/knight1/numerai_EH/lib/python3.10/site-packages (18.0.0)\n",
            "Requirement already satisfied: fastparquet in /home/knight1/numerai_EH/lib/python3.10/site-packages (2024.2.0)\n",
            "Requirement already satisfied: pytz in /home/knight1/numerai_EH/lib/python3.10/site-packages (from numerapi) (2024.2)\n",
            "Requirement already satisfied: python-dateutil in /home/knight1/numerai_EH/lib/python3.10/site-packages (from numerapi) (2.9.0.post0)\n",
            "Requirement already satisfied: tqdm>=4.29.1 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from numerapi) (4.67.1)\n",
            "Requirement already satisfied: click>=7.0 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from numerapi) (8.1.8)\n",
            "Requirement already satisfied: pandas>=1.1.0 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from numerapi) (2.2.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from requests) (2024.12.14)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from fastparquet) (1.24.4)\n",
            "Requirement already satisfied: cramjam>=2.3 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from fastparquet) (2.8.3)\n",
            "Requirement already satisfied: fsspec in /home/knight1/numerai_EH/lib/python3.10/site-packages (from fastparquet) (2024.3.1)\n",
            "Requirement already satisfied: packaging in /home/knight1/numerai_EH/lib/python3.10/site-packages (from fastparquet) (24.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from pandas>=1.1.0->numerapi) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from python-dateutil->numerapi) (1.17.0)\n",
            "Requirement already satisfied: findspark in /home/knight1/numerai_EH/lib/python3.10/site-packages (2.0.1)\n",
            "Requirement already satisfied: h2o-pysparkling-3.5 in /home/knight1/numerai_EH/lib/python3.10/site-packages (3.46.0.6.post1)\n",
            "Requirement already satisfied: requests in /home/knight1/numerai_EH/lib/python3.10/site-packages (from h2o-pysparkling-3.5) (2.32.3)\n",
            "Requirement already satisfied: tabulate in /home/knight1/numerai_EH/lib/python3.10/site-packages (from h2o-pysparkling-3.5) (0.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from requests->h2o-pysparkling-3.5) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from requests->h2o-pysparkling-3.5) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from requests->h2o-pysparkling-3.5) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from requests->h2o-pysparkling-3.5) (2024.12.14)\n",
            "Requirement already satisfied: torch in /home/knight1/numerai_EH/lib/python3.10/site-packages (2.5.1)\n",
            "Requirement already satisfied: filelock in /home/knight1/numerai_EH/lib/python3.10/site-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /home/knight1/numerai_EH/lib/python3.10/site-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /home/knight1/numerai_EH/lib/python3.10/site-packages (from torch) (2024.3.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: pyarrow in /home/knight1/numerai_EH/lib/python3.10/site-packages (18.0.0)\n",
            "Requirement already satisfied: xgboost in /home/knight1/numerai_EH/lib/python3.10/site-packages (2.1.3)\n",
            "Requirement already satisfied: numpy in /home/knight1/numerai_EH/lib/python3.10/site-packages (from xgboost) (1.24.4)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /home/knight1/numerai_EH/lib/python3.10/site-packages (from xgboost) (1.15.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://pypi.nvidia.com\n",
            "Requirement already satisfied: cudf-cu12 in /home/knight1/numerai_EH/lib/python3.10/site-packages (24.12.0)\n",
            "Requirement already satisfied: cachetools in /home/knight1/numerai_EH/lib/python3.10/site-packages (from cudf-cu12) (5.5.0)\n",
            "Requirement already satisfied: cuda-python<13.0a0,<=12.6.0,>=12.0 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from cudf-cu12) (12.6.0)\n",
            "Requirement already satisfied: cupy-cuda12x>=12.0.0 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from cudf-cu12) (13.3.0)\n",
            "Requirement already satisfied: fsspec>=0.6.0 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from cudf-cu12) (2024.3.1)\n",
            "Requirement already satisfied: libcudf-cu12==24.12.* in /home/knight1/numerai_EH/lib/python3.10/site-packages (from cudf-cu12) (24.12.0)\n",
            "Requirement already satisfied: numba-cuda<0.0.18,>=0.0.13 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from cudf-cu12) (0.0.17.1)\n",
            "Requirement already satisfied: numpy<3.0a0,>=1.23 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from cudf-cu12) (1.24.4)\n",
            "Requirement already satisfied: nvtx>=0.2.1 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from cudf-cu12) (0.2.10)\n",
            "Requirement already satisfied: packaging in /home/knight1/numerai_EH/lib/python3.10/site-packages (from cudf-cu12) (24.2)\n",
            "Requirement already satisfied: pandas<2.2.4dev0,>=2.0 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from cudf-cu12) (2.2.3)\n",
            "Requirement already satisfied: pyarrow<19.0.0a0,>=14.0.0 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from cudf-cu12) (18.0.0)\n",
            "Requirement already satisfied: pylibcudf-cu12==24.12.* in /home/knight1/numerai_EH/lib/python3.10/site-packages (from cudf-cu12) (24.12.0)\n",
            "Requirement already satisfied: pynvjitlink-cu12 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from cudf-cu12) (0.4.0)\n",
            "Requirement already satisfied: rich in /home/knight1/numerai_EH/lib/python3.10/site-packages (from cudf-cu12) (13.9.4)\n",
            "Requirement already satisfied: rmm-cu12==24.12.* in /home/knight1/numerai_EH/lib/python3.10/site-packages (from cudf-cu12) (24.12.1)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from cudf-cu12) (4.12.2)\n",
            "Requirement already satisfied: libkvikio-cu12==24.12.* in /home/knight1/numerai_EH/lib/python3.10/site-packages (from libcudf-cu12==24.12.*->cudf-cu12) (24.12.1)\n",
            "Requirement already satisfied: nvidia-nvcomp-cu12==4.1.0.6 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from libcudf-cu12==24.12.*->cudf-cu12) (4.1.0.6)\n",
            "Requirement already satisfied: numba>=0.57 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from rmm-cu12==24.12.*->cudf-cu12) (0.61.0)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from cupy-cuda12x>=12.0.0->cudf-cu12) (0.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from pandas<2.2.4dev0,>=2.0->cudf-cu12) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from pandas<2.2.4dev0,>=2.0->cudf-cu12) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from pandas<2.2.4dev0,>=2.0->cudf-cu12) (2025.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from rich->cudf-cu12) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from rich->cudf-cu12) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->cudf-cu12) (0.1.2)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from numba>=0.57->rmm-cu12==24.12.*->cudf-cu12) (0.44.0)\n",
            "Requirement already satisfied: six>=1.5 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas<2.2.4dev0,>=2.0->cudf-cu12) (1.17.0)\n",
            "Requirement already satisfied: rmm-cu12 in /home/knight1/numerai_EH/lib/python3.10/site-packages (24.12.1)\n",
            "Requirement already satisfied: pylibcudf-cu12 in /home/knight1/numerai_EH/lib/python3.10/site-packages (24.12.0)\n",
            "Requirement already satisfied: cuda-python<13.0a0,<=12.6.0,>=12.0 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from rmm-cu12) (12.6.0)\n",
            "Requirement already satisfied: numba>=0.57 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from rmm-cu12) (0.61.0)\n",
            "Requirement already satisfied: numpy<3.0a0,>=1.23 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from rmm-cu12) (1.24.4)\n",
            "Requirement already satisfied: libcudf-cu12==24.12.* in /home/knight1/numerai_EH/lib/python3.10/site-packages (from pylibcudf-cu12) (24.12.0)\n",
            "Requirement already satisfied: nvtx>=0.2.1 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from pylibcudf-cu12) (0.2.10)\n",
            "Requirement already satisfied: packaging in /home/knight1/numerai_EH/lib/python3.10/site-packages (from pylibcudf-cu12) (24.2)\n",
            "Requirement already satisfied: pyarrow<19.0.0a0,>=14.0.0 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from pylibcudf-cu12) (18.0.0)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from pylibcudf-cu12) (4.12.2)\n",
            "Requirement already satisfied: libkvikio-cu12==24.12.* in /home/knight1/numerai_EH/lib/python3.10/site-packages (from libcudf-cu12==24.12.*->pylibcudf-cu12) (24.12.1)\n",
            "Requirement already satisfied: nvidia-nvcomp-cu12==4.1.0.6 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from libcudf-cu12==24.12.*->pylibcudf-cu12) (4.1.0.6)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /home/knight1/numerai_EH/lib/python3.10/site-packages (from numba>=0.57->rmm-cu12) (0.44.0)\n",
            "Modules imported successfully\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "#!apt-get install openjdk-8-jdk-headless -qq > /dev/null ## we use jdk 11\n",
        "!pip install -U h2o\n",
        "!pip install pyspark==3.5.0\n",
        "!pip install numerapi requests pyarrow fastparquet\n",
        "!pip install findspark\n",
        "# Install H2O Sparkling Water - corrected installation\n",
        "!pip install h2o-pysparkling-3.5\n",
        "# Install torch for GPU distribution\n",
        "!pip install torch\n",
        "# Step 1: Downgrade NumPy to the latest 1.x version\n",
        "#!pip install numpy<2\n",
        "\n",
        "!pip install pyarrow xgboost\n",
        "!pip install cudf-cu12 --extra-index-url=https://pypi.nvidia.com\n",
        "!pip install rmm-cu12 pylibcudf-cu12\n",
        "\n",
        "# Step 3: Verify the installation works without errors\n",
        "!python -c \"import numpy; import scipy; from sklearn.base import BaseEstimator; print('Modules imported successfully')\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb95833e-0644-454a-9fae-bfe749930f22",
      "metadata": {
        "id": "eb95833e-0644-454a-9fae-bfe749930f22"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MinimalExample\") \\\n",
        "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41463536-87b9-4e12-923e-c79cf6d4558b",
      "metadata": {
        "id": "41463536-87b9-4e12-923e-c79cf6d4558b"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Imports and Advanced Setup Functions\n",
        "import os\n",
        "import sys\n",
        "import socket\n",
        "import subprocess\n",
        "import traceback\n",
        "import warnings\n",
        "import tempfile\n",
        "import numpy as np\n",
        "import requests\n",
        "import h2o\n",
        "import gc\n",
        "import time\n",
        "from functools import reduce\n",
        "from pyspark.sql import DataFrame\n",
        "\n",
        "# Enhanced GPU and Library Imports\n",
        "try:\n",
        "    import pynvml\n",
        "    NVML_AVAILABLE = True\n",
        "except ImportError:\n",
        "    NVML_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    import torch\n",
        "    TORCH_AVAILABLE = True\n",
        "except ImportError:\n",
        "    TORCH_AVAILABLE = False\n",
        "\n",
        "import builtins\n",
        "from h2o.automl import H2OAutoML\n",
        "from h2o.exceptions import H2ODependencyWarning\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import ArrayType, DoubleType\n",
        "\n",
        "# Advanced Warning and Logging Configuration\n",
        "warnings.filterwarnings('ignore', category=H2ODependencyWarning)\n",
        "warnings.filterwarnings('ignore', message='.*GPU.*')\n",
        "warnings.filterwarnings('ignore', message='.*Arrow.*')\n",
        "\n",
        "def get_system_resources():\n",
        "    \"\"\"Dynamically detect and configure system resources.\"\"\"\n",
        "    config = {\n",
        "        'limit_features': False,\n",
        "        'features_per_group': 50,\n",
        "        'max_runtime_secs': 1200,\n",
        "        'max_runtime_secs_combined': 2400,\n",
        "        'max_models': 100,\n",
        "        'nfolds': 5,\n",
        "        'batch_size': 1000,\n",
        "        'chunk_size': 10000\n",
        "    }\n",
        "\n",
        "    # Detect GPU Resources\n",
        "    if NVML_AVAILABLE:\n",
        "        try:\n",
        "            pynvml.nvmlInit()\n",
        "            gpu_count = pynvml.nvmlDeviceGetCount()\n",
        "            config['gpu_count'] = gpu_count\n",
        "\n",
        "            # Dynamically adjust memory and processing parameters\n",
        "            total_memory = builtins.min(gpu_count * 60, 360)  # Use builtins.min explicitly\n",
        "            config['memory'] = f'{total_memory}g'\n",
        "            config['gpu_memory_reserve'] = f'{builtins.max(8, gpu_count * 4)}g'  # Use builtins.max\n",
        "            config['executor_instances'] = gpu_count\n",
        "        except Exception as e:\n",
        "            print(f\"GPU detection error: {e}\")\n",
        "            config['memory'] = '224g'\n",
        "            config['gpu_memory_reserve'] = '8g'\n",
        "            config['executor_instances'] = gpu_count\n",
        "    else:\n",
        "        config['memory'] = '180g'\n",
        "        config['gpu_memory_reserve'] = '8g'\n",
        "        config['executor_instances'] = gpu_count\n",
        "\n",
        "    return config\n",
        "\n",
        "# Dynamic Configuration\n",
        "CONFIG = get_system_resources()\n",
        "\n",
        "def get_local_network_config():\n",
        "    \"\"\"Advanced network configuration detection.\"\"\"\n",
        "    try:\n",
        "        # Prefer IPv4 local address\n",
        "        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
        "        s.connect((\"8.8.8.8\", 80))\n",
        "        local_ip = s.getsockname()[0]\n",
        "        s.close()\n",
        "\n",
        "        # Get hostname\n",
        "        hostname = socket.gethostname()\n",
        "\n",
        "        return {\n",
        "            'local_ip': local_ip,\n",
        "            'hostname': hostname,\n",
        "            'available_ports': [54321, 8080, 7077]  # Common distributed computing ports\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Network configuration error: {e}\")\n",
        "        return {\n",
        "            'local_ip': '127.0.0.1',\n",
        "            'hostname': 'localhost',\n",
        "            'available_ports': [54321]\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74847652-7567-4641-90a6-1623e9866a1e",
      "metadata": {
        "id": "74847652-7567-4641-90a6-1623e9866a1e",
        "outputId": "67cff26e-356c-4859-f010-cfb5adb653c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
            "Attempting to start a local H2O server...\n",
            "  Java Version: openjdk version \"1.8.0_432\"; OpenJDK Runtime Environment (build 1.8.0_432-8u432-ga~us1-0ubuntu2~22.04-ga); OpenJDK 64-Bit Server VM (build 25.432-bga, mixed mode)\n",
            "  Starting server from /home/knight1/numerai_EH/lib/python3.10/site-packages/h2o/backend/bin/h2o.jar\n",
            "  Ice root: /tmp/tmp6n7fpyg0\n",
            "  JVM stdout: /tmp/tmp6n7fpyg0/h2o_knight1_started_from_python.out\n",
            "  JVM stderr: /tmp/tmp6n7fpyg0/h2o_knight1_started_from_python.err\n",
            "  Server is running at http://127.0.0.1:54321\n",
            "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "\n",
              "#h2o-table-21.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-21 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-21 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-21 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-21 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-21 .h2o-table th,\n",
              "#h2o-table-21 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-21 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-21\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption></caption>\n",
              "    <thead></thead>\n",
              "    <tbody><tr><td>H2O_cluster_uptime:</td>\n",
              "<td>01 secs</td></tr>\n",
              "<tr><td>H2O_cluster_timezone:</td>\n",
              "<td>Europe/Amsterdam</td></tr>\n",
              "<tr><td>H2O_data_parsing_timezone:</td>\n",
              "<td>UTC</td></tr>\n",
              "<tr><td>H2O_cluster_version:</td>\n",
              "<td>3.46.0.6</td></tr>\n",
              "<tr><td>H2O_cluster_version_age:</td>\n",
              "<td>2 months and 22 days</td></tr>\n",
              "<tr><td>H2O_cluster_name:</td>\n",
              "<td>H2O_from_python_knight1_l1z605</td></tr>\n",
              "<tr><td>H2O_cluster_total_nodes:</td>\n",
              "<td>1</td></tr>\n",
              "<tr><td>H2O_cluster_free_memory:</td>\n",
              "<td>26.63 Gb</td></tr>\n",
              "<tr><td>H2O_cluster_total_cores:</td>\n",
              "<td>20</td></tr>\n",
              "<tr><td>H2O_cluster_allowed_cores:</td>\n",
              "<td>20</td></tr>\n",
              "<tr><td>H2O_cluster_status:</td>\n",
              "<td>locked, healthy</td></tr>\n",
              "<tr><td>H2O_connection_url:</td>\n",
              "<td>http://127.0.0.1:54321</td></tr>\n",
              "<tr><td>H2O_connection_proxy:</td>\n",
              "<td>{\"http\": null, \"https\": null}</td></tr>\n",
              "<tr><td>H2O_internal_security:</td>\n",
              "<td>False</td></tr>\n",
              "<tr><td>Python_version:</td>\n",
              "<td>3.10.12 final</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n"
            ],
            "text/plain": [
              "--------------------------  ------------------------------\n",
              "H2O_cluster_uptime:         01 secs\n",
              "H2O_cluster_timezone:       Europe/Amsterdam\n",
              "H2O_data_parsing_timezone:  UTC\n",
              "H2O_cluster_version:        3.46.0.6\n",
              "H2O_cluster_version_age:    2 months and 22 days\n",
              "H2O_cluster_name:           H2O_from_python_knight1_l1z605\n",
              "H2O_cluster_total_nodes:    1\n",
              "H2O_cluster_free_memory:    26.63 Gb\n",
              "H2O_cluster_total_cores:    20\n",
              "H2O_cluster_allowed_cores:  20\n",
              "H2O_cluster_status:         locked, healthy\n",
              "H2O_connection_url:         http://127.0.0.1:54321\n",
              "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
              "H2O_internal_security:      False\n",
              "Python_version:             3.10.12 final\n",
              "--------------------------  ------------------------------"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
            "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
            "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
            "AutoML progress: |███████"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkConf\n",
        "import h2o\n",
        "from pyspark.sql.functions import col\n",
        "from h2o.automl import H2OAutoML\n",
        "\n",
        "# Initialize Spark with needed configs\n",
        "spark = SparkSession.builder \\\n",
        "   .appName(\"CryptoDataProcessing\") \\\n",
        "   .config(\"spark.driver.memory\", \"8g\") \\\n",
        "   .config(\"spark.executor.memory\", \"4g\") \\\n",
        "   .config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
        "   .master(\"local[*]\") \\\n",
        "   .getOrCreate()\n",
        "\n",
        "# Initialize H2O\n",
        "h2o.init()\n",
        "\n",
        "# Load data\n",
        "h2o_frame = h2o.import_file(\"yiedl_latest.parquet\")\n",
        "\n",
        "import numerapi\n",
        "import os\n",
        "\n",
        "napi = numerapi.NumerAPI()\n",
        "\n",
        "# Download training data and live universe\n",
        "napi.download_dataset(\"crypto/v1.0/train_targets.parquet\", os.getcwd() + \"/numerai_train_targets.parquet\")\n",
        "napi.download_dataset(\"crypto/v1.0/live_universe.parquet\", os.getcwd() + \"/numerai_live_universe.parquet\")\n",
        "\n",
        "# Load into h2o\n",
        "train = h2o.import_file(\"numerai_train_targets.parquet\")\n",
        "live = h2o.import_file(\"numerai_live_universe.parquet\")\n",
        "\n",
        "def feature_engineering(frame):\n",
        "   for col in ['pvm', 'sentiment', 'onchain']:\n",
        "       if col in frame.columns:\n",
        "           frame[f'{col}_squared'] = frame[col]**2\n",
        "           frame[f'{col}_cubed'] = frame[col]**3\n",
        "\n",
        "   if all(x in frame.columns for x in ['pvm', 'sentiment', 'onchain']):\n",
        "       frame['pvm_sentiment'] = frame['pvm'] * frame['sentiment']\n",
        "       frame['pvm_onchain'] = frame['pvm'] * frame['onchain']\n",
        "       frame['sentiment_onchain'] = frame['sentiment'] * frame['onchain']\n",
        "\n",
        "   return frame\n",
        "\n",
        "enhanced_train = feature_engineering(train)\n",
        "enhanced_live = feature_engineering(live)\n",
        "\n",
        "aml = H2OAutoML(max_models=20, seed=1, max_runtime_secs=26000)\n",
        "aml.train(x=[col for col in enhanced_train.columns if col not in ['target', 'era']],\n",
        "         y='target',\n",
        "         training_frame=enhanced_train)\n",
        "\n",
        "print(aml.leaderboard)\n",
        "\n",
        "\n",
        "# Cleanup\n",
        "h2o.cluster().shutdown()\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fe47b90-bd70-46b2-ba74-1c5aeeb773a5",
      "metadata": {
        "id": "9fe47b90-bd70-46b2-ba74-1c5aeeb773a5",
        "outputId": "a652b56f-49fe-46c7-8bb1-3717a21c30e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking whether there is an H2O instance running at http://localhost:54321. connected.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "\n",
              "#h2o-table-18.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-18 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-18 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-18 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-18 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-18 .h2o-table th,\n",
              "#h2o-table-18 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-18 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-18\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption></caption>\n",
              "    <thead></thead>\n",
              "    <tbody><tr><td>H2O_cluster_uptime:</td>\n",
              "<td>1 hour 55 mins</td></tr>\n",
              "<tr><td>H2O_cluster_timezone:</td>\n",
              "<td>Europe/Amsterdam</td></tr>\n",
              "<tr><td>H2O_data_parsing_timezone:</td>\n",
              "<td>UTC</td></tr>\n",
              "<tr><td>H2O_cluster_version:</td>\n",
              "<td>3.46.0.6</td></tr>\n",
              "<tr><td>H2O_cluster_version_age:</td>\n",
              "<td>2 months and 22 days</td></tr>\n",
              "<tr><td>H2O_cluster_name:</td>\n",
              "<td>H2O_from_python_knight1_5e51as</td></tr>\n",
              "<tr><td>H2O_cluster_total_nodes:</td>\n",
              "<td>1</td></tr>\n",
              "<tr><td>H2O_cluster_free_memory:</td>\n",
              "<td>24.85 Gb</td></tr>\n",
              "<tr><td>H2O_cluster_total_cores:</td>\n",
              "<td>20</td></tr>\n",
              "<tr><td>H2O_cluster_allowed_cores:</td>\n",
              "<td>20</td></tr>\n",
              "<tr><td>H2O_cluster_status:</td>\n",
              "<td>locked, healthy</td></tr>\n",
              "<tr><td>H2O_connection_url:</td>\n",
              "<td>http://localhost:54321</td></tr>\n",
              "<tr><td>H2O_connection_proxy:</td>\n",
              "<td>{\"http\": null, \"https\": null}</td></tr>\n",
              "<tr><td>H2O_internal_security:</td>\n",
              "<td>False</td></tr>\n",
              "<tr><td>Python_version:</td>\n",
              "<td>3.10.12 final</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n"
            ],
            "text/plain": [
              "--------------------------  ------------------------------\n",
              "H2O_cluster_uptime:         1 hour 55 mins\n",
              "H2O_cluster_timezone:       Europe/Amsterdam\n",
              "H2O_data_parsing_timezone:  UTC\n",
              "H2O_cluster_version:        3.46.0.6\n",
              "H2O_cluster_version_age:    2 months and 22 days\n",
              "H2O_cluster_name:           H2O_from_python_knight1_5e51as\n",
              "H2O_cluster_total_nodes:    1\n",
              "H2O_cluster_free_memory:    24.85 Gb\n",
              "H2O_cluster_total_cores:    20\n",
              "H2O_cluster_allowed_cores:  20\n",
              "H2O_cluster_status:         locked, healthy\n",
              "H2O_connection_url:         http://localhost:54321\n",
              "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
              "H2O_internal_security:      False\n",
              "Python_version:             3.10.12 final\n",
              "--------------------------  ------------------------------"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[28], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://api.yiedl.ai/yiedl/v1/downloadDataset?type=latest\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     46\u001b[0m output_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myiedl_latest.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 47\u001b[0m \u001b[43mdownload_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_filename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Get absolute path\u001b[39;00m\n\u001b[1;32m     50\u001b[0m absolute_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myiedl_latest.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[28], line 35\u001b[0m, in \u001b[0;36mdownload_file\u001b[0;34m(url, output_filename)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdownload_file\u001b[39m(url, output_filename):\n\u001b[0;32m---> 35\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
            "File \u001b[0;32m~/numerai_EH/lib/python3.10/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/numerai_EH/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/numerai_EH/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
            "File \u001b[0;32m~/numerai_EH/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
            "File \u001b[0;32m~/numerai_EH/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
            "File \u001b[0;32m~/numerai_EH/lib/python3.10/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[0;32m~/numerai_EH/lib/python3.10/site-packages/urllib3/connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
            "File \u001b[0;32m~/numerai_EH/lib/python3.10/site-packages/urllib3/connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    513\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    519\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
            "File \u001b[0;32m/usr/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
            "File \u001b[0;32m/usr/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[0;32m/usr/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1303\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1300\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1301\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1302\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
            "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1159\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pysparkling.conf import H2OConf\n",
        "from pyspark import SparkConf\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier\n",
        "from pyspark.ml.pipeline import Pipeline\n",
        "from h2o.automl import H2OAutoML\n",
        "from pysparkling import H2OContext\n",
        "\n",
        "\n",
        "from pysparkling import H2OContext\n",
        "from pysparkling.conf import H2OConf\n",
        "\n",
        "import h2o\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize H2O\n",
        "h2o.init()\n",
        "\n",
        "import os\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"CryptoDataProcessing\") \\\n",
        "    .config(\"spark.driver.memory\", \"8g\") \\\n",
        "    .config(\"spark.executor.memory\", \"220g\") \\\n",
        "    .config(\"spark.hadoop.fs.defaultFS\", \"file:///\") \\\n",
        "    .config(\"spark.hadoop.fs.hdfs.impl\", \"org.apache.hadoop.fs.LocalFileSystem\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Helper function to download files\n",
        "def download_file(url, output_filename):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        with open(output_filename, 'wb') as file:\n",
        "            file.write(response.content)\n",
        "        print(f\"File downloaded successfully as {output_filename}\")\n",
        "    else:\n",
        "        print(\"Failed to download file\")\n",
        "\n",
        "# Use H2O directly instead of H2OContext\n",
        "# Download YIEDL crypto latest dataset\n",
        "url = 'https://api.yiedl.ai/yiedl/v1/downloadDataset?type=latest'\n",
        "output_filename = 'yiedl_latest.parquet'\n",
        "download_file(url, output_filename)\n",
        "\n",
        "# Get absolute path\n",
        "absolute_path = os.path.abspath(\"yiedl_latest.parquet\")\n",
        "\n",
        "# Create H2O configuration\n",
        "h2o_conf = H2OConf()\n",
        "h2o_conf.setInternalClusterMode()\n",
        "\n",
        "# Read Parquet file directly into H2O Frame\n",
        "h2o_frame = h2o.import_file(f\"file://{absolute_path}\")\n",
        "\n",
        "# Print H2O Frame\n",
        "#print(h2o_frame)\n",
        "\n",
        "# Stop Spark session\n",
        "spark.stop()\n",
        "\n",
        "\n",
        "\n",
        "# Advanced feature preprocessing\n",
        "def advanced_feature_preprocessing(df):\n",
        "    symbol_window = Window.partitionBy('symbol')\n",
        "    date_window = Window.partitionBy('date')\n",
        "\n",
        "    enhanced_df = (df\n",
        "        .withColumn('pvm_rolling_mean_5', F.avg('pvm').over(symbol_window.orderBy('date').rowsBetween(-5, 0)))\n",
        "        .withColumn('pvm_rolling_mean_10', F.avg('pvm').over(symbol_window.orderBy('date').rowsBetween(-10, 0)))\n",
        "        .withColumn('sentiment_rolling_mean_5', F.avg('sentiment').over(symbol_window.orderBy('date').rowsBetween(-5, 0)))\n",
        "        .withColumn('onchain_rolling_mean_5', F.avg('onchain').over(symbol_window.orderBy('date').rowsBetween(-5, 0)))\n",
        "        .withColumn('pvm_rsi', F.expr('(pvm - lag(pvm, 14) over (partition by symbol order by date)) / 14'))\n",
        "        .withColumn('sentiment_rsi', F.expr('(sentiment - lag(sentiment, 14) over (partition by symbol order by date)) / 14'))\n",
        "        .withColumn('pvm_volatility', F.stddev('pvm').over(symbol_window.orderBy('date').rowsBetween(-10, 0)))\n",
        "        .withColumn('sentiment_volatility', F.stddev('sentiment').over(symbol_window.orderBy('date').rowsBetween(-10, 0)))\n",
        "        .withColumn('pvm_percentile', F.percent_rank().over(symbol_window.orderBy('pvm')))\n",
        "        .withColumn('sentiment_percentile', F.percent_rank().over(symbol_window.orderBy('sentiment')))\n",
        "    )\n",
        "\n",
        "    return enhanced_df\n",
        "\n",
        "# Add advanced polynomial features\n",
        "def add_advanced_polynomial_features(df):\n",
        "    base_features = ['pvm', 'sentiment', 'onchain']\n",
        "\n",
        "    for feature in base_features:\n",
        "        df = (df\n",
        "            .withColumn(f\"{feature}_squared\", F.pow(F.col(feature), 2))\n",
        "            .withColumn(f\"{feature}_cubed\", F.pow(F.col(feature), 3))\n",
        "            .withColumn(f\"{feature}_log\", F.log(F.abs(F.col(feature)) + 1))\n",
        "        )\n",
        "\n",
        "    interaction_features = [('pvm', 'sentiment'), ('pvm', 'onchain'), ('sentiment', 'onchain')]\n",
        "\n",
        "    for (feat1, feat2) in interaction_features:\n",
        "        df = (df\n",
        "            .withColumn(f\"{feat1}_{feat2}_interaction\", F.col(feat1) * F.col(feat2))\n",
        "            .withColumn(f\"{feat1}_{feat2}_diff\", F.abs(F.col(feat1) - F.col(feat2)))\n",
        "        )\n",
        "\n",
        "    return df\n",
        "\n",
        "# Preprocess and enhance the dataset\n",
        "preprocessed_df = advanced_feature_preprocessing(h2o_frame)\n",
        "enhanced_df = add_advanced_polynomial_features(preprocessed_df)\n",
        "\n",
        "# Convert Spark DataFrame to H2O Frame\n",
        "h2o_frame = hc.asH2OFrame(enhanced_df)\n",
        "\n",
        "# Initialize H2O AutoML\n",
        "aml = H2OAutoML(\n",
        "    max_models=20,\n",
        "    seed=1,\n",
        "    max_runtime_secs=600  # 10 minutes\n",
        ")\n",
        "\n",
        "# Train models using AutoML\n",
        "target_column = \"target\"  # Replace with your actual target column\n",
        "aml.train(x=h2o_frame.columns[:-1],  # exclude target\n",
        "          y=target_column,\n",
        "          training_frame=h2o_frame)\n",
        "\n",
        "# Display the H2O leaderboard\n",
        "leaderboard = aml.leaderboard\n",
        "print(leaderboard)\n",
        "\n",
        "# Stop H2O and Spark sessions\n",
        "h2o.cluster().shutdown()\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "823f4e14-fd31-4e13-9e0b-3c70bec5f181",
      "metadata": {
        "id": "823f4e14-fd31-4e13-9e0b-3c70bec5f181"
      },
      "outputs": [],
      "source": [
        "# Cell 2: Core Processing Functions with Enhanced Error Handling\n",
        "def analyze_feature_columns(df, prefix, sample_size=1000):\n",
        "    \"\"\"\n",
        "    Advanced feature column analysis with robust error handling.\n",
        "\n",
        "    Args:\n",
        "        df (DataFrame): Spark DataFrame to analyze\n",
        "        prefix (str): Feature column prefix\n",
        "        sample_size (int): Sample size for analysis\n",
        "\n",
        "    Returns:\n",
        "        tuple or None: Feature range information\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Get feature columns with robust filtering\n",
        "        feature_cols = sorted([\n",
        "            col for col in df.columns\n",
        "            if col.startswith(prefix + '_') and\n",
        "            not col.endswith('_array') and\n",
        "            '_' in col.split(prefix + '_')[1]\n",
        "        ])\n",
        "\n",
        "        if not feature_cols:\n",
        "            print(f\"No features found for prefix: {prefix}\")\n",
        "            return None\n",
        "\n",
        "        # Efficient non-null count\n",
        "        counts = df.select([\n",
        "            F.count(F.when(F.col(col).isNotNull(), True)).alias(col)\n",
        "            for col in feature_cols[:5]\n",
        "        ]).collect()[0].asDict()\n",
        "\n",
        "        print(f\"\\n{prefix} feature sample counts: {counts}\")\n",
        "\n",
        "        # Extract valid feature numbers with error resilience\n",
        "        valid_numbers = []\n",
        "        for col in feature_cols:\n",
        "            try:\n",
        "                # More robust number extraction\n",
        "                num = int(col.split('_')[-1])\n",
        "                row_count = df.select(\n",
        "                    F.count(F.when(F.col(col).isNotNull(), True))\n",
        "                ).collect()[0][0]\n",
        "\n",
        "                if row_count > 0:\n",
        "                    valid_numbers.append(num)\n",
        "            except (ValueError, IndexError) as e:\n",
        "                print(f\"Skipping invalid column {col}: {e}\")\n",
        "                continue\n",
        "\n",
        "        if valid_numbers:\n",
        "            # Use builtins to avoid PySpark function conflicts\n",
        "            min_num = builtins.min(valid_numbers)\n",
        "            max_num = builtins.max(valid_numbers)\n",
        "            return min_num, max_num, len(valid_numbers)\n",
        "\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Comprehensive error in feature column analysis: {e}\")\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "def get_feature_ranges(df):\n",
        "    \"\"\"\n",
        "    Dynamically determine feature ranges with comprehensive error handling.\n",
        "\n",
        "    Args:\n",
        "        df (DataFrame): Spark DataFrame containing features\n",
        "\n",
        "    Returns:\n",
        "        dict: Feature ranges for different feature types\n",
        "    \"\"\"\n",
        "    feature_ranges = {}\n",
        "    feature_prefixes = ['pvm', 'sentiment', 'onchain']\n",
        "\n",
        "    for prefix in feature_prefixes:\n",
        "        try:\n",
        "            result = analyze_feature_columns(df, prefix)\n",
        "            if result:\n",
        "                min_num, max_num, count = result\n",
        "                feature_ranges[prefix] = (min_num, max_num)\n",
        "                print(f\"Found {count} valid {prefix} features: using range {min_num} to {max_num}\")\n",
        "            else:\n",
        "                print(f\"No valid features found for {prefix}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {prefix} features: {str(e)}\")\n",
        "            traceback.print_exc()\n",
        "\n",
        "    if not feature_ranges:\n",
        "        raise ValueError(\"No valid feature ranges found across all prefixes\")\n",
        "\n",
        "    return feature_ranges\n",
        "\n",
        "def process_feature_group(df, feature_prefix, start_idx, end_idx):\n",
        "    \"\"\"\n",
        "    Advanced feature group processing with enhanced memory management.\n",
        "\n",
        "    Args:\n",
        "        df (DataFrame): Input Spark DataFrame\n",
        "        feature_prefix (str): Prefix for feature columns\n",
        "        start_idx (int): Starting feature index\n",
        "        end_idx (int): Ending feature index\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: Processed feature DataFrame\n",
        "    \"\"\"\n",
        "    print(f\"\\nProcessing {feature_prefix} features {start_idx} to {end_idx}\")\n",
        "\n",
        "    cols = [f'{feature_prefix}_{str(i).zfill(4)}' for i in range(start_idx, end_idx + 1)]\n",
        "    existing_cols = [col for col in cols if col in df.columns]\n",
        "\n",
        "    if not existing_cols:\n",
        "        raise ValueError(f\"No valid columns found for {feature_prefix}\")\n",
        "\n",
        "    print(f\"Found {len(existing_cols)} columns for {feature_prefix}\")\n",
        "\n",
        "    # GPU memory monitoring\n",
        "    if NVML_AVAILABLE:\n",
        "        monitor_gpu_memory()\n",
        "\n",
        "    try:\n",
        "        # Distributed batch processing\n",
        "        result = None\n",
        "        batch_size = CONFIG.get('batch_size', 1000)\n",
        "\n",
        "        for i in range(0, len(existing_cols), batch_size):\n",
        "            batch_cols = existing_cols[i:min(i + batch_size, len(existing_cols))]\n",
        "\n",
        "            # Batch processing with repartitioning\n",
        "            batch_df = df.select(['date', 'symbol'] + batch_cols)\n",
        "            batch_df = batch_df.repartition(200)\n",
        "\n",
        "            # Null handling and type conversion\n",
        "            for col in batch_cols:\n",
        "                batch_df = batch_df.withColumn(\n",
        "                    col,\n",
        "                    F.when(F.col(col).isNull(), 0.0)\n",
        "                     .otherwise(F.col(col).cast(\"double\"))\n",
        "                )\n",
        "\n",
        "            # Vector assembler for batch\n",
        "            assembler = VectorAssembler(\n",
        "                inputCols=batch_cols,\n",
        "                outputCol=f\"features_{feature_prefix}_{start_idx}_{end_idx}\",\n",
        "                handleInvalid=\"keep\"\n",
        "            )\n",
        "\n",
        "            batch_df = assembler.transform(batch_df)\n",
        "\n",
        "            # Union or set result\n",
        "            result = batch_df if result is None else result.unionAll(batch_df)\n",
        "\n",
        "            # Memory cleanup\n",
        "            cleanup_memory()\n",
        "\n",
        "        # Final GPU memory check\n",
        "        if NVML_AVAILABLE:\n",
        "            monitor_gpu_memory()\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {feature_prefix}: {str(e)}\")\n",
        "        cleanup_memory()\n",
        "        raise\n",
        "\n",
        "def monitor_gpu_memory():\n",
        "    \"\"\"\n",
        "    Advanced GPU memory monitoring with detailed insights.\n",
        "    \"\"\"\n",
        "    if not NVML_AVAILABLE:\n",
        "        print(\"NVML not available for GPU monitoring.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        pynvml.nvmlInit()\n",
        "        gpu_count = pynvml.nvmlDeviceGetCount()\n",
        "\n",
        "        print(\"\\n--- GPU Memory Monitor ---\")\n",
        "        for i in range(gpu_count):\n",
        "            handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
        "            name = pynvml.nvmlDeviceGetName(handle)\n",
        "            memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
        "\n",
        "            print(f\"GPU {i} ({name.decode()}):\")\n",
        "            print(f\"  Memory Used: {memory_info.used / 1024**3:.2f} GB\")\n",
        "            print(f\"  Memory Free: {memory_info.free / 1024**3:.2f} GB\")\n",
        "            print(f\"  Memory Total: {memory_info.total / 1024**3:.2f} GB\")\n",
        "\n",
        "            # Optional: Get compute utilization if available\n",
        "            try:\n",
        "                utilization = pynvml.nvmlDeviceGetUtilizationRates(handle)\n",
        "                print(f\"  GPU Utilization: {utilization.gpu}%\")\n",
        "                print(f\"  Memory Utilization: {utilization.memory}%\")\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"GPU monitoring error: {e}\")\n",
        "    finally:\n",
        "        try:\n",
        "            pynvml.nvmlShutdown()\n",
        "        except:\n",
        "            pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47b0a597-c837-4e43-8c4b-5d76b1cc42ef",
      "metadata": {
        "id": "47b0a597-c837-4e43-8c4b-5d76b1cc42ef",
        "outputId": "d40946ea-dec4-422c-be20-11b8179ead60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark session created with advanced GPU support.\n",
            "Distributed GPU processing environment is ready.\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Environment Initialization Functions\n",
        "def initialize_spark_session():\n",
        "    \"\"\"\n",
        "    Create an advanced Spark session with comprehensive GPU and distributed computing support.\n",
        "\n",
        "    Returns:\n",
        "        SparkSession or None: Configured Spark session\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Comprehensive environment configuration\n",
        "        network_config = get_local_network_config()\n",
        "        spark = (SparkSession.builder\n",
        "            .appName(\"DistributedFeatureProcessing\")\n",
        "            .config(\"spark.driver.host\", network_config['local_ip'])\n",
        "            .config(\"spark.driver.bindAddress\", network_config['local_ip'])\n",
        "\n",
        "            # Memory and Resource Configurations\n",
        "            .config(\"spark.driver.memory\", CONFIG['memory'])\n",
        "            .config(\"spark.executor.memory\", CONFIG['memory'])\n",
        "\n",
        "            # GPU Specific Configurations\n",
        "            .config(\"spark.executor.instances\", CONFIG.get('executor_instances', 1))\n",
        "            .config(\"spark.executor.resource.gpu.amount\", \"1\")\n",
        "            .config(\"spark.rapids.sql.concurrentGpuTasks\", \"1\")\n",
        "            .config(\"spark.rapids.memory.gpu.pool\", \"ARENA\")\n",
        "            .config(\"spark.rapids.memory.gpu.reserve\", CONFIG['gpu_memory_reserve'])\n",
        "\n",
        "            # Advanced Distributed Processing\n",
        "            .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
        "            .config(\"spark.dynamicAllocation.enabled\", \"false\")\n",
        "            .config(\"spark.sql.shuffle.partitions\", CONFIG.get('executor_instances', 1) * 2)\n",
        "\n",
        "            # Performance Tuning\n",
        "            .config(\"spark.sql.files.maxPartitionBytes\", \"128m\")\n",
        "            .config(\"spark.default.parallelism\", CONFIG.get('executor_instances', 1) * 2)\n",
        "\n",
        "            .master(\"local[*]\")\n",
        "            .getOrCreate())\n",
        "\n",
        "        # Reduce logging noise\n",
        "        spark.sparkContext.setLogLevel(\"ERROR\")\n",
        "\n",
        "        print(\"Spark session created with advanced GPU support.\")\n",
        "        return spark\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Spark session initialization error: {e}\")\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main execution function with comprehensive error handling.\n",
        "    \"\"\"\n",
        "    spark = None\n",
        "    try:\n",
        "        # Initialize Spark\n",
        "        spark = initialize_spark_session()\n",
        "        if not spark:\n",
        "            print(\"Failed to create Spark session.\")\n",
        "            return\n",
        "\n",
        "        # Additional initialization and processing steps would go here\n",
        "        print(\"Distributed GPU processing environment is ready.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Main execution error: {e}\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "    finally:\n",
        "        # Cleanup\n",
        "        if spark:\n",
        "            spark.stop()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea8f17e5-5c68-4b65-ae20-07228288ceaf",
      "metadata": {
        "id": "ea8f17e5-5c68-4b65-ae20-07228288ceaf",
        "outputId": "e0d91896-6e82-4ec3-e792-d8aaeadebf70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detected 3 GPUs.\n",
            "GPU memory cleared.\n",
            "Allocating 8 GB of memory on GPU 0...\n",
            "Memory allocated on GPU 0.\n",
            "GPU 0 memory usage (before computation): Allocated = 16.00 GB, Reserved = 16.02 GB\n",
            "Performing matrix multiplication on GPU 0...\n",
            "Failed to allocate memory or perform computation on GPU 0: Expected size for first two dimensions of batch2 tensor to be: [1024, 1024] but got: [1024, 2048].\n",
            "GPU memory cleared.\n",
            "Allocating 8 GB of memory on GPU 1...\n",
            "Memory allocated on GPU 1.\n",
            "GPU 1 memory usage (before computation): Allocated = 16.00 GB, Reserved = 16.02 GB\n",
            "Performing matrix multiplication on GPU 1...\n",
            "Failed to allocate memory or perform computation on GPU 1: Expected size for first two dimensions of batch2 tensor to be: [1024, 1024] but got: [1024, 2048].\n",
            "GPU memory cleared.\n",
            "Allocating 8 GB of memory on GPU 2...\n",
            "Memory allocated on GPU 2.\n",
            "GPU 2 memory usage (before computation): Allocated = 16.00 GB, Reserved = 16.02 GB\n",
            "Performing matrix multiplication on GPU 2...\n",
            "Failed to allocate memory or perform computation on GPU 2: Expected size for first two dimensions of batch2 tensor to be: [1024, 1024] but got: [1024, 2048].\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "import time\n",
        "\n",
        "def clear_gpu_memory():\n",
        "    \"\"\"Clear GPU memory using PyTorch.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "        print(\"GPU memory cleared.\")\n",
        "\n",
        "def allocate_and_compute_on_gpus():\n",
        "    \"\"\"Allocate memory and perform compute operations on each GPU separately.\"\"\"\n",
        "    num_gpus = torch.cuda.device_count()\n",
        "    if num_gpus == 0:\n",
        "        print(\"No GPUs detected.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Detected {num_gpus} GPUs.\")\n",
        "\n",
        "    for gpu_id in range(num_gpus):\n",
        "        device = torch.device(f\"cuda:{gpu_id}\")\n",
        "        torch.cuda.set_device(device)\n",
        "\n",
        "        # Clear GPU memory before allocation\n",
        "        clear_gpu_memory()\n",
        "\n",
        "        # Allocate 8 GB of memory on the GPU\n",
        "        print(f\"Allocating 8 GB of memory on GPU {gpu_id}...\")\n",
        "        try:\n",
        "            # Create a tensor of size (1024, 2048, 1024) ~ 8 GB\n",
        "            tensor1 = torch.randn((1024, 2048, 1024), device=device, dtype=torch.float32)\n",
        "            # Create another tensor of size (1024, 2048, 1024) ~ 8 GB\n",
        "            tensor2 = torch.randn((1024, 2048, 1024), device=device, dtype=torch.float32)\n",
        "            # Total memory allocated: ~16 GB (8 GB + 8 GB)\n",
        "            # Additional memory will be used during computation (e.g., intermediate results)\n",
        "\n",
        "            print(f\"Memory allocated on GPU {gpu_id}.\")\n",
        "\n",
        "            # Verify memory usage before computation\n",
        "            memory_allocated = torch.cuda.memory_allocated(device) / 1024**3  # Convert to GB\n",
        "            memory_reserved = torch.cuda.memory_reserved(device) / 1024**3  # Convert to GB\n",
        "            print(f\"GPU {gpu_id} memory usage (before computation): Allocated = {memory_allocated:.2f} GB, Reserved = {memory_reserved:.2f} GB\")\n",
        "\n",
        "            # Perform a compute-heavy operation (matrix multiplication) in a loop\n",
        "            print(f\"Performing matrix multiplication on GPU {gpu_id}...\")\n",
        "            start_time = time.time()\n",
        "            for _ in range(10):  # Repeat computation 10 times\n",
        "                result = torch.matmul(tensor1, tensor2)  # Matrix multiplication\n",
        "                torch.cuda.synchronize()  # Wait for the operation to complete\n",
        "            compute_time = time.time() - start_time\n",
        "            print(f\"Matrix multiplication completed on GPU {gpu_id} in {compute_time:.2f} seconds.\")\n",
        "\n",
        "            # Verify memory usage after computation\n",
        "            memory_allocated = torch.cuda.memory_allocated(device) / 1024**3  # Convert to GB\n",
        "            memory_reserved = torch.cuda.memory_reserved(device) / 1024**3  # Convert to GB\n",
        "            print(f\"GPU {gpu_id} memory usage (after computation): Allocated = {memory_allocated:.2f} GB, Reserved = {memory_reserved:.2f} GB\")\n",
        "\n",
        "            # Free memory\n",
        "            del tensor1, tensor2, result\n",
        "            torch.cuda.empty_cache()\n",
        "        except RuntimeError as e:\n",
        "            print(f\"Failed to allocate memory or perform computation on GPU {gpu_id}: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Set environment variable to reduce fragmentation\n",
        "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "    allocate_and_compute_on_gpus()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35c72fb2-e419-40dc-a5a8-b416c9c2de2c",
      "metadata": {
        "id": "35c72fb2-e419-40dc-a5a8-b416c9c2de2c",
        "outputId": "fee1c6ba-efcb-48f2-db36-25ec64c31e0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU 0 memory cleared.\n",
            "GPU 1 memory cleared.\n",
            "GPU 2 memory cleared.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "#spark.stop()\n",
        "def clear_gpu_memory():\n",
        "    \"\"\"Clear GPU memory on all available GPUs.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        for gpu_id in range(torch.cuda.device_count()):\n",
        "            device = torch.device(f\"cuda:{gpu_id}\")\n",
        "            torch.cuda.set_device(device)\n",
        "            torch.cuda.empty_cache()\n",
        "            torch.cuda.reset_peak_memory_stats()\n",
        "            print(f\"GPU {gpu_id} memory cleared.\")\n",
        "    else:\n",
        "        print(\"No GPUs detected.\")\n",
        "\n",
        "# Clear GPU memory\n",
        "clear_gpu_memory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db1399af-d4db-4929-a602-b02b73615075",
      "metadata": {
        "id": "db1399af-d4db-4929-a602-b02b73615075"
      },
      "outputs": [],
      "source": [
        "# Cell 2: Core Processing Functions with Enhanced Error Handling\n",
        "\n",
        "def analyze_feature_columns(df, prefix, sample_size=1000):\n",
        "    \"\"\"\n",
        "    Advanced feature column analysis with robust error handling.\n",
        "\n",
        "    Args:\n",
        "        df (DataFrame): Spark DataFrame to analyze\n",
        "        prefix (str): Feature column prefix\n",
        "        sample_size (int): Sample size for analysis\n",
        "\n",
        "    Returns:\n",
        "        tuple or None: Feature range information\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Get feature columns with robust filtering\n",
        "        feature_cols = sorted([\n",
        "            col for col in df.columns\n",
        "            if col.startswith(prefix + '_') and\n",
        "            not col.endswith('_array') and\n",
        "            '_' in col.split(prefix + '_')[1]\n",
        "        ])\n",
        "\n",
        "        if not feature_cols:\n",
        "            print(f\"No features found for prefix: {prefix}\")\n",
        "            return None\n",
        "\n",
        "        # Efficient non-null count\n",
        "        counts = df.select([\n",
        "            F.count(F.when(F.col(col).isNotNull(), True)).alias(col)\n",
        "            for col in feature_cols[:5]\n",
        "        ]).collect()[0].asDict()\n",
        "\n",
        "        print(f\"\\n{prefix} feature sample counts: {counts}\")\n",
        "\n",
        "        # Extract valid feature numbers with error resilience\n",
        "        valid_numbers = []\n",
        "        for col in feature_cols:\n",
        "            try:\n",
        "                # More robust number extraction\n",
        "                num = int(col.split('_')[-1])\n",
        "                row_count = df.select(\n",
        "                    F.count(F.when(F.col(col).isNotNull(), True))\n",
        "                ).collect()[0][0]\n",
        "\n",
        "                if row_count > 0:\n",
        "                    valid_numbers.append(num)\n",
        "            except (ValueError, IndexError) as e:\n",
        "                print(f\"Skipping invalid column {col}: {e}\")\n",
        "                continue\n",
        "\n",
        "        if valid_numbers:\n",
        "            # Use builtins to avoid PySpark function conflicts\n",
        "            min_num = builtins.min(valid_numbers)\n",
        "            max_num = builtins.max(valid_numbers)\n",
        "            return min_num, max_num, len(valid_numbers)\n",
        "\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Comprehensive error in feature column analysis: {e}\")\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "def get_feature_ranges(df):\n",
        "    \"\"\"\n",
        "    Dynamically determine feature ranges with comprehensive error handling.\n",
        "\n",
        "    Args:\n",
        "        df (DataFrame): Spark DataFrame containing features\n",
        "\n",
        "    Returns:\n",
        "        dict: Feature ranges for different feature types\n",
        "    \"\"\"\n",
        "    feature_ranges = {}\n",
        "    feature_prefixes = ['pvm', 'sentiment', 'onchain']\n",
        "\n",
        "    for prefix in feature_prefixes:\n",
        "        try:\n",
        "            result = analyze_feature_columns(df, prefix)\n",
        "            if result:\n",
        "                min_num, max_num, count = result\n",
        "                feature_ranges[prefix] = (min_num, max_num)\n",
        "                print(f\"Found {count} valid {prefix} features: using range {min_num} to {max_num}\")\n",
        "            else:\n",
        "                print(f\"No valid features found for {prefix}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {prefix} features: {str(e)}\")\n",
        "            traceback.print_exc()\n",
        "\n",
        "    if not feature_ranges:\n",
        "        raise ValueError(\"No valid feature ranges found across all prefixes\")\n",
        "\n",
        "    return feature_ranges\n",
        "\n",
        "def process_feature_group(df, feature_prefix, start_idx, end_idx):\n",
        "    \"\"\"\n",
        "    Advanced feature group processing with enhanced memory management.\n",
        "\n",
        "    Args:\n",
        "        df (DataFrame): Input Spark DataFrame\n",
        "        feature_prefix (str): Prefix for feature columns\n",
        "        start_idx (int): Starting feature index\n",
        "        end_idx (int): Ending feature index\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: Processed feature DataFrame\n",
        "    \"\"\"\n",
        "    print(f\"\\nProcessing {feature_prefix} features {start_idx} to {end_idx}\")\n",
        "\n",
        "    cols = [f'{feature_prefix}_{str(i).zfill(4)}' for i in range(start_idx, end_idx + 1)]\n",
        "    existing_cols = [col for col in cols if col in df.columns]\n",
        "\n",
        "    if not existing_cols:\n",
        "        raise ValueError(f\"No valid columns found for {feature_prefix}\")\n",
        "\n",
        "    print(f\"Found {len(existing_cols)} columns for {feature_prefix}\")\n",
        "\n",
        "    # GPU memory monitoring\n",
        "    if NVML_AVAILABLE:\n",
        "        monitor_gpu_memory()\n",
        "\n",
        "    try:\n",
        "        # Distributed batch processing\n",
        "        result = None\n",
        "        batch_size = CONFIG.get('batch_size', 1000)\n",
        "\n",
        "        for i in range(0, len(existing_cols), batch_size):\n",
        "            batch_cols = existing_cols[i:min(i + batch_size, len(existing_cols))]\n",
        "\n",
        "            # Batch processing with repartitioning\n",
        "            batch_df = df.select(['date', 'symbol'] + batch_cols)\n",
        "            batch_df = batch_df.repartition(200)\n",
        "\n",
        "            # Null handling and type conversion\n",
        "            for col in batch_cols:\n",
        "                batch_df = batch_df.withColumn(\n",
        "                    col,\n",
        "                    F.when(F.col(col).isNull(), 0.0)\n",
        "                     .otherwise(F.col(col).cast(\"double\"))\n",
        "                )\n",
        "\n",
        "            # Vector assembler for batch\n",
        "            assembler = VectorAssembler(\n",
        "                inputCols=batch_cols,\n",
        "                outputCol=f\"features_{feature_prefix}_{start_idx}_{end_idx}\",\n",
        "                handleInvalid=\"keep\"\n",
        "            )\n",
        "\n",
        "            batch_df = assembler.transform(batch_df)\n",
        "\n",
        "            # Union or set result\n",
        "            result = batch_df if result is None else result.unionAll(batch_df)\n",
        "\n",
        "            # Memory cleanup\n",
        "            cleanup_memory()\n",
        "\n",
        "        # Final GPU memory check\n",
        "        if NVML_AVAILABLE:\n",
        "            monitor_gpu_memory()\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {feature_prefix}: {str(e)}\")\n",
        "        cleanup_memory()\n",
        "        raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "330474c9-474d-420a-b1a4-788178c7b53d",
      "metadata": {
        "id": "330474c9-474d-420a-b1a4-788178c7b53d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(torch.cuda.device_count())  # Should print 3\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
        "else:\n",
        "    print(\"No GPUs available.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb757ba2-67e2-49d2-ba8d-e8bbb579bdee",
      "metadata": {
        "id": "bb757ba2-67e2-49d2-ba8d-e8bbb579bdee"
      },
      "outputs": [],
      "source": [
        "import cupy as cp\n",
        "\n",
        "# Check available GPUs\n",
        "print(f\"Number of GPUs: {cp.cuda.runtime.getDeviceCount()}\")\n",
        "\n",
        "# Test basic CuPy functionality\n",
        "x = cp.array([1, 2, 3])\n",
        "print(x * 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baac267f-0d51-4c50-88fb-3ba57f1d83d0",
      "metadata": {
        "id": "baac267f-0d51-4c50-88fb-3ba57f1d83d0",
        "outputId": "06fcb620-aa00-4252-9a12-f5ce3035f98b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of GPUs: 3\n",
            "GPU 0: b'NVIDIA GeForce RTX 3090'\n",
            "GPU 0 test: [2 4 6]\n",
            "GPU 1: b'NVIDIA GeForce RTX 3090'\n",
            "GPU 1 test: [2 4 6]\n",
            "GPU 2: b'NVIDIA RTX A5000'\n",
            "GPU 2 test: [2 4 6]\n"
          ]
        }
      ],
      "source": [
        "import cupy as cp\n",
        "\n",
        "# Check available GPUs\n",
        "print(f\"Number of GPUs: {cp.cuda.runtime.getDeviceCount()}\")\n",
        "\n",
        "# Test each GPU\n",
        "for i in range(cp.cuda.runtime.getDeviceCount()):\n",
        "    with cp.cuda.Device(i):\n",
        "        print(f\"GPU {i}: {cp.cuda.runtime.getDeviceProperties(i)['name']}\")\n",
        "        x = cp.array([1, 2, 3])\n",
        "        print(f\"GPU {i} test: {x * 2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a57cad7-c1f2-434f-aa4f-f5d7997e4632",
      "metadata": {
        "id": "8a57cad7-c1f2-434f-aa4f-f5d7997e4632"
      },
      "outputs": [],
      "source": [
        "import h2o\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.conf import SparkConf\n",
        "import socket\n",
        "import os\n",
        "import time\n",
        "import uuid\n",
        "\n",
        "def get_local_ip():\n",
        "    try:\n",
        "        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
        "        s.connect((\"8.8.8.8\", 80))\n",
        "        ip = s.getsockname()[0]\n",
        "        s.close()\n",
        "        return ip\n",
        "    except:\n",
        "        return \"127.0.0.1\"\n",
        "\n",
        "def ensure_directory(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    return path\n",
        "\n",
        "def create_spark_session():\n",
        "    \"\"\"Create a new Spark session with basic configurations\"\"\"\n",
        "    local_ip = get_local_ip()\n",
        "\n",
        "    # Stop any existing sessions\n",
        "    active_session = SparkSession._instantiatedSession\n",
        "    if active_session is not None:\n",
        "        active_session.stop()\n",
        "\n",
        "    # Create new configuration\n",
        "    conf = SparkConf()\n",
        "    conf.set(\"spark.master\", \"local[*]\")\n",
        "    conf.set(\"spark.app.name\", \"SparkH2O\")\n",
        "    conf.set(\"spark.driver.memory\", \"4g\")\n",
        "    conf.set(\"spark.executor.memory\", \"4g\")\n",
        "    conf.set(\"spark.driver.host\", local_ip)\n",
        "    conf.set(\"spark.driver.bindAddress\", local_ip)\n",
        "    conf.set(\"spark.network.timeout\", \"3600s\")\n",
        "    conf.set(\"spark.executor.heartbeatInterval\", \"120s\")\n",
        "\n",
        "    return SparkSession.builder \\\n",
        "        .config(conf=conf) \\\n",
        "        .getOrCreate()\n",
        "\n",
        "def init_h2o(cluster_name):\n",
        "    \"\"\"Initialize H2O\"\"\"\n",
        "    try:\n",
        "        print(\"Starting H2O cluster...\")\n",
        "\n",
        "        # Create ice_root directory\n",
        "        ice_root = ensure_directory(\"/tmp/h2o_ice_root\")\n",
        "        print(f\"Created ice_root directory at {ice_root}\")\n",
        "\n",
        "        # Get the local IP for later status message\n",
        "        local_ip = get_local_ip()\n",
        "        print(f\"Local IP: {local_ip}\")\n",
        "\n",
        "        # Initialize H2O on localhost\n",
        "        h2o.init(\n",
        "            port=54321,\n",
        "            ip=\"localhost\",  # Start on localhost\n",
        "            nthreads=-1,\n",
        "            ice_root=ice_root,\n",
        "            start_h2o=True,\n",
        "            enable_assertions=False,\n",
        "            bind_to_localhost=True,  # Bind to localhost initially\n",
        "            max_mem_size=\"4G\",\n",
        "            strict_version_check=False,\n",
        "            name=cluster_name,\n",
        "            jvm_custom_args=['-Xmx4g']\n",
        "        )\n",
        "\n",
        "        # Get cluster info\n",
        "        connection = h2o.connection()\n",
        "        cluster_url = connection.base_url\n",
        "        print(f\"\\nH2O cluster URL: {cluster_url}\")\n",
        "        print(f\"H2O connection IP: {local_ip}\")\n",
        "\n",
        "        # Show cluster status\n",
        "        print(\"\\nCluster Status:\")\n",
        "        h2o.cluster().show_status()\n",
        "\n",
        "        print(\"H2O cluster started successfully\")\n",
        "        return connection\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to start H2O: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def spark_to_h2o(spark_df):\n",
        "    \"\"\"Convert Spark DataFrame to H2O Frame\"\"\"\n",
        "    try:\n",
        "        # First save as CSV\n",
        "        temp_path = \"/tmp/temp_data.csv\"\n",
        "        spark_df.toPandas().to_csv(temp_path, index=False)\n",
        "        # Load into H2O\n",
        "        h2o_frame = h2o.import_file(temp_path)\n",
        "        os.remove(temp_path)\n",
        "        return h2o_frame\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting Spark DataFrame to H2O Frame: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def h2o_to_spark(h2o_frame, spark):\n",
        "    \"\"\"Convert H2O Frame to Spark DataFrame\"\"\"\n",
        "    try:\n",
        "        # Convert to pandas first\n",
        "        pandas_df = h2o_frame.as_data_frame()\n",
        "        # Convert to Spark\n",
        "        return spark.createDataFrame(pandas_df)\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting H2O Frame to Spark DataFrame: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        # Get H2O driver path\n",
        "        h2o_jar = os.path.join(os.path.dirname(h2o.__file__), 'backend', 'bin', 'h2o.jar')\n",
        "        if not os.path.exists(h2o_jar):\n",
        "            raise Exception(f\"H2O driver JAR not found at {h2o_jar}\")\n",
        "\n",
        "        print(f\"Using H2O JAR: {h2o_jar}\")\n",
        "        os.environ[\"H2O_DRIVER_JAR\"] = h2o_jar\n",
        "\n",
        "        # Kill any existing H2O processes\n",
        "        os.system(\"pkill -f h2o.jar\")\n",
        "        time.sleep(2)  # Wait for processes to be killed\n",
        "\n",
        "        # Generate a unique cluster name\n",
        "        cluster_name = f\"h2o_spark_cluster_{str(uuid.uuid4())[:8]}\"\n",
        "        print(f\"Using cluster name: {cluster_name}\")\n",
        "\n",
        "        # Initialize H2O first\n",
        "        connection = init_h2o(cluster_name)\n",
        "\n",
        "        # Then create Spark session\n",
        "        spark = create_spark_session()\n",
        "\n",
        "        print(\"\\nSetup complete!\")\n",
        "        print(f\"H2O Connection Info: {connection.base_url}\")\n",
        "        print(f\"Spark Session Info: {spark.sparkContext.applicationId}\")\n",
        "\n",
        "        # Test the connection\n",
        "        print(\"\\nTesting H2O connection...\")\n",
        "        test_frame = h2o.create_frame(rows=10, cols=5)\n",
        "        print(\"H2O test frame shape:\", test_frame.shape)\n",
        "\n",
        "        return spark, connection\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")\n",
        "        # Clean up\n",
        "        try:\n",
        "            h2o.cluster().shutdown()\n",
        "        except:\n",
        "            pass\n",
        "        raise\n",
        "    finally:\n",
        "        if os.path.exists(\"/tmp/h2o_ice_root\"):\n",
        "            try:\n",
        "                import shutil\n",
        "                shutil.rmtree(\"/tmp/h2o_ice_root\")\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88ff6407-a00b-4b20-9256-63dfa6030516",
      "metadata": {
        "id": "88ff6407-a00b-4b20-9256-63dfa6030516"
      },
      "outputs": [],
      "source": [
        "from xgboost.spark import SparkXGBRegressor\n",
        "\n",
        "def train_xgboost(spark, train_df, test_df):\n",
        "    \"\"\"Train a distributed XGBoost model with GPU support.\"\"\"\n",
        "    # Define feature and label columns\n",
        "    feature_cols = [col for col in train_df.columns if col != \"label\"]\n",
        "    label_col = \"label\"\n",
        "\n",
        "    # Create XGBoost regressor\n",
        "    xgb_regressor = SparkXGBRegressor(\n",
        "        features_col=feature_cols,\n",
        "        label_col=label_col,\n",
        "        num_workers=3,  # Number of GPUs\n",
        "        device=\"cuda\",  # Use GPU\n",
        "        tree_method=\"gpu_hist\",  # GPU-accelerated histogram-based algorithm\n",
        "        max_depth=6,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        objective=\"reg:squarederror\",\n",
        "        eval_metric=\"rmse\"\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    model = xgb_regressor.fit(train_df)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = model.transform(test_df)\n",
        "    predictions.show()\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a608955-7acd-4507-b07a-7c7dc82db5bf",
      "metadata": {
        "id": "9a608955-7acd-4507-b07a-7c7dc82db5bf"
      },
      "outputs": [],
      "source": [
        "from xgboost.spark import SparkXGBRegressor\n",
        "\n",
        "def train_xgboost(spark, train_df, test_df):\n",
        "    \"\"\"Train a distributed XGBoost model with GPU support.\"\"\"\n",
        "    # Define feature and label columns\n",
        "    feature_cols = [col for col in train_df.columns if col != \"label\"]\n",
        "    label_col = \"label\"\n",
        "\n",
        "    # Create XGBoost regressor\n",
        "    xgb_regressor = SparkXGBRegressor(\n",
        "        features_col=feature_cols,\n",
        "        label_col=label_col,\n",
        "        num_workers=3,  # Number of GPUs\n",
        "        device=\"cuda\",  # Use GPU\n",
        "        tree_method=\"gpu_hist\",  # GPU-accelerated histogram-based algorithm\n",
        "        max_depth=6,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        objective=\"reg:squarederror\",\n",
        "        eval_metric=\"rmse\"\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    model = xgb_regressor.fit(train_df)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = model.transform(test_df)\n",
        "    predictions.show()\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32e4c052-4a22-4b50-9181-7da933b273bb",
      "metadata": {
        "id": "32e4c052-4a22-4b50-9181-7da933b273bb"
      },
      "outputs": [],
      "source": [
        "from xgboost.spark import SparkXGBRegressor\n",
        "\n",
        "def train_xgboost(spark, train_df, test_df):\n",
        "    \"\"\"Train a distributed XGBoost model with GPU support.\"\"\"\n",
        "    # Define feature and label columns\n",
        "    feature_cols = [col for col in train_df.columns if col != \"label\"]\n",
        "    label_col = \"label\"\n",
        "\n",
        "    # Create XGBoost regressor\n",
        "    xgb_regressor = SparkXGBRegressor(\n",
        "        features_col=feature_cols,\n",
        "        label_col=label_col,\n",
        "        num_workers=3,  # Number of GPUs\n",
        "        device=\"cuda\",  # Use GPU\n",
        "        tree_method=\"gpu_hist\",  # GPU-accelerated histogram-based algorithm\n",
        "        max_depth=6,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        objective=\"reg:squarederror\",\n",
        "        eval_metric=\"rmse\"\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    model = xgb_regressor.fit(train_df)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = model.transform(test_df)\n",
        "    predictions.show()\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f6ae565-2a8f-41e9-ac4f-c2ae405b110c",
      "metadata": {
        "id": "2f6ae565-2a8f-41e9-ac4f-c2ae405b110c"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "def create_spark_session():\n",
        "    \"\"\"\n",
        "    Create and configure a Spark session.\n",
        "    \"\"\"\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"H2O-Spark Integration\") \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .config(\"spark.executor.memory\", \"200g\") \\\n",
        "        .config(\"spark.driver.memory\", \"2g\") \\\n",
        "        .config(\"spark.executor.cores\", \"1\") \\\n",
        "        .getOrCreate()\n",
        "    return spark\n",
        "\n",
        "def spark_to_h2o(spark_df):\n",
        "    \"\"\"Convert Spark DataFrame to H2O Frame.\"\"\"\n",
        "    pandas_df = spark_df.toPandas()\n",
        "    h2o_frame = h2o.H2OFrame(pandas_df)\n",
        "    return h2o_frame\n",
        "\n",
        "def h2o_to_spark(h2o_frame, spark):\n",
        "    \"\"\"Convert H2O Frame to Spark DataFrame.\"\"\"\n",
        "    pandas_df = h2o_frame.as_data_frame()\n",
        "    spark_df = spark.createDataFrame(pandas_df)\n",
        "    return spark_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "612f359b-9e0a-4d3e-a0c8-1cb7e14d957f",
      "metadata": {
        "id": "612f359b-9e0a-4d3e-a0c8-1cb7e14d957f"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    try:\n",
        "        # Create Spark session\n",
        "        spark = create_spark_session()\n",
        "\n",
        "        # Initialize H2O\n",
        "        init_h2o()\n",
        "\n",
        "        # Load data into Spark DataFrame\n",
        "        train_data_path = \"path/to/train_data.parquet\"\n",
        "        test_data_path = \"path/to/test_data.parquet\"\n",
        "        train_df = spark.read.parquet(train_data_path)\n",
        "        test_df = spark.read.parquet(test_data_path)\n",
        "\n",
        "        # Convert Spark DataFrame to H2O Frame (optional)\n",
        "        h2o_train_frame = spark_to_h2o(train_df)\n",
        "        h2o_test_frame = spark_to_h2o(test_df)\n",
        "\n",
        "        # Train XGBoost model with GPU support\n",
        "        xgb_model = train_xgboost(spark, train_df, test_df)\n",
        "\n",
        "        # Convert H2O Frame back to Spark DataFrame (optional)\n",
        "        spark_train_df = h2o_to_spark(h2o_train_frame, spark)\n",
        "        spark_test_df = h2o_to_spark(h2o_test_frame, spark)\n",
        "\n",
        "        print(\"Workflow completed successfully\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")\n",
        "        raise\n",
        "    finally:\n",
        "        # Shutdown H2O cluster\n",
        "        h2o.cluster().shutdown()\n",
        "        # Stop Spark session\n",
        "        spark.stop()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60931ca2-c78c-4ec1-b272-b99b841ed955",
      "metadata": {
        "id": "60931ca2-c78c-4ec1-b272-b99b841ed955"
      },
      "outputs": [],
      "source": [
        "# Cell 2: Core Processing Functions\n",
        "\n",
        "def analyze_feature_columns(df, prefix, sample_size=1000):\n",
        "    \"\"\"\n",
        "    Analyze feature columns with efficient sampling\n",
        "    \"\"\"\n",
        "    # Get feature columns\n",
        "    feature_cols = sorted([col for col in df.columns\n",
        "                         if col.startswith(prefix + '_') and\n",
        "                         not col.endswith('_array')])\n",
        "\n",
        "    if not feature_cols:\n",
        "        return None\n",
        "\n",
        "    # Count non-null values in first few columns\n",
        "    counts = df.select([\n",
        "        F.count(F.when(F.col(col).isNotNull(), True)).alias(col)\n",
        "        for col in feature_cols[:5]\n",
        "    ]).collect()[0].asDict()\n",
        "\n",
        "    print(f\"\\n{prefix} feature sample counts: {counts}\")\n",
        "\n",
        "    # Extract valid feature numbers\n",
        "    valid_numbers = []\n",
        "    for col in feature_cols:\n",
        "        try:\n",
        "            num = int(col.split('_')[-1])\n",
        "            row_count = df.select(\n",
        "                F.count(F.when(F.col(col).isNotNull(), True))\n",
        "            ).collect()[0][0]\n",
        "            if row_count > 0:\n",
        "                valid_numbers.append(num)\n",
        "        except (ValueError, IndexError):\n",
        "            continue\n",
        "\n",
        "    if valid_numbers:\n",
        "        # Use builtins explicitly to avoid confusion with PySpark functions\n",
        "        import builtins\n",
        "        min_num = builtins.min(valid_numbers)\n",
        "        max_num = builtins.max(valid_numbers)\n",
        "        return min_num, max_num, len(valid_numbers)\n",
        "\n",
        "    return None\n",
        "\n",
        "def get_feature_ranges(df):\n",
        "    \"\"\"\n",
        "    Determine feature ranges dynamically for each feature type.\n",
        "    \"\"\"\n",
        "    feature_ranges = {}\n",
        "    feature_prefixes = ['pvm', 'sentiment', 'onchain']\n",
        "\n",
        "    for prefix in feature_prefixes:\n",
        "        try:\n",
        "            result = analyze_feature_columns(df, prefix)\n",
        "            if result:\n",
        "                min_num, max_num, count = result\n",
        "                feature_ranges[prefix] = (min_num, max_num)\n",
        "                print(f\"Found {count} valid {prefix} features: using range {min_num} to {max_num}\")\n",
        "            else:\n",
        "                print(f\"No valid features found for {prefix}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {prefix} features: {str(e)}\")\n",
        "            traceback.print_exc()\n",
        "            continue\n",
        "\n",
        "    if not feature_ranges:\n",
        "        raise ValueError(\"No valid feature ranges found\")\n",
        "\n",
        "    return feature_ranges\n",
        "\n",
        "def process_feature_group(df, feature_prefix, start_idx, end_idx):\n",
        "    \"\"\"Process features in batches for better memory management.\"\"\"\n",
        "    print(f\"\\nProcessing {feature_prefix} features {start_idx} to {end_idx}\")\n",
        "\n",
        "    cols = [f'{feature_prefix}_{str(i).zfill(4)}' for i in range(start_idx, end_idx + 1)]\n",
        "    existing_cols = [col for col in cols if col in df.columns]\n",
        "\n",
        "    if not existing_cols:\n",
        "        raise ValueError(f\"No valid columns found for {feature_prefix}\")\n",
        "\n",
        "    print(f\"Found {len(existing_cols)} columns for {feature_prefix}\")\n",
        "    monitor_gpu_memory()\n",
        "\n",
        "    try:\n",
        "        # Process in batches\n",
        "        result = None\n",
        "        batch_size = CONFIG['batch_size']\n",
        "\n",
        "        for i in range(0, len(existing_cols), batch_size):\n",
        "            batch_cols = existing_cols[i:min(i + batch_size, len(existing_cols))]\n",
        "\n",
        "            # Process batch\n",
        "            batch_df = df.select(['date', 'symbol'] + batch_cols)\n",
        "            batch_df = batch_df.repartition(200)\n",
        "\n",
        "            for col in batch_cols:\n",
        "                batch_df = batch_df.withColumn(\n",
        "                    col,\n",
        "                    F.when(F.col(col).isNull(), 0.0)\n",
        "                     .otherwise(F.col(col).cast(\"double\"))\n",
        "                )\n",
        "\n",
        "            # Create VectorAssembler for batch\n",
        "            assembler = VectorAssembler(\n",
        "                inputCols=batch_cols,\n",
        "                outputCol=f\"features_{feature_prefix}_{start_idx}_{end_idx}\",\n",
        "                handleInvalid=\"keep\"\n",
        "            )\n",
        "\n",
        "            batch_df = assembler.transform(batch_df)\n",
        "\n",
        "            if result is None:\n",
        "                result = batch_df\n",
        "            else:\n",
        "                result = result.unionAll(batch_df)\n",
        "\n",
        "            # Clean up batch\n",
        "            cleanup_memory()\n",
        "\n",
        "        monitor_gpu_memory()\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {feature_prefix}: {str(e)}\")\n",
        "        cleanup_memory()\n",
        "        raise\n",
        "\n",
        "def load_data_in_chunks(spark, path):\n",
        "    \"\"\"Load and process data in chunks with optimized resource utilization.\"\"\"\n",
        "    chunk_size = CONFIG['chunk_size']\n",
        "    print(f\"\\nLoading data in chunks of {chunk_size} rows...\")\n",
        "\n",
        "    try:\n",
        "        # Get total row count\n",
        "        total_rows = spark.read.parquet(path).count()\n",
        "        print(f\"Total rows to process: {total_rows}\")\n",
        "\n",
        "        # Get GPU count\n",
        "        if NVML_AVAILABLE:\n",
        "            pynvml.nvmlInit()\n",
        "            num_gpus = pynvml.nvmlDeviceGetCount()\n",
        "        else:\n",
        "            num_gpus = 3  # Default to 1 if no GPUs are detected\n",
        "\n",
        "        # Calculate partition size\n",
        "        partition_size = builtins.max(chunk_size // num_gpus, 1000)  # Explicitly use built-in max\n",
        "\n",
        "        chunks = []\n",
        "        for start in range(0, total_rows, chunk_size):\n",
        "            end = start + chunk_size if start + chunk_size < total_rows else total_rows\n",
        "\n",
        "            # Read chunk\n",
        "            chunk = spark.read.parquet(path).repartition(num_gpus * 3).orderBy('date').limit(chunk_size)\n",
        "            chunks.append(chunk)\n",
        "            print(f\"Loaded chunk {len(chunks)}: rows {start} to {end}\")\n",
        "            monitor_gpu_memory()\n",
        "\n",
        "        print(f\"Loaded {len(chunks)} chunks\")\n",
        "\n",
        "        # Union all chunks\n",
        "        result = reduce(DataFrame.unionAll, chunks)\n",
        "        result = result.repartition(num_gpus * 3)\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {str(e)}\")\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "def convert_to_h2o_frames(results, chunk_size=10000):\n",
        "    \"\"\"Convert Spark DataFrames to H2O Frames with chunking.\"\"\"\n",
        "    h2o_frames = {}\n",
        "\n",
        "    for feature_type, result_df in results.items():\n",
        "        try:\n",
        "            feature_col = f\"features_{feature_type}_1_50_array\"\n",
        "\n",
        "            # Process in chunks\n",
        "            total_rows = result_df.count()\n",
        "            num_chunks = (total_rows + chunk_size - 1) // chunk_size\n",
        "\n",
        "            all_chunks = []\n",
        "            for i in range(num_chunks):\n",
        "                # Get chunk of data\n",
        "                chunk_df = result_df.limit(chunk_size).offset(i * chunk_size)\n",
        "                chunk_rows = chunk_df.select(\"date\", \"symbol\", feature_col).collect()\n",
        "\n",
        "                if not chunk_rows:\n",
        "                    continue\n",
        "\n",
        "                # Create chunk DataFrame\n",
        "                chunk_data = []\n",
        "                for row in chunk_rows:\n",
        "                    date = str(row['date']) if row['date'] is not None else ''\n",
        "                    symbol = str(row['symbol']) if row['symbol'] is not None else ''\n",
        "                    features = row[feature_col]\n",
        "\n",
        "                    if features is None:\n",
        "                        features = [np.nan] * 50\n",
        "                    else:\n",
        "                        features = (features + [np.nan] * 50)[:50]\n",
        "\n",
        "                    features = [None if pd.isna(f) else float(f) for f in features]\n",
        "                    chunk_data.append([date, symbol] + features)\n",
        "\n",
        "                all_chunks.extend(chunk_data)\n",
        "\n",
        "            # Convert all chunks to H2O Frame\n",
        "            if all_chunks:\n",
        "                columns = ['date', 'symbol'] + [\n",
        "                    f'{feature_type}_{i+1:04d}'\n",
        "                    for i in range(len(all_chunks[0])-2)\n",
        "                ]\n",
        "\n",
        "                pandas_df = pd.DataFrame(all_chunks, columns=columns)\n",
        "                pandas_df = pandas_df[\n",
        "                    (pandas_df['date'] != '') &\n",
        "                    (pandas_df['symbol'] != '')\n",
        "                ]\n",
        "\n",
        "                h2o_frame = h2o.H2OFrame(pandas_df)\n",
        "                h2o_frames[feature_type] = h2o_frame\n",
        "                print(f\"Successfully created H2O frame for {feature_type}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error importing {feature_type} to H2O Frame: {e}\")\n",
        "            traceback.print_exc()\n",
        "\n",
        "    return h2o_frames\n",
        "\n",
        "def run_enhanced_automl(h2o_frames, target_strategy='random'):\n",
        "    \"\"\"Run AutoML on H2O frames with enhanced group analysis and combined predictions\"\"\"\n",
        "    def add_target_column(h2o_frames, target_strategy='random'):\n",
        "        \"\"\"Add target column to H2O frames.\"\"\"\n",
        "        frames_with_target = h2o_frames.copy()\n",
        "\n",
        "        for feature_type, h2o_frame in frames_with_target.items():\n",
        "            num_rows = h2o_frame.nrows\n",
        "\n",
        "            if target_strategy == 'random':\n",
        "                target = h2o.H2OFrame(np.random.randint(0, 2, size=(num_rows, 1)),\n",
        "                                    column_names=['target'])\n",
        "                target = target.asfactor()\n",
        "            elif target_strategy == 'symbol_based':\n",
        "                target = h2o.H2OFrame(\n",
        "                    h2o_frame['symbol']\n",
        "                        .apply(lambda x: 1 if '$' in str(x) else 0)\n",
        "                        .as_data_frame()\n",
        "                ).set_names(['target']).asfactor()\n",
        "            elif target_strategy == 'date_based':\n",
        "                target = h2o.H2OFrame(\n",
        "                    h2o_frame['date']\n",
        "                        .apply(lambda x: 1 if str(x)[:4] > '2020' else 0)\n",
        "                        .as_data_frame()\n",
        "                ).set_names(['target']).asfactor()\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown target strategy: {target_strategy}\")\n",
        "\n",
        "            h2o_frame['target'] = target\n",
        "            frames_with_target[feature_type] = h2o_frame\n",
        "\n",
        "        return frames_with_target\n",
        "\n",
        "    def create_combined_frame(frames_with_target):\n",
        "        \"\"\"Create a combined H2O frame with features from all groups.\"\"\"\n",
        "        print(\"\\nCreating combined feature frame...\")\n",
        "        try:\n",
        "            # Get the first frame to use as base\n",
        "            base_frame = next(iter(frames_with_target.values()))\n",
        "\n",
        "            # Convert base frame to pandas first\n",
        "            base_pd = base_frame.as_data_frame()\n",
        "\n",
        "            # Initialize combined data with core columns\n",
        "            combined_data = base_pd[['date', 'symbol']].copy()\n",
        "\n",
        "            # Add target column\n",
        "            target_pd = base_frame['target'].as_data_frame()\n",
        "            combined_data['target'] = target_pd\n",
        "\n",
        "            # Add features from each group with prefixes\n",
        "            for feature_type, h2o_frame in frames_with_target.items():\n",
        "                feature_cols = [col for col in h2o_frame.columns\n",
        "                              if col not in ['date', 'symbol', 'target']]\n",
        "\n",
        "                if not feature_cols:\n",
        "                    print(f\"Warning: No feature columns found for {feature_type}\")\n",
        "                    continue\n",
        "\n",
        "                # Convert feature columns to pandas\n",
        "                feature_df = h2o_frame[feature_cols].as_data_frame()\n",
        "\n",
        "                # Add prefix to column names\n",
        "                feature_df.columns = [f\"{feature_type}_{col}\" for col in feature_cols]\n",
        "\n",
        "                print(f\"\\nAdding {len(feature_cols)} features from {feature_type}\")\n",
        "                print(\"Sample of new feature names:\", list(feature_df.columns)[:3])\n",
        "\n",
        "                # Join with combined data\n",
        "                combined_data = pd.concat([combined_data, feature_df], axis=1)\n",
        "\n",
        "            print(f\"\\nFinal combined frame shape: {combined_data.shape}\")\n",
        "            print(f\"Total features: {combined_data.shape[1] - 3}\")\n",
        "\n",
        "            # Convert back to H2O Frame\n",
        "            combined_h2o = h2o.H2OFrame(combined_data)\n",
        "            combined_h2o['target'] = combined_h2o['target'].asfactor()\n",
        "\n",
        "            return combined_h2o\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in create_combined_frame: {str(e)}\")\n",
        "            traceback.print_exc()\n",
        "            return None\n",
        "\n",
        "    def run_automl_for_frame(train, valid, name, max_runtime_secs=1200):\n",
        "        \"\"\"Run AutoML for a specific frame with comprehensive settings.\"\"\"\n",
        "        print(f\"\\nRunning AutoML for {name}\")\n",
        "\n",
        "        train['target'] = train['target'].asfactor()\n",
        "        valid['target'] = valid['target'].asfactor()\n",
        "\n",
        "        predictors = [col for col in train.columns\n",
        "                     if col not in ['date', 'symbol', 'target']]\n",
        "\n",
        "        aml = H2OAutoML(\n",
        "            max_runtime_secs=max_runtime_secs,\n",
        "            seed=1234,\n",
        "            sort_metric='AUC',\n",
        "            max_models=CONFIG['max_models'],\n",
        "            stopping_metric='AUC',\n",
        "            nfolds=CONFIG['nfolds'],\n",
        "            keep_cross_validation_predictions=True,\n",
        "            verbosity='debug'\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            aml.train(\n",
        "                x=predictors,\n",
        "                y='target',\n",
        "                training_frame=train,\n",
        "                validation_frame=valid\n",
        "            )\n",
        "\n",
        "            best_model = aml.leader\n",
        "            performance = best_model.model_performance(valid)\n",
        "\n",
        "            # Enhanced model analysis with better error handling\n",
        "            try:\n",
        "                cv_metrics = None\n",
        "                if hasattr(best_model, 'cross_validation_metrics'):\n",
        "                    try:\n",
        "                        cv_metrics = best_model.cross_validation_metrics()\n",
        "                    except:\n",
        "                        print(f\"Warning: Could not get cross validation metrics for {name}\")\n",
        "\n",
        "                variable_importance = None\n",
        "                if hasattr(best_model, 'varimp'):\n",
        "                    try:\n",
        "                        variable_importance = best_model.varimp()\n",
        "                    except:\n",
        "                        print(f\"Warning: Could not get variable importance for {name}\")\n",
        "\n",
        "                model_analysis = {\n",
        "                    'automl': aml,\n",
        "                    'best_model': best_model,\n",
        "                    'leaderboard': aml.leaderboard,\n",
        "                    'performance': performance,\n",
        "                    'cv_metrics': cv_metrics,\n",
        "                    'variable_importance': variable_importance\n",
        "                }\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Error in model analysis for {name}: {str(e)}\")\n",
        "                model_analysis = {\n",
        "                    'automl': aml,\n",
        "                    'best_model': best_model,\n",
        "                    'leaderboard': aml.leaderboard,\n",
        "                    'performance': performance,\n",
        "                    'cv_metrics': None,\n",
        "                    'variable_importance': None\n",
        "                }\n",
        "\n",
        "            print(f\"\\nBest {name} Model Performance:\")\n",
        "            print(performance)\n",
        "            print(\"\\nConfusion Matrix:\")\n",
        "            print(performance.confusion_matrix())\n",
        "            print(f\"\\n{name} Leaderboard:\")\n",
        "            print(aml.leaderboard)\n",
        "\n",
        "            return model_analysis\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in AutoML for {name}: {e}\")\n",
        "            traceback.print_exc()\n",
        "            return None\n",
        "\n",
        "    # Add target column to all frames\n",
        "    frames_with_target = add_target_column(h2o_frames, target_strategy)\n",
        "\n",
        "    # Create combined frame\n",
        "    combined_frame = create_combined_frame(frames_with_target)\n",
        "\n",
        "    # Results dictionary for all models\n",
        "    all_results = {\n",
        "        'individual_models': {},\n",
        "        'combined_model': None,\n",
        "        'ensemble_predictions': None,\n",
        "        'feature_analysis': {}\n",
        "    }\n",
        "\n",
        "    # Run AutoML for each feature type\n",
        "    for feature_type, h2o_frame in frames_with_target.items():\n",
        "        train, valid = h2o_frame.split_frame(ratios=[0.8], seed=1234)\n",
        "        result = run_automl_for_frame(train, valid, feature_type)\n",
        "\n",
        "        if result:\n",
        "            all_results['individual_models'][feature_type] = result\n",
        "\n",
        "    # Run AutoML on combined features\n",
        "    if combined_frame is not None:\n",
        "        train_combined, valid_combined = combined_frame.split_frame(ratios=[0.8], seed=1234)\n",
        "        combined_result = run_automl_for_frame(\n",
        "            train_combined,\n",
        "            valid_combined,\n",
        "            \"combined_features\",\n",
        "            max_runtime_secs=CONFIG['max_runtime_secs_combined']\n",
        "        )\n",
        "\n",
        "        if combined_result:\n",
        "            all_results['combined_model'] = combined_result\n",
        "\n",
        "    # Create ensemble predictions\n",
        "    if all_results['individual_models']:\n",
        "        print(\"\\nCreating ensemble predictions...\")\n",
        "        ensemble_predictions = {}\n",
        "        weights = {}\n",
        "\n",
        "        # Calculate weights based on model performance\n",
        "        for feature_type, result in all_results['individual_models'].items():\n",
        "            auc = result['performance'].auc()\n",
        "            weights[feature_type] = auc\n",
        "\n",
        "        # Normalize weights\n",
        "        total_weight = sum(weights.values())\n",
        "        weights = {k: v/total_weight for k, v in weights.items()}\n",
        "\n",
        "        # Get predictions from all models\n",
        "        for feature_type, result in all_results['individual_models'].items():\n",
        "            predictions = result['best_model'].predict(valid_combined)\n",
        "            ensemble_predictions[feature_type] = predictions\n",
        "\n",
        "        # Calculate weighted ensemble predictions\n",
        "        weighted_preds = None\n",
        "        for feature_type, preds in ensemble_predictions.items():\n",
        "            if weighted_preds is None:\n",
        "                weighted_preds = preds * weights[feature_type]\n",
        "            else:\n",
        "                weighted_preds += preds * weights[feature_type]\n",
        "\n",
        "        all_results['ensemble_predictions'] = {\n",
        "            'predictions': weighted_preds,\n",
        "            'weights': weights\n",
        "        }\n",
        "\n",
        "    # Analyze feature importance across all models\n",
        "    feature_importance = {}\n",
        "    for feature_type, result in all_results['individual_models'].items():\n",
        "        if result['variable_importance'] is not None:\n",
        "            feature_importance[feature_type] = result['variable_importance']\n",
        "\n",
        "    if all_results['combined_model'] and all_results['combined_model']['variable_importance'] is not None:\n",
        "        feature_importance['combined'] = all_results['combined_model']['variable_importance']\n",
        "\n",
        "    all_results['feature_analysis'] = feature_importance\n",
        "\n",
        "    return all_results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1a8ec27-0961-40ca-a243-f75099cd1649",
      "metadata": {
        "id": "b1a8ec27-0961-40ca-a243-f75099cd1649"
      },
      "source": [
        "## oude code voor deepseek.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e0a71fb-4d81-4ca5-a13a-773ebc8da2ba",
      "metadata": {
        "id": "8e0a71fb-4d81-4ca5-a13a-773ebc8da2ba"
      },
      "outputs": [],
      "source": [
        "from pysparkling import H2OContext\n",
        "\n",
        "import torch\n",
        "import pynvml\n",
        "\n",
        "def clear_gpu_memory():\n",
        "    \"\"\"Clear GPU memory.\"\"\"\n",
        "    try:\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            print(\"PyTorch GPU cache cleared.\")\n",
        "\n",
        "        if pynvml:\n",
        "            pynvml.nvmlInit()\n",
        "            device_count = pynvml.nvmlDeviceGetCount()\n",
        "            for i in range(device_count):\n",
        "                handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
        "                mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
        "                print(f\"Before Cleanup - GPU {i}: Used {mem_info.used / 1024**3:.2f} GB\")\n",
        "                # Reset compute mode (if necessary)\n",
        "                try:\n",
        "                    pynvml.nvmlDeviceSetComputeMode(handle, pynvml.NVML_COMPUTEMODE_DEFAULT)\n",
        "                except pynvml.NVMLError as e:\n",
        "                    print(f\"Could not reset compute mode for GPU {i}: {str(e)}\")\n",
        "                mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
        "                print(f\"After Cleanup - GPU {i}: Used {mem_info.used / 1024**3:.2f} GB\")\n",
        "        else:\n",
        "            print(\"pynvml not available.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during GPU memory cleanup: {str(e)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab517391-2a67-45d0-a1b2-0a28dd8b67bb",
      "metadata": {
        "id": "ab517391-2a67-45d0-a1b2-0a28dd8b67bb",
        "outputId": "fb93fc21-4351-41dd-ce67-e09cdb1fe6d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n%%time\\n# Cell 3: Main Execution\\n\\ndef analyze_results(enhanced_results):\\n    \"\"\"Analyze and visualize model results.\"\"\"\\n    if not enhanced_results:\\n        print(\"No results to analyze\")\\n        return\\n    \\n    try:\\n        # Create performance comparison plots\\n        plt.figure(figsize=(12, 6))\\n        \\n        # Model performance comparison\\n        aucs = []\\n        accuracies = []\\n        model_names = []\\n        \\n        # Gather metrics from individual models\\n        for feature_type, results in enhanced_results[\\'individual_models\\'].items():\\n            aucs.append(results[\\'performance\\'].auc())\\n            accuracies.append(results[\\'performance\\'].accuracy())\\n            model_names.append(feature_type)\\n        \\n        # Add combined model metrics if available\\n        if enhanced_results.get(\\'combined_model\\'):\\n            aucs.append(enhanced_results[\\'combined_model\\'][\\'performance\\'].auc())\\n            accuracies.append(enhanced_results[\\'combined_model\\'][\\'performance\\'].accuracy())\\n            model_names.append(\\'combined\\')\\n        \\n        # Plot performance metrics\\n        x = np.arange(len(model_names))\\n        width = 0.35\\n        \\n        fig, ax = plt.subplots(figsize=(10, 6))\\n        ax.bar(x - width/2, aucs, width, label=\\'AUC\\')\\n        ax.bar(x + width/2, accuracies, width, label=\\'Accuracy\\')\\n        \\n        ax.set_ylabel(\\'Score\\')\\n        ax.set_title(\\'Model Performance Comparison\\')\\n        ax.set_xticks(x)\\n        ax.set_xticklabels(model_names)\\n        ax.legend()\\n        \\n        plt.tight_layout()\\n        plt.show()\\n        \\n        # Feature importance visualization\\n        if enhanced_results.get(\\'feature_analysis\\'):\\n            for model_type, importance in enhanced_results[\\'feature_analysis\\'].items():\\n                if importance is not None:\\n                    plt.figure(figsize=(10, 6))\\n                    if isinstance(importance, pd.DataFrame):\\n                        importance.head(10).plot(kind=\\'barh\\')\\n                    else:\\n                        imp_df = pd.DataFrame(importance).head(10)\\n                        imp_df.plot(kind=\\'barh\\')\\n                    plt.title(f\\'{model_type} Feature Importance\\')\\n                    plt.tight_layout()\\n                    plt.show()\\n    except Exception as e:\\n        print(f\"Error in result analysis: {str(e)}\")\\n        traceback.print_exc()\\n\\ndef main():\\n    \"\"\"Main execution function with memory management.\"\"\"\\n    spark = None\\n    h2o_conn = None\\n    ice_dir = None\\n    cached_dfs = []\\n    \\n    try:\\n        # Setup environment\\n        if not setup_environment():\\n            print(\"Environment setup failed.\")\\n            return None, None, None, None, None\\n        \\n        # Create Spark session and initialize H2O\\n        spark = create_spark_session()\\n        h2o_conn, ice_dir = initialize_h2o()\\n        \\n        if not h2o_conn:\\n            print(\"Failed to initialize H2O.\")\\n            return None, None, None, None, None\\n        \\n        # Load data in chunks\\n        print(\"\\nLoading data...\")\\n        absolute_path = os.path.abspath(\\'yiedl_latest.parquet\\')\\n        df = load_data_in_chunks(spark, \"file://\" + absolute_path)\\n        \\n        if df is None:\\n            print(\"Failed to load data.\")\\n            return None, None, None, None, None\\n            \\n        df.cache()\\n        cached_dfs.append(df)\\n        \\n        # Get feature ranges\\n        feature_types = get_feature_ranges(df)\\n        if not feature_types:\\n            print(\"No features found to process.\")\\n            return None, None, None, None, None\\n            \\n        print(\"\\nDetected feature ranges:\", feature_types)\\n        \\n        # Process features\\n        results = {}\\n        for feature_type, (start, end) in feature_types.items():\\n            try:\\n                result_df = process_feature_group(df, feature_type, start, end)\\n                result_df.cache()\\n                cached_dfs.append(result_df)\\n                results[feature_type] = result_df\\n                \\n                feature_col = f\"features_{feature_type}_{start}_{end}_array\"\\n                print(f\"\\n{feature_type.upper()} Features Sample:\")\\n                result_df.select(\"date\", \"symbol\", feature_col).show(5, truncate=False)\\n                \\n            except Exception as e:\\n                print(f\"Error processing {feature_type}: {str(e)}\")\\n                continue\\n        \\n        if not results:\\n            print(\"No features were successfully processed.\")\\n            return None, None, None, None, None\\n        \\n        # Convert to H2O frames\\n        print(\"\\nConverting to H2O frames...\")\\n        h2o_frames = convert_to_h2o_frames(results)\\n        \\n        if not h2o_frames:\\n            print(\"Failed to create H2O frames.\")\\n            return None, None, None, None, None\\n        \\n        # Run Enhanced AutoML\\n        print(\"\\nRunning Enhanced AutoML with combined features and ensemble predictions...\")\\n        enhanced_results = run_enhanced_automl(h2o_frames, target_strategy=\\'random\\')\\n        \\n        if enhanced_results:\\n            print(\"\\nModel Performance Summary:\")\\n            \\n            # Individual model results\\n            for feature_type, model_results in enhanced_results[\\'individual_models\\'].items():\\n                print(f\"\\n{feature_type.upper()} Model:\")\\n                print(f\"AUC: {model_results[\\'performance\\'].auc():.4f}\")\\n                print(f\"Accuracy: {model_results[\\'performance\\'].accuracy():.4f}\")\\n                if model_results.get(\\'cv_metrics\\'):\\n                    print(f\"CV AUC: {model_results[\\'cv_metrics\\'].auc():.4f}\")\\n            \\n            # Combined model results\\n            if enhanced_results.get(\\'combined_model\\'):\\n                print(\"\\nCombined Model:\")\\n                combined_perf = enhanced_results[\\'combined_model\\'][\\'performance\\']\\n                print(f\"AUC: {combined_perf.auc():.4f}\")\\n                print(f\"Accuracy: {combined_perf.accuracy():.4f}\")\\n            \\n            # Ensemble results\\n            if enhanced_results.get(\\'ensemble_predictions\\'):\\n                print(\"\\nEnsemble Weights:\")\\n                for model_type, weight in enhanced_results[\\'ensemble_predictions\\'][\\'weights\\'].items():\\n                    print(f\"{model_type}: {weight:.4f}\")\\n        \\n        return spark, h2o_conn, results, h2o_frames, enhanced_results\\n    \\n    except Exception as e:\\n        print(f\"Processing error: {str(e)}\")\\n        traceback.print_exc()\\n        return None, None, None, None, None\\n    \\n    finally:\\n        # Cleanup\\n        print(\"\\nPerforming cleanup...\")\\n        cleanup_memory()\\n        \\n        # Unpersist cached DataFrames\\n        for cached_df in cached_dfs:\\n            try:\\n                if cached_df is not None:\\n                    cached_df.unpersist()\\n            except Exception as e:\\n                print(f\"Error unpersisting DataFrame: {str(e)}\")\\n        \\n        # Cleanup H2O and remove ice directory\\n        try:\\n            if h2o.connection():\\n                print(\"Shutting down H2O cluster...\")\\n                h2o.cluster().shutdown()\\n            if ice_dir and os.path.exists(ice_dir):\\n                print(f\"Removing H2O ice directory: {ice_dir}\")\\n                import shutil\\n                shutil.rmtree(ice_dir)\\n        except Exception as e:\\n            print(f\"H2O cleanup error: {str(e)}\")\\n        \\n        # Stop Spark\\n        try:\\n            if spark:\\n                spark.stop()\\n                print(\"Spark session stopped successfully\")\\n        except Exception as e:\\n            print(f\"Error stopping Spark: {str(e)}\")\\n\\nif __name__ == \"__main__\":\\n    # Run main process\\n    spark, h2o_conn, results, h2o_frames, enhanced_results = main()\\n    \\n    # Analyze results if available\\n    if enhanced_results:\\n        analyze_results(enhanced_results)\\n'"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "%%time\n",
        "# Cell 3: Main Execution\n",
        "\n",
        "def analyze_results(enhanced_results):\n",
        "    \"\"\"Analyze and visualize model results.\"\"\"\n",
        "    if not enhanced_results:\n",
        "        print(\"No results to analyze\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # Create performance comparison plots\n",
        "        plt.figure(figsize=(12, 6))\n",
        "\n",
        "        # Model performance comparison\n",
        "        aucs = []\n",
        "        accuracies = []\n",
        "        model_names = []\n",
        "\n",
        "        # Gather metrics from individual models\n",
        "        for feature_type, results in enhanced_results['individual_models'].items():\n",
        "            aucs.append(results['performance'].auc())\n",
        "            accuracies.append(results['performance'].accuracy())\n",
        "            model_names.append(feature_type)\n",
        "\n",
        "        # Add combined model metrics if available\n",
        "        if enhanced_results.get('combined_model'):\n",
        "            aucs.append(enhanced_results['combined_model']['performance'].auc())\n",
        "            accuracies.append(enhanced_results['combined_model']['performance'].accuracy())\n",
        "            model_names.append('combined')\n",
        "\n",
        "        # Plot performance metrics\n",
        "        x = np.arange(len(model_names))\n",
        "        width = 0.35\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        ax.bar(x - width/2, aucs, width, label='AUC')\n",
        "        ax.bar(x + width/2, accuracies, width, label='Accuracy')\n",
        "\n",
        "        ax.set_ylabel('Score')\n",
        "        ax.set_title('Model Performance Comparison')\n",
        "        ax.set_xticks(x)\n",
        "        ax.set_xticklabels(model_names)\n",
        "        ax.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Feature importance visualization\n",
        "        if enhanced_results.get('feature_analysis'):\n",
        "            for model_type, importance in enhanced_results['feature_analysis'].items():\n",
        "                if importance is not None:\n",
        "                    plt.figure(figsize=(10, 6))\n",
        "                    if isinstance(importance, pd.DataFrame):\n",
        "                        importance.head(10).plot(kind='barh')\n",
        "                    else:\n",
        "                        imp_df = pd.DataFrame(importance).head(10)\n",
        "                        imp_df.plot(kind='barh')\n",
        "                    plt.title(f'{model_type} Feature Importance')\n",
        "                    plt.tight_layout()\n",
        "                    plt.show()\n",
        "    except Exception as e:\n",
        "        print(f\"Error in result analysis: {str(e)}\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function with memory management.\"\"\"\n",
        "    spark = None\n",
        "    h2o_conn = None\n",
        "    ice_dir = None\n",
        "    cached_dfs = []\n",
        "\n",
        "    try:\n",
        "        # Setup environment\n",
        "        if not setup_environment():\n",
        "            print(\"Environment setup failed.\")\n",
        "            return None, None, None, None, None\n",
        "\n",
        "        # Create Spark session and initialize H2O\n",
        "        spark = create_spark_session()\n",
        "        h2o_conn, ice_dir = initialize_h2o()\n",
        "\n",
        "        if not h2o_conn:\n",
        "            print(\"Failed to initialize H2O.\")\n",
        "            return None, None, None, None, None\n",
        "\n",
        "        # Load data in chunks\n",
        "        print(\"\\nLoading data...\")\n",
        "        absolute_path = os.path.abspath('yiedl_latest.parquet')\n",
        "        df = load_data_in_chunks(spark, \"file://\" + absolute_path)\n",
        "\n",
        "        if df is None:\n",
        "            print(\"Failed to load data.\")\n",
        "            return None, None, None, None, None\n",
        "\n",
        "        df.cache()\n",
        "        cached_dfs.append(df)\n",
        "\n",
        "        # Get feature ranges\n",
        "        feature_types = get_feature_ranges(df)\n",
        "        if not feature_types:\n",
        "            print(\"No features found to process.\")\n",
        "            return None, None, None, None, None\n",
        "\n",
        "        print(\"\\nDetected feature ranges:\", feature_types)\n",
        "\n",
        "        # Process features\n",
        "        results = {}\n",
        "        for feature_type, (start, end) in feature_types.items():\n",
        "            try:\n",
        "                result_df = process_feature_group(df, feature_type, start, end)\n",
        "                result_df.cache()\n",
        "                cached_dfs.append(result_df)\n",
        "                results[feature_type] = result_df\n",
        "\n",
        "                feature_col = f\"features_{feature_type}_{start}_{end}_array\"\n",
        "                print(f\"\\n{feature_type.upper()} Features Sample:\")\n",
        "                result_df.select(\"date\", \"symbol\", feature_col).show(5, truncate=False)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {feature_type}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        if not results:\n",
        "            print(\"No features were successfully processed.\")\n",
        "            return None, None, None, None, None\n",
        "\n",
        "        # Convert to H2O frames\n",
        "        print(\"\\nConverting to H2O frames...\")\n",
        "        h2o_frames = convert_to_h2o_frames(results)\n",
        "\n",
        "        if not h2o_frames:\n",
        "            print(\"Failed to create H2O frames.\")\n",
        "            return None, None, None, None, None\n",
        "\n",
        "        # Run Enhanced AutoML\n",
        "        print(\"\\nRunning Enhanced AutoML with combined features and ensemble predictions...\")\n",
        "        enhanced_results = run_enhanced_automl(h2o_frames, target_strategy='random')\n",
        "\n",
        "        if enhanced_results:\n",
        "            print(\"\\nModel Performance Summary:\")\n",
        "\n",
        "            # Individual model results\n",
        "            for feature_type, model_results in enhanced_results['individual_models'].items():\n",
        "                print(f\"\\n{feature_type.upper()} Model:\")\n",
        "                print(f\"AUC: {model_results['performance'].auc():.4f}\")\n",
        "                print(f\"Accuracy: {model_results['performance'].accuracy():.4f}\")\n",
        "                if model_results.get('cv_metrics'):\n",
        "                    print(f\"CV AUC: {model_results['cv_metrics'].auc():.4f}\")\n",
        "\n",
        "            # Combined model results\n",
        "            if enhanced_results.get('combined_model'):\n",
        "                print(\"\\nCombined Model:\")\n",
        "                combined_perf = enhanced_results['combined_model']['performance']\n",
        "                print(f\"AUC: {combined_perf.auc():.4f}\")\n",
        "                print(f\"Accuracy: {combined_perf.accuracy():.4f}\")\n",
        "\n",
        "            # Ensemble results\n",
        "            if enhanced_results.get('ensemble_predictions'):\n",
        "                print(\"\\nEnsemble Weights:\")\n",
        "                for model_type, weight in enhanced_results['ensemble_predictions']['weights'].items():\n",
        "                    print(f\"{model_type}: {weight:.4f}\")\n",
        "\n",
        "        return spark, h2o_conn, results, h2o_frames, enhanced_results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Processing error: {str(e)}\")\n",
        "        traceback.print_exc()\n",
        "        return None, None, None, None, None\n",
        "\n",
        "    finally:\n",
        "        # Cleanup\n",
        "        print(\"\\nPerforming cleanup...\")\n",
        "        cleanup_memory()\n",
        "\n",
        "        # Unpersist cached DataFrames\n",
        "        for cached_df in cached_dfs:\n",
        "            try:\n",
        "                if cached_df is not None:\n",
        "                    cached_df.unpersist()\n",
        "            except Exception as e:\n",
        "                print(f\"Error unpersisting DataFrame: {str(e)}\")\n",
        "\n",
        "        # Cleanup H2O and remove ice directory\n",
        "        try:\n",
        "            if h2o.connection():\n",
        "                print(\"Shutting down H2O cluster...\")\n",
        "                h2o.cluster().shutdown()\n",
        "            if ice_dir and os.path.exists(ice_dir):\n",
        "                print(f\"Removing H2O ice directory: {ice_dir}\")\n",
        "                import shutil\n",
        "                shutil.rmtree(ice_dir)\n",
        "        except Exception as e:\n",
        "            print(f\"H2O cleanup error: {str(e)}\")\n",
        "\n",
        "        # Stop Spark\n",
        "        try:\n",
        "            if spark:\n",
        "                spark.stop()\n",
        "                print(\"Spark session stopped successfully\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error stopping Spark: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run main process\n",
        "    spark, h2o_conn, results, h2o_frames, enhanced_results = main()\n",
        "\n",
        "    # Analyze results if available\n",
        "    if enhanced_results:\n",
        "        analyze_results(enhanced_results)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e405136b-79ce-40a4-a0a3-0b26f5673430",
      "metadata": {
        "id": "e405136b-79ce-40a4-a0a3-0b26f5673430"
      },
      "source": [
        "### old cells below for research using claude.ai en chatgpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "460c9189-a3c1-4fc8-8cfa-5de67b774a86",
      "metadata": {
        "id": "460c9189-a3c1-4fc8-8cfa-5de67b774a86"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.sql import functions as F\n",
        "import h2o\n",
        "from h2o.automl import H2OAutoML\n",
        "import traceback\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import socket\n",
        "import subprocess\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def setup_environment():\n",
        "    \"\"\"\n",
        "    Set up Python environment with necessary dependencies.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if setup is successful, False otherwise\n",
        "    \"\"\"\n",
        "    print(\"Setting up Python environment...\")\n",
        "\n",
        "    # List of required packages\n",
        "    required_packages = [\n",
        "        'pyspark',\n",
        "        'h2o',\n",
        "        'numpy',\n",
        "        'pandas'\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        # Verify and install required packages\n",
        "        for package in required_packages:\n",
        "            try:\n",
        "                __import__(package.replace('-', '_'))\n",
        "                print(f\"{package} is already installed.\")\n",
        "            except ImportError:\n",
        "                print(f\"Installing {package}...\")\n",
        "                subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Environment setup error: {e}\")\n",
        "        return False\n",
        "\n",
        "def get_local_ip():\n",
        "    \"\"\"Retrieve the local IP address.\"\"\"\n",
        "    try:\n",
        "        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
        "        s.connect((\"8.8.8.8\", 80))\n",
        "        ip = s.getsockname()[0]\n",
        "        s.close()\n",
        "        return ip\n",
        "    except Exception:\n",
        "        return \"127.0.0.1\"\n",
        "\n",
        "def create_spark_session():\n",
        "    \"\"\"\n",
        "    Create a comprehensive Spark session with optimal configurations.\n",
        "\n",
        "    Returns:\n",
        "        SparkSession: Configured Spark session\n",
        "    \"\"\"\n",
        "    from pyspark.sql import SparkSession\n",
        "\n",
        "    # Get local IP for configuration\n",
        "    local_ip = get_local_ip()\n",
        "\n",
        "    # Create Spark session with comprehensive configurations\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"SparkH2ODataProcessor\") \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .config(\"spark.driver.host\", local_ip) \\\n",
        "        .config(\"spark.driver.bindAddress\", local_ip) \\\n",
        "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\") \\\n",
        "        .config(\"spark.driver.memory\", \"180g\") \\\n",
        "        .config(\"spark.executor.memory\", \"180g\") \\\n",
        "        .config(\"spark.network.timeout\", \"3600s\") \\\n",
        "        .config(\"spark.executor.heartbeatInterval\", \"120s\") \\\n",
        "        .config(\"spark.sql.session.timeZone\", \"UTC\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "    return spark\n",
        "\n",
        "def initialize_h2o_context():\n",
        "    \"\"\"\n",
        "    Initialize H2O context with robust error handling.\n",
        "\n",
        "    Returns:\n",
        "        H2OContext or None: Initialized H2O context\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"Initializing H2O...\")\n",
        "\n",
        "        h2o.init(\n",
        "            ip=\"localhost\",\n",
        "            port=54321,\n",
        "            strict_version_check=False,\n",
        "            max_mem_size=\"180G\"\n",
        "        )\n",
        "\n",
        "        return h2o.connection()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"H2O initialization error: {e}\")\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "def process_feature_group(df, feature_prefix, start_idx, end_idx):\n",
        "    \"\"\"\n",
        "    Process a group of features and return transformed dataframe.\n",
        "    \"\"\"\n",
        "    from pyspark.ml.feature import VectorAssembler\n",
        "    from pyspark.sql.types import ArrayType, DoubleType\n",
        "\n",
        "    # Generate column names\n",
        "    cols = [f'{feature_prefix}_{str(i).zfill(4)}' for i in range(start_idx, end_idx + 1)]\n",
        "\n",
        "    # Select relevant columns and cast to double\n",
        "    df_subset = df.select(['date', 'symbol'] + cols)\n",
        "\n",
        "    for col_name in cols:\n",
        "        df_subset = df_subset.withColumn(col_name, F.col(col_name).cast(\"double\"))\n",
        "\n",
        "    # Vector to array conversion function\n",
        "    def vector_to_array(vector):\n",
        "        return vector.toArray().tolist() if vector is not None else None\n",
        "    vector_to_array_udf = F.udf(vector_to_array, ArrayType(DoubleType()))\n",
        "\n",
        "    # Create VectorAssembler\n",
        "    assembler = VectorAssembler(\n",
        "        inputCols=cols,\n",
        "        outputCol=f\"features_{feature_prefix}_{start_idx}_{end_idx}\",\n",
        "        handleInvalid=\"keep\"\n",
        "    )\n",
        "\n",
        "    # Transform and add array column\n",
        "    result = assembler.transform(df_subset).withColumn(\n",
        "        f\"features_{feature_prefix}_{start_idx}_{end_idx}_array\",\n",
        "        vector_to_array_udf(f\"features_{feature_prefix}_{start_idx}_{end_idx}\")\n",
        "    )\n",
        "\n",
        "    return result\n",
        "\n",
        "def convert_to_h2o_frames(results):\n",
        "    \"\"\"\n",
        "    Convert Spark DataFrames to H2O Frames with robust handling.\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "\n",
        "    h2o_frames = {}\n",
        "    for feature_type, result_df in results.items():\n",
        "        feature_col = f\"features_{feature_type}_1_50_array\"\n",
        "\n",
        "        # Collect rows and prepare data\n",
        "        rows = result_df.select(\"date\", \"symbol\", feature_col).collect()\n",
        "\n",
        "        # Create column names\n",
        "        columns = ['date', 'symbol'] + [f'{feature_type}_{i+1:04d}' for i in range(50)]\n",
        "\n",
        "        # Prepare data for conversion\n",
        "        data_rows = []\n",
        "        for row in rows:\n",
        "            # Safely extract values\n",
        "            date = str(row['date']) if row['date'] is not None else ''\n",
        "            symbol = str(row['symbol']) if row['symbol'] is not None else ''\n",
        "            features = row[feature_col]\n",
        "\n",
        "            # Ensure features is a list of 50 elements\n",
        "            if features is None:\n",
        "                features = [np.nan] * 50\n",
        "            else:\n",
        "                features = (features + [np.nan] * 50)[:50]\n",
        "\n",
        "            # Validate and convert features\n",
        "            features = [\n",
        "                None if pd.isna(f) else float(f)\n",
        "                for f in features\n",
        "            ]\n",
        "\n",
        "            data_rows.append([date, symbol] + features)\n",
        "\n",
        "        # Convert to Pandas DataFrame\n",
        "        pandas_df = pd.DataFrame(data_rows, columns=columns)\n",
        "\n",
        "        # Remove any rows with completely empty date or symbol\n",
        "        pandas_df = pandas_df[\n",
        "            (pandas_df['date'] != '') &\n",
        "            (pandas_df['symbol'] != '')\n",
        "        ]\n",
        "\n",
        "        try:\n",
        "            # Try H2O import with minimal parameters\n",
        "            h2o_frame = h2o.H2OFrame(pandas_df)\n",
        "            h2o_frames[feature_type] = h2o_frame\n",
        "        except Exception as e:\n",
        "            print(f\"Error importing {feature_type} to H2O Frame: {e}\")\n",
        "            traceback.print_exc()\n",
        "\n",
        "    return h2o_frames\n",
        "\n",
        "def run_enhanced_automl(h2o_frames, target_strategy='random'):\n",
        "    \"\"\"\n",
        "    Run AutoML on H2O frames with enhanced group analysis and combined predictions\n",
        "    \"\"\"\n",
        "    def add_target_column(h2o_frames, target_strategy='random'):\n",
        "        \"\"\"Add target column to H2O frames.\"\"\"\n",
        "        frames_with_target = h2o_frames.copy()\n",
        "\n",
        "        for feature_type, h2o_frame in frames_with_target.items():\n",
        "            num_rows = h2o_frame.nrows\n",
        "\n",
        "            if target_strategy == 'random':\n",
        "                target = h2o.H2OFrame(np.random.randint(0, 2, size=(num_rows, 1)),\n",
        "                                    column_names=['target'])\n",
        "                target = target.asfactor()\n",
        "            elif target_strategy == 'symbol_based':\n",
        "                target = h2o.H2OFrame(\n",
        "                    h2o_frame['symbol']\n",
        "                        .apply(lambda x: 1 if '$' in str(x) else 0)\n",
        "                        .as_data_frame()\n",
        "                ).set_names(['target']).asfactor()\n",
        "            elif target_strategy == 'date_based':\n",
        "                target = h2o.H2OFrame(\n",
        "                    h2o_frame['date']\n",
        "                        .apply(lambda x: 1 if str(x)[:4] > '2020' else 0)\n",
        "                        .as_data_frame()\n",
        "                ).set_names(['target']).asfactor()\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown target strategy: {target_strategy}\")\n",
        "\n",
        "            h2o_frame['target'] = target\n",
        "            frames_with_target[feature_type] = h2o_frame\n",
        "\n",
        "        return frames_with_target\n",
        "\n",
        "    def create_combined_frame(frames_with_target):\n",
        "        \"\"\"Create a combined H2O frame with features from all groups.\"\"\"\n",
        "        print(\"\\nCreating combined feature frame...\")\n",
        "        try:\n",
        "            # Get the first frame to use as base\n",
        "            base_frame = next(iter(frames_with_target.values()))\n",
        "\n",
        "            # Convert base frame to pandas first\n",
        "            base_pd = base_frame.as_data_frame()\n",
        "\n",
        "            # Initialize combined data with core columns\n",
        "            combined_data = base_pd[['date', 'symbol']].copy()\n",
        "\n",
        "            # Add target column\n",
        "            target_pd = base_frame['target'].as_data_frame()\n",
        "            combined_data['target'] = target_pd\n",
        "\n",
        "            # Add features from each group with prefixes\n",
        "            for feature_type, h2o_frame in frames_with_target.items():\n",
        "                feature_cols = [col for col in h2o_frame.columns\n",
        "                              if col not in ['date', 'symbol', 'target']]\n",
        "\n",
        "                if not feature_cols:\n",
        "                    print(f\"Warning: No feature columns found for {feature_type}\")\n",
        "                    continue\n",
        "\n",
        "                # Convert feature columns to pandas\n",
        "                feature_df = h2o_frame[feature_cols].as_data_frame()\n",
        "\n",
        "                # Add prefix to column names\n",
        "                feature_df.columns = [f\"{feature_type}_{col}\" for col in feature_cols]\n",
        "\n",
        "                print(f\"\\nAdding {len(feature_cols)} features from {feature_type}\")\n",
        "                print(\"Sample of new feature names:\", list(feature_df.columns)[:3])\n",
        "\n",
        "                # Join with combined data\n",
        "                combined_data = pd.concat([combined_data, feature_df], axis=1)\n",
        "\n",
        "            print(f\"\\nFinal combined frame shape: {combined_data.shape}\")\n",
        "            print(f\"Total features: {combined_data.shape[1] - 3}\")\n",
        "\n",
        "            # Convert back to H2O Frame\n",
        "            combined_h2o = h2o.H2OFrame(combined_data)\n",
        "            combined_h2o['target'] = combined_h2o['target'].asfactor()\n",
        "\n",
        "            return combined_h2o\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in create_combined_frame: {str(e)}\")\n",
        "            traceback.print_exc()\n",
        "            return None\n",
        "\n",
        "    def run_automl_for_frame(train, valid, name, max_runtime_secs=1200):\n",
        "        \"\"\"Run AutoML for a specific frame with comprehensive settings.\"\"\"\n",
        "        print(f\"\\nRunning AutoML for {name}\")\n",
        "\n",
        "        train['target'] = train['target'].asfactor()\n",
        "        valid['target'] = valid['target'].asfactor()\n",
        "\n",
        "        predictors = [col for col in train.columns\n",
        "                     if col not in ['date', 'symbol', 'target']]\n",
        "\n",
        "        aml = H2OAutoML(\n",
        "            max_runtime_secs=max_runtime_secs,\n",
        "            seed=1234,\n",
        "            sort_metric='AUC',\n",
        "            max_models=100,\n",
        "            stopping_metric='AUC',\n",
        "            nfolds=5,\n",
        "            keep_cross_validation_predictions=True,\n",
        "            verbosity='debug'\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            aml.train(\n",
        "                x=predictors,\n",
        "                y='target',\n",
        "                training_frame=train,\n",
        "                validation_frame=valid\n",
        "            )\n",
        "\n",
        "            best_model = aml.leader\n",
        "            performance = best_model.model_performance(valid)\n",
        "\n",
        "            # Enhanced model analysis\n",
        "            model_analysis = {\n",
        "                'automl': aml,\n",
        "                'best_model': best_model,\n",
        "                'leaderboard': aml.leaderboard,\n",
        "                'performance': performance,\n",
        "                'cv_metrics': aml.cross_validation_metrics(),\n",
        "                'variable_importance': best_model.varimp() if hasattr(best_model, 'varimp') else None\n",
        "            }\n",
        "\n",
        "            print(f\"\\nBest {name} Model Performance:\")\n",
        "            print(performance)\n",
        "            print(\"\\nConfusion Matrix:\")\n",
        "            print(performance.confusion_matrix())\n",
        "            print(f\"\\n{name} Leaderboard:\")\n",
        "            print(aml.leaderboard)\n",
        "\n",
        "            return model_analysis\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in AutoML for {name}: {e}\")\n",
        "            traceback.print_exc()\n",
        "            return None\n",
        "\n",
        "    # Add target column to all frames\n",
        "    frames_with_target = add_target_column(h2o_frames, target_strategy)\n",
        "\n",
        "    # Create combined frame\n",
        "    combined_frame = create_combined_frame(frames_with_target)\n",
        "\n",
        "    # Results dictionary for all models\n",
        "    all_results = {\n",
        "        'individual_models': {},\n",
        "        'combined_model': None,\n",
        "        'ensemble_predictions': None,\n",
        "        'feature_analysis': {}\n",
        "    }\n",
        "\n",
        "    # Run AutoML for each feature type\n",
        "    for feature_type, h2o_frame in frames_with_target.items():\n",
        "        train, valid = h2o_frame.split_frame(ratios=[0.8], seed=1234)\n",
        "        result = run_automl_for_frame(train, valid, feature_type)\n",
        "\n",
        "        if result:\n",
        "            all_results['individual_models'][feature_type] = result\n",
        "\n",
        "    # Run AutoML on combined features\n",
        "    if combined_frame is not None:\n",
        "        train_combined, valid_combined = combined_frame.split_frame(ratios=[0.8], seed=1234)\n",
        "        combined_result = run_automl_for_frame(\n",
        "            train_combined,\n",
        "            valid_combined,\n",
        "            \"combined_features\",\n",
        "            max_runtime_secs=2400\n",
        "        )\n",
        "\n",
        "        if combined_result:\n",
        "            all_results['combined_model'] = combined_result\n",
        "\n",
        "    # Create ensemble predictions\n",
        "    if all_results['individual_models']:\n",
        "        print(\"\\nCreating ensemble predictions...\")\n",
        "        ensemble_predictions = {}\n",
        "        weights = {}\n",
        "\n",
        "        # Calculate weights based on model performance\n",
        "        for feature_type, result in all_results['individual_models'].items():\n",
        "            auc = result['performance'].auc()\n",
        "            weights[feature_type] = auc\n",
        "\n",
        "        # Normalize weights\n",
        "        total_weight = sum(weights.values())\n",
        "        weights = {k: v/total_weight for k, v in weights.items()}\n",
        "\n",
        "        # Get predictions from all models\n",
        "        for feature_type, result in all_results['individual_models'].items():\n",
        "            predictions = result['best_model'].predict(valid_combined)\n",
        "            ensemble_predictions[feature_type] = predictions\n",
        "\n",
        "        # Calculate weighted ensemble predictions\n",
        "        weighted_preds = None\n",
        "        for feature_type, preds in ensemble_predictions.items():\n",
        "            if weighted_preds is None:\n",
        "                weighted_preds = preds * weights[feature_type]\n",
        "            else:\n",
        "                weighted_preds += preds * weights[feature_type]\n",
        "\n",
        "        all_results['ensemble_predictions'] = {\n",
        "            'predictions': weighted_preds,\n",
        "            'weights': weights\n",
        "        }\n",
        "\n",
        "    # Analyze feature importance across all models\n",
        "    feature_importance = {}\n",
        "    for feature_type, result in all_results['individual_models'].items():\n",
        "        if result['variable_importance'] is not None:\n",
        "            feature_importance[feature_type] = result['variable_importance']\n",
        "\n",
        "    if all_results['combined_model'] and all_results['combined_model']['variable_importance'] is not None:\n",
        "        feature_importance['combined'] = all_results['combined_model']['variable_importance']\n",
        "\n",
        "    all_results['feature_analysis'] = feature_importance\n",
        "\n",
        "    return all_results\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main data processing function with comprehensive error handling and enhanced AutoML integration.\n",
        "    \"\"\"\n",
        "    # Verify and set up environment\n",
        "    if not setup_environment():\n",
        "        print(\"Environment setup failed.\")\n",
        "        return None, None, None, None, None\n",
        "\n",
        "    # Create Spark session\n",
        "    spark = create_spark_session()\n",
        "\n",
        "    # Initialize H2O context\n",
        "    h2o_conn = initialize_h2o_context()\n",
        "\n",
        "    try:\n",
        "        # Test Spark connection\n",
        "        print(\"Testing Spark connection...\")\n",
        "        test_df = spark.createDataFrame([(1, \"test\")], [\"id\", \"value\"])\n",
        "        test_df.show()\n",
        "\n",
        "        # Check H2O connection\n",
        "        if not h2o_conn:\n",
        "            print(\"Failed to initialize H2O.\")\n",
        "            return None, None, None, None, None\n",
        "\n",
        "        # Load data\n",
        "        print(\"\\nLoading data...\")\n",
        "        absolute_path = os.path.abspath('yiedl_latest.parquet')\n",
        "        df = spark.read.parquet(\"file://\" + absolute_path)\n",
        "\n",
        "        # Process different feature types\n",
        "        feature_types = {\n",
        "            'pvm': (1, 50),\n",
        "            'sentiment': (1, 50),\n",
        "            'onchain': (1, 50)\n",
        "        }\n",
        "\n",
        "        results = {}\n",
        "        for feature_type, (start, end) in feature_types.items():\n",
        "            print(f\"\\nProcessing {feature_type} features {start}-{end}...\")\n",
        "            results[feature_type] = process_feature_group(df, feature_type, start, end)\n",
        "\n",
        "            feature_col = f\"features_{feature_type}_{start}_{end}_array\"\n",
        "            print(f\"\\n{feature_type.upper()} Features Sample:\")\n",
        "            results[feature_type].select(\"date\", \"symbol\", feature_col).show(5, truncate=False)\n",
        "\n",
        "            print(f\"\\nStats for {feature_type}:\")\n",
        "            results[feature_type].select(F.count(\"*\").alias(\"count\")).show()\n",
        "\n",
        "        # Convert to H2O Frames\n",
        "        h2o_frames = convert_to_h2o_frames(results)\n",
        "\n",
        "        # Run Enhanced AutoML\n",
        "        print(\"\\nRunning Enhanced AutoML with combined features and ensemble predictions...\")\n",
        "        enhanced_results = run_enhanced_automl(h2o_frames, target_strategy='random')\n",
        "\n",
        "        # Print comprehensive results summary\n",
        "        if enhanced_results:\n",
        "            print(\"\\nEnhanced AutoML Results Summary:\")\n",
        "\n",
        "            # Individual model results\n",
        "            for feature_type, model_results in enhanced_results['individual_models'].items():\n",
        "                print(f\"\\n{feature_type.upper()} Model Performance:\")\n",
        "                print(f\"AUC: {model_results['performance'].auc()}\")\n",
        "                print(f\"Accuracy: {model_results['performance'].accuracy()}\")\n",
        "                print(\"\\nTop Models:\")\n",
        "                print(model_results['leaderboard'].head().as_data_frame())\n",
        "\n",
        "            # Combined model results\n",
        "            if enhanced_results['combined_model']:\n",
        "                print(\"\\nCombined Features Model Performance:\")\n",
        "                combined_perf = enhanced_results['combined_model']['performance']\n",
        "                print(f\"AUC: {combined_perf.auc()}\")\n",
        "                print(f\"Accuracy: {combined_perf.accuracy()}\")\n",
        "                print(\"\\nTop Combined Models:\")\n",
        "                print(enhanced_results['combined_model']['leaderboard'].head().as_data_frame())\n",
        "\n",
        "            # Ensemble results\n",
        "            if enhanced_results['ensemble_predictions']:\n",
        "                print(\"\\nEnsemble Model Weights:\")\n",
        "                for model_type, weight in enhanced_results['ensemble_predictions']['weights'].items():\n",
        "                    print(f\"{model_type}: {weight:.4f}\")\n",
        "\n",
        "            # Feature importance analysis\n",
        "            if enhanced_results['feature_analysis']:\n",
        "                print(\"\\nFeature Importance Summary:\")\n",
        "                for model_type, importance in enhanced_results['feature_analysis'].items():\n",
        "                    if importance is not None:\n",
        "                        print(f\"\\n{model_type.upper()} Top Important Features:\")\n",
        "                        if isinstance(importance, pd.DataFrame):\n",
        "                            print(importance.head())\n",
        "                        else:\n",
        "                            print(importance)\n",
        "\n",
        "        return spark, h2o_conn, results, h2o_frames, enhanced_results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Processing error: {e}\")\n",
        "        traceback.print_exc()\n",
        "        return None, None, None, None, None\n",
        "\n",
        "    finally:\n",
        "        # Cleanup\n",
        "        try:\n",
        "            if 'h2o' in sys.modules:\n",
        "                h2o.cluster().shutdown()\n",
        "            if spark:\n",
        "                spark.stop()\n",
        "        except Exception as cleanup_error:\n",
        "            print(f\"Cleanup error: {cleanup_error}\")\n",
        "\n",
        "def analyze_enhanced_results(enhanced_results):\n",
        "    \"\"\"\n",
        "    Analyze and visualize the enhanced AutoML results\n",
        "    \"\"\"\n",
        "    if not enhanced_results:\n",
        "        print(\"No enhanced results to analyze\")\n",
        "        return\n",
        "\n",
        "    # Create performance comparison plots\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Model performance comparison\n",
        "    aucs = []\n",
        "    accuracies = []\n",
        "    model_names = []\n",
        "\n",
        "    # Gather metrics from individual models\n",
        "    for feature_type, results in enhanced_results['individual_models'].items():\n",
        "        aucs.append(results['performance'].auc())\n",
        "        accuracies.append(results['performance'].accuracy())\n",
        "        model_names.append(feature_type)\n",
        "\n",
        "    # Add combined model metrics if available\n",
        "    if enhanced_results['combined_model']:\n",
        "        aucs.append(enhanced_results['combined_model']['performance'].auc())\n",
        "        accuracies.append(enhanced_results['combined_model']['performance'].accuracy())\n",
        "        model_names.append('combined')\n",
        "\n",
        "    # Plot performance metrics\n",
        "    x = np.arange(len(model_names))\n",
        "    width = 0.35\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    ax.bar(x - width/2, aucs, width, label='AUC')\n",
        "    ax.bar(x + width/2, accuracies, width, label='Accuracy')\n",
        "\n",
        "    ax.set_ylabel('Score')\n",
        "    ax.set_title('Model Performance Comparison')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(model_names)\n",
        "    ax.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Feature importance visualization\n",
        "    if enhanced_results['feature_analysis']:\n",
        "        for model_type, importance in enhanced_results['feature_analysis'].items():\n",
        "            if importance is not None:\n",
        "                plt.figure(figsize=(10, 6))\n",
        "\n",
        "                if isinstance(importance, pd.DataFrame):\n",
        "                    importance.head(10).plot(kind='barh')\n",
        "                else:\n",
        "                    # Convert to DataFrame if not already\n",
        "                    imp_df = pd.DataFrame(importance).head(10)\n",
        "                    imp_df.plot(kind='barh')\n",
        "\n",
        "                plt.title(f'{model_type} Feature Importance')\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "# Run the entire process\n",
        "if __name__ == \"__main__\":\n",
        "    spark, h2o_conn, results, h2o_frames, enhanced_results = main()\n",
        "\n",
        "    if enhanced_results:\n",
        "        analyze_enhanced_results(enhanced_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b46d435-0402-402f-a678-0a8a9a1e3e8e",
      "metadata": {
        "id": "5b46d435-0402-402f-a678-0a8a9a1e3e8e"
      },
      "outputs": [],
      "source": [
        "def analyze_feature_group_models(feature_group_results, h2o_frames):\n",
        "    \"\"\"\n",
        "    Analyze and combine results from different feature group models\n",
        "\n",
        "    Args:\n",
        "        feature_group_results (dict): Results from each feature group model\n",
        "        h2o_frames (dict): H2O frames used for training\n",
        "\n",
        "    Returns:\n",
        "        dict: Combined analysis results\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logger.info(\"Analyzing feature group model results...\")\n",
        "\n",
        "        combined_analysis = {\n",
        "            'model_metrics': {},\n",
        "            'feature_importance': {},\n",
        "            'cross_group_correlations': {},\n",
        "            'ensemble_predictions': None\n",
        "        }\n",
        "\n",
        "        # Analyze each feature group's model performance\n",
        "        for feature_type, results in feature_group_results.items():\n",
        "            # Extract model metrics\n",
        "            model_metrics = {\n",
        "                'auc': results.model.model_performance().auc(),\n",
        "                'logloss': results.model.model_performance().logloss(),\n",
        "                'accuracy': results.model.model_performance().accuracy(),\n",
        "                'precision': results.model.model_performance().precision(),\n",
        "                'recall': results.model.model_performance().recall()\n",
        "            }\n",
        "            combined_analysis['model_metrics'][feature_type] = model_metrics\n",
        "\n",
        "            # Get feature importance if available\n",
        "            try:\n",
        "                feature_importance = results.model.varimp(use_pandas=True)\n",
        "                combined_analysis['feature_importance'][feature_type] = feature_importance\n",
        "            except:\n",
        "                logger.warning(f\"Could not extract feature importance for {feature_type}\")\n",
        "\n",
        "        # Calculate cross-group prediction correlations\n",
        "        predictions = {}\n",
        "        for feature_type, results in feature_group_results.items():\n",
        "            pred = results.model.predict(h2o_frames[feature_type])\n",
        "            predictions[feature_type] = pred.as_data_frame()['predict']\n",
        "\n",
        "        # Convert predictions to pandas DataFrame for correlation analysis\n",
        "        pred_df = pd.DataFrame(predictions)\n",
        "        combined_analysis['cross_group_correlations'] = pred_df.corr()\n",
        "\n",
        "        # Create ensemble predictions using weighted voting\n",
        "        weights = {\n",
        "            feature_type: metrics['auc']  # Use AUC as weight\n",
        "            for feature_type, metrics in combined_analysis['model_metrics'].items()\n",
        "        }\n",
        "\n",
        "        # Normalize weights\n",
        "        total_weight = sum(weights.values())\n",
        "        weights = {k: v/total_weight for k, v in weights.items()}\n",
        "\n",
        "        # Calculate weighted ensemble predictions\n",
        "        ensemble_pred = np.zeros(len(next(iter(predictions.values()))))\n",
        "        for feature_type, preds in predictions.items():\n",
        "            ensemble_pred += weights[feature_type] * preds\n",
        "\n",
        "        combined_analysis['ensemble_predictions'] = (ensemble_pred > 0.5).astype(int)\n",
        "\n",
        "        # Calculate ensemble metrics\n",
        "        true_labels = h2o_frames[next(iter(h2o_frames))]['target'].as_data_frame()\n",
        "        ensemble_metrics = {\n",
        "            'accuracy': np.mean(combined_analysis['ensemble_predictions'] == true_labels),\n",
        "            'weighted_auc_score': sum(weights[ft] * metrics['auc']\n",
        "                                    for ft, metrics in combined_analysis['model_metrics'].items())\n",
        "        }\n",
        "        combined_analysis['ensemble_metrics'] = ensemble_metrics\n",
        "\n",
        "        # Generate summary report\n",
        "        report = f\"\"\"\n",
        "Feature Group Model Analysis Summary:\n",
        "-----------------------------------\n",
        "Number of feature groups analyzed: {len(feature_group_results)}\n",
        "Best performing group: {max(combined_analysis['model_metrics'].items(),\n",
        "                          key=lambda x: x[1]['auc'])[0]}\n",
        "Ensemble weighted AUC: {ensemble_metrics['weighted_auc_score']:.4f}\n",
        "Ensemble accuracy: {ensemble_metrics['accuracy']:.4f}\n",
        "\n",
        "Individual Group Performances:\n",
        "\"\"\"\n",
        "        for feature_type, metrics in combined_analysis['model_metrics'].items():\n",
        "            report += f\"\\n{feature_type}:\"\n",
        "            report += f\"\\n  AUC: {metrics['auc']:.4f}\"\n",
        "            report += f\"\\n  Accuracy: {metrics['accuracy']:.4f}\"\n",
        "            report += f\"\\n  Recall: {metrics['recall']:.4f}\"\n",
        "\n",
        "        combined_analysis['summary_report'] = report\n",
        "        logger.info(\"Model analysis completed successfully\")\n",
        "\n",
        "        return combined_analysis\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in analyze_feature_group_models: {e}\")\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "# Usage example:\n",
        "# assuming feature_group_results contains the H2O model results per feature group\n",
        "# and h2o_frames contains the corresponding H2O frames\n",
        "\n",
        "analysis_results = analyze_feature_group_models(feature_group_results, h2o_frames)\n",
        "\n",
        "if analysis_results:\n",
        "    print(analysis_results['summary_report'])\n",
        "\n",
        "    # Plot feature importance for each group\n",
        "    for feature_type, importance_df in analysis_results['feature_importance'].items():\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        importance_df.plot(kind='bar', x='variable', y='relative_importance')\n",
        "        plt.title(f'Feature Importance - {feature_type}')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # Plot correlation heatmap\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(analysis_results['cross_group_correlations'],\n",
        "                annot=True, cmap='coolwarm', center=0)\n",
        "    plt.title('Cross-Group Prediction Correlations')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eaa2d4d9-6a48-4f51-afdc-974d8775de6a",
      "metadata": {
        "id": "eaa2d4d9-6a48-4f51-afdc-974d8775de6a"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import logging\n",
        "import os\n",
        "import socket\n",
        "import subprocess\n",
        "import sys\n",
        "import traceback\n",
        "from typing import Dict, Any\n",
        "\n",
        "import h2o\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from h2o.automl import H2OAutoML\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import GBTClassifier, RandomForestClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.sql import SparkSession, Window, functions as F\n",
        "from pyspark.sql.types import ArrayType, DoubleType\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Load configurations from file\n",
        "with open('config.json') as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "def setup_environment():\n",
        "    \"\"\"\n",
        "    Set up Python environment with necessary dependencies.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if setup is successful, False otherwise\n",
        "    \"\"\"\n",
        "    logger.info(\"Setting up Python environment...\")\n",
        "\n",
        "    try:\n",
        "        # Verify and install required packages\n",
        "        for package in config['required_packages']:\n",
        "            try:\n",
        "                __import__(package.replace('-', '_'))\n",
        "                logger.info(f\"{package} is already installed.\")\n",
        "            except ImportError:\n",
        "                logger.info(f\"Installing {package}...\")\n",
        "                subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Environment setup error: {e}\")\n",
        "        return False\n",
        "\n",
        "def get_local_ip():\n",
        "    \"\"\"Retrieve the local IP address.\"\"\"\n",
        "    try:\n",
        "        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
        "        s.connect((\"8.8.8.8\", 80))\n",
        "        ip = s.getsockname()[0]\n",
        "        s.close()\n",
        "        return ip\n",
        "    except Exception:\n",
        "        return \"127.0.0.1\"\n",
        "\n",
        "def create_spark_session():\n",
        "    \"\"\"\n",
        "    Create a Spark session with configurations from file.\n",
        "\n",
        "    Returns:\n",
        "        SparkSession: Configured Spark session\n",
        "    \"\"\"\n",
        "    logger.info(\"Creating Spark session...\")\n",
        "\n",
        "    # Get local IP for configuration\n",
        "    local_ip = get_local_ip()\n",
        "\n",
        "    # Create Spark session with configurations from file\n",
        "    spark_config = config['spark']\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(spark_config['app_name']) \\\n",
        "        .master(spark_config['master']) \\\n",
        "        .config(\"spark.driver.host\", local_ip) \\\n",
        "        .config(\"spark.driver.bindAddress\", local_ip) \\\n",
        "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", spark_config['arrow_enabled']) \\\n",
        "        .config(\"spark.driver.memory\", spark_config['driver_memory']) \\\n",
        "        .config(\"spark.executor.memory\", spark_config['executor_memory']) \\\n",
        "        .config(\"spark.network.timeout\", spark_config['network_timeout']) \\\n",
        "        .config(\"spark.executor.heartbeatInterval\", spark_config['heartbeat_interval']) \\\n",
        "        .config(\"spark.sql.session.timeZone\", spark_config['timezone']) \\\n",
        "        .getOrCreate()\n",
        "\n",
        "    return spark\n",
        "\n",
        "def initialize_h2o_context():\n",
        "    \"\"\"\n",
        "    Initialize H2O context with configurations from file.\n",
        "\n",
        "    Returns:\n",
        "        H2OContext or None: Initialized H2O context\n",
        "    \"\"\"\n",
        "    logger.info(\"Initializing H2O context...\")\n",
        "\n",
        "    try:\n",
        "        h2o_config = config['h2o']\n",
        "        h2o.init(\n",
        "            ip=h2o_config['ip'],\n",
        "            port=h2o_config['port'],\n",
        "            strict_version_check=False,\n",
        "            max_mem_size=h2o_config['max_mem_size']\n",
        "        )\n",
        "        return h2o.connection()\n",
        "    except Exception as e:\n",
        "        logger.error(f\"H2O initialization error: {e}\")\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "def validate_data(df):\n",
        "    \"\"\"\n",
        "    Validate input data and perform quality checks.\n",
        "\n",
        "    Args:\n",
        "        df (pyspark.sql.DataFrame): Input DataFrame\n",
        "\n",
        "    Returns:\n",
        "        bool: True if data validation passes, False otherwise\n",
        "    \"\"\"\n",
        "    logger.info(\"Validating input data...\")\n",
        "\n",
        "    try:\n",
        "        # Check for required columns\n",
        "        required_columns = config['required_columns']\n",
        "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
        "        if missing_columns:\n",
        "            logger.error(f\"Missing required columns: {missing_columns}\")\n",
        "            return False\n",
        "\n",
        "        # Check for missing values\n",
        "        missing_counts = df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns]).collect()[0].asDict()\n",
        "        if any(count > 0 for count in missing_counts.values()):\n",
        "            logger.warning(f\"Missing values detected: {missing_counts}\")\n",
        "\n",
        "        # Additional data quality checks can be added here\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Data validation error: {e}\")\n",
        "        return False\n",
        "\n",
        "def process_feature_group(df, feature_prefix, start_idx, end_idx):\n",
        "    \"\"\"\n",
        "    Process a group of features and return transformed DataFrame.\n",
        "    \"\"\"\n",
        "    logger.info(f\"Processing {feature_prefix} features {start_idx}-{end_idx}...\")\n",
        "\n",
        "    # Generate column names\n",
        "    cols = [f'{feature_prefix}_{str(i).zfill(4)}' for i in range(start_idx, end_idx + 1)]\n",
        "\n",
        "    # Select relevant columns and cast to double\n",
        "    df_subset = df.select(['date', 'symbol'] + cols)\n",
        "\n",
        "    for col_name in cols:\n",
        "        df_subset = df_subset.withColumn(col_name, F.col(col_name).cast(\"double\"))\n",
        "\n",
        "    # Vector to array conversion UDF\n",
        "    vector_to_array_udf = F.udf(lambda vector: vector.toArray().tolist() if vector is not None else None, ArrayType(DoubleType()))\n",
        "\n",
        "    # Create VectorAssembler\n",
        "    assembler = VectorAssembler(\n",
        "        inputCols=cols,\n",
        "        outputCol=f\"features_{feature_prefix}_{start_idx}_{end_idx}\",\n",
        "        handleInvalid=\"keep\"\n",
        "    )\n",
        "\n",
        "    # Transform and add array column\n",
        "    result = assembler.transform(df_subset).withColumn(\n",
        "        f\"features_{feature_prefix}_{start_idx}_{end_idx}_array\",\n",
        "        vector_to_array_udf(f\"features_{feature_prefix}_{start_idx}_{end_idx}\")\n",
        "    )\n",
        "\n",
        "    return result\n",
        "\n",
        "def engineer_features(df):\n",
        "    \"\"\"\n",
        "    Perform feature engineering on the input DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df (pyspark.sql.DataFrame): Input DataFrame\n",
        "\n",
        "    Returns:\n",
        "        pyspark.sql.DataFrame: DataFrame with engineered features\n",
        "    \"\"\"\n",
        "    logger.info(\"Performing feature engineering...\")\n",
        "\n",
        "    # Rolling window features\n",
        "    window_spec = Window.partitionBy('symbol').orderBy('date')\n",
        "    df = df \\\n",
        "        .withColumn('pvm_rolling_mean_5', F.avg('pvm').over(window_spec.rowsBetween(-5, 0))) \\\n",
        "        .withColumn('pvm_rolling_mean_10', F.avg('pvm').over(window_spec.rowsBetween(-10, 0))) \\\n",
        "        .withColumn('sentiment_rolling_mean_5', F.avg('sentiment').over(window_spec.rowsBetween(-5, 0))) \\\n",
        "        .withColumn('onchain_rolling_mean_5', F.avg('onchain').over(window_spec.rowsBetween(-5, 0)))\n",
        "\n",
        "    # Volatility measures\n",
        "    df = df \\\n",
        "        .withColumn('pvm_volatility', F.stddev('pvm').over(window_spec.rowsBetween(-10, 0))) \\\n",
        "        .withColumn('sentiment_volatility', F.stddev('sentiment').over(window_spec.rowsBetween(-10, 0)))\n",
        "\n",
        "    # Additional feature engineering techniques can be added here\n",
        "\n",
        "    return df\n",
        "\n",
        "def convert_to_h2o_frames(results):\n",
        "    \"\"\"\n",
        "    Convert Spark DataFrames to H2O Frames with robust handling.\n",
        "    \"\"\"\n",
        "    logger.info(\"Converting Spark DataFrames to H2O Frames...\")\n",
        "\n",
        "    h2o_frames = {}\n",
        "    for feature_type, result_df in results.items():\n",
        "        feature_col = f\"features_{feature_type}_1_50_array\"\n",
        "\n",
        "        # Collect rows and prepare data\n",
        "        rows = result_df.select(\"date\", \"symbol\", feature_col).collect()\n",
        "\n",
        "        # Create column names\n",
        "        columns = ['date', 'symbol'] + [f'{feature_type}_{i+1:04d}' for i in range(50)]\n",
        "\n",
        "        # Prepare data for conversion\n",
        "        data_rows = []\n",
        "        for row in rows:\n",
        "            date = str(row['date']) if row['date'] is not None else ''\n",
        "            symbol = str(row['symbol']) if row['symbol'] is not None else ''\n",
        "            features = row[feature_col]\n",
        "\n",
        "            # Ensure features is a list of 50 elements\n",
        "            if features is None:\n",
        "                features = [np.nan] * 50\n",
        "            else:\n",
        "                features = (features + [np.nan] * 50)[:50]\n",
        "\n",
        "            # Validate and convert features\n",
        "            features = [\n",
        "                None if pd.isna(f) else float(f)\n",
        "                for f in features\n",
        "            ]\n",
        "\n",
        "            data_rows.append([date, symbol] + features)\n",
        "\n",
        "        # Convert to Pandas DataFrame\n",
        "        pandas_df = pd.DataFrame(data_rows, columns=columns)\n",
        "\n",
        "        # Remove rows with empty date or symbol\n",
        "        pandas_df = pandas_df[\n",
        "            (pandas_df['date'] != '') &\n",
        "            (pandas_df['symbol'] != '')\n",
        "        ]\n",
        "\n",
        "        try:\n",
        "            h2o_frame = h2o.H2OFrame(pandas_df)\n",
        "            h2o_frames[feature_type] = h2o_frame\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error importing {feature_type} to H2O Frame: {e}\")\n",
        "            traceback.print_exc()\n",
        "\n",
        "    return h2o_frames\n",
        "\n",
        "def create_model_pipeline(feature_cols):\n",
        "    \"\"\"\n",
        "    Create a machine learning pipeline with multiple classifiers.\n",
        "\n",
        "    Args:\n",
        "        feature_cols (list): List of feature column names\n",
        "\n",
        "    Returns:\n",
        "        Pipeline: Machine learning pipeline\n",
        "    \"\"\"\n",
        "    logger.info(\"Creating model pipeline...\")\n",
        "\n",
        "    # Vector Assembler\n",
        "    assembler = VectorAssembler(\n",
        "        inputCols=feature_cols,\n",
        "        outputCol=\"features\"\n",
        "    )\n",
        "\n",
        "    # Feature scaling\n",
        "    scaler = StandardScaler(\n",
        "        inputCol=\"features\",\n",
        "        outputCol=\"scaled_features\"\n",
        "    )\n",
        "\n",
        "    # Classifiers\n",
        "    rf_classifier = RandomForestClassifier(\n",
        "        labelCol=\"target\",\n",
        "        featuresCol=\"scaled_features\",\n",
        "        numTrees=config['random_forest']['num_trees']\n",
        "    )\n",
        "\n",
        "    gbt_classifier = GBTClassifier(\n",
        "        labelCol=\"target\",\n",
        "        featuresCol=\"scaled_features\",\n",
        "        maxIter=config['gradient_boosting']['max_iter']\n",
        "    )\n",
        "\n",
        "    # Pipeline\n",
        "    pipeline = Pipeline(stages=[\n",
        "        assembler,\n",
        "        scaler,\n",
        "        rf_classifier,\n",
        "        gbt_classifier\n",
        "    ])\n",
        "\n",
        "    return pipeline\n",
        "\n",
        "def evaluate_models(models, test_data):\n",
        "    \"\"\"\n",
        "    Evaluate trained models on test data.\n",
        "\n",
        "    Args:\n",
        "        models (dict): Trained models\n",
        "        test_data (pyspark.sql.DataFrame): Test data\n",
        "\n",
        "    Returns:\n",
        "        dict: Model evaluation results\n",
        "    \"\"\"\n",
        "    logger.info(\"Evaluating models...\")\n",
        "\n",
        "    evaluator = BinaryClassificationEvaluator(labelCol=\"target\")\n",
        "\n",
        "    evaluation_results = {}\n",
        "    for model_name, model in models.items():\n",
        "        predictions = model.transform(test_data)\n",
        "\n",
        "        auc = evaluator.evaluate(predictions)\n",
        "        accuracy = predictions.filter(predictions.prediction == predictions.target).count() / predictions.count()\n",
        "\n",
        "        evaluation_results[model_name] = {\n",
        "            'auc': auc,\n",
        "            'accuracy': accuracy\n",
        "        }\n",
        "\n",
        "    return evaluation_results\n",
        "\n",
        "def tune_hyperparameters(pipeline, train_data, evaluator):\n",
        "    \"\"\"\n",
        "    Perform hyperparameter tuning using cross-validation.\n",
        "\n",
        "    Args:\n",
        "        pipeline (Pipeline): Model pipeline\n",
        "        train_data (pyspark.sql.DataFrame): Training data\n",
        "        evaluator (pyspark.ml.evaluation.Evaluator): Evaluation metric\n",
        "\n",
        "    Returns:\n",
        "        tuple: Best model and best parameters\n",
        "    \"\"\"\n",
        "    logger.info(\"Performing hyperparameter tuning...\")\n",
        "\n",
        "    param_grid = ParamGridBuilder() \\\n",
        "        .addGrid(pipeline.stages[-2].numTrees, config['random_forest']['num_trees_grid']) \\\n",
        "        .addGrid(pipeline.stages[-1].maxIter, config['gradient_boosting']['max_iter_grid']) \\\n",
        "        .build()\n",
        "\n",
        "    cv = CrossValidator(\n",
        "        estimator=pipeline,\n",
        "        estimatorParamMaps=param_grid,\n",
        "        evaluator=evaluator,\n",
        "        numFolds=config['cross_validation']['num_folds']\n",
        "    )\n",
        "\n",
        "    cv_model = cv.fit(train_data)\n",
        "\n",
        "    return cv_model.bestModel, {param.name: value for param, value in cv_model.bestModel.extractParamMap().items()}\n",
        "\n",
        "def run_automl(h2o_frames, target_strategy='random'):\n",
        "    \"\"\"\n",
        "    Run AutoML on H2O frames with different target strategies.\n",
        "    \"\"\"\n",
        "    logger.info(\"Running AutoML...\")\n",
        "\n",
        "    def add_target_column(h2o_frames, target_strategy='random'):\n",
        "        \"\"\"Add target column to H2O frames.\"\"\"\n",
        "        frames_with_target = h2o_frames.copy()\n",
        "\n",
        "        for feature_type, h2o_frame in frames_with_target.items():\n",
        "            num_rows = h2o_frame.nrows\n",
        "\n",
        "            if target_strategy == 'random':\n",
        "                target = h2o.H2OFrame(\n",
        "                    np.random.randint(0, 2, size=(num_rows, 1)),\n",
        "                    column_names=['target']\n",
        "                ).asfactor()\n",
        "            elif target_strategy == 'symbol_based':\n",
        "                target = h2o.H2OFrame(\n",
        "                    h2o_frame['symbol']\n",
        "                        .apply(lambda x: 1 if '$' in str(x) else 0)\n",
        "                        .as_data_frame()\n",
        "                ).set_names(['target']).asfactor()\n",
        "            elif target_strategy == 'date_based':\n",
        "                target = h2o.H2OFrame(\n",
        "                    h2o_frame['date']\n",
        "                        .apply(lambda x: 1 if str(x)[:4] > '2020' else 0)\n",
        "                        .as_data_frame()\n",
        "                ).set_names(['target']).asfactor()\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown target strategy: {target_strategy}\")\n",
        "\n",
        "            h2o_frame['target'] = target\n",
        "            frames_with_target[feature_type] = h2o_frame\n",
        "\n",
        "        return frames_with"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IHWOONeRhJiI",
      "metadata": {
        "id": "IHWOONeRhJiI"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# Initialize H2O\n",
        "import h2o\n",
        "from pysparkling import *\n",
        "hc = H2OContext.getOrCreate()\n",
        "\n",
        "# Convert Spark DataFrames to H2O Frames\n",
        "train_h2o = hc.asH2OFrame(train_processed)\n",
        "live_h2o = hc.asH2OFrame(live_processed)\n",
        "\n",
        "# Initialize H2O AutoML\n",
        "aml = H2OAutoML(\n",
        "    max_models=20,\n",
        "    seed=1,\n",
        "    max_runtime_secs=600  # 3600: 1 hour 600: 10 min\n",
        ")\n",
        "\n",
        "# Train models using AutoML\n",
        "target_column = \"target\"  # Replace with your actual target column\n",
        "aml.train(x=train_h2o.columns[:-1],  # exclude target\n",
        "          y=target_column,\n",
        "          training_frame=train_h2o)\n",
        "\n",
        "# Get model performance\n",
        "print(aml.leader.model_performance(train_h2o))\n",
        "\n",
        "# Make predictions on live data\n",
        "predictions = aml.leader.predict(live_h2o)\n",
        "\n",
        "# Save predictions\n",
        "predictions_df = h2o.as_list(predictions)\n",
        "predictions_spark = spark.createDataFrame(predictions_df)\n",
        "predictions_spark.write.parquet(\"predictions.parquet\")\n",
        "\n",
        "# Stop H2O and Spark sessions\n",
        "h2o.cluster().shutdown()\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71ec9555-aad0-46f6-9d6b-b876bc3ec8c8",
      "metadata": {
        "id": "71ec9555-aad0-46f6-9d6b-b876bc3ec8c8"
      },
      "source": [
        "## Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fc98973-f713-4c11-9736-64914840090c",
      "metadata": {
        "id": "4fc98973-f713-4c11-9736-64914840090c"
      },
      "outputs": [],
      "source": [
        "# Download the Numerai training data to the current directory\n",
        "import os\n",
        "napi.download_dataset(filename = \"crypto/v1.0/train_targets.parquet\",\n",
        "                      dest_path = os.getcwd() + \"/numerai_train_targets.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "919a0699-f468-4035-bed6-2135e15dd648",
      "metadata": {
        "id": "919a0699-f468-4035-bed6-2135e15dd648"
      },
      "outputs": [],
      "source": [
        "# Download the Numerai live crypto universe to the current directory\n",
        "import os\n",
        "napi.download_dataset(filename = \"crypto/v1.0/live_universe.parquet\",\n",
        "                      dest_path = os.getcwd() + \"/numerai_live_universe.parquet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "351f2ecc-ee08-44cc-a449-6278c5495cb0",
      "metadata": {
        "id": "351f2ecc-ee08-44cc-a449-6278c5495cb0"
      },
      "source": [
        "## Import and Display the Numerai Crypto Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd4fdd66-5aed-47e9-badf-372c73c90287",
      "metadata": {
        "id": "cd4fdd66-5aed-47e9-badf-372c73c90287"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9de00365-3cea-4c96-b622-8eb63e7c7497",
      "metadata": {
        "id": "9de00365-3cea-4c96-b622-8eb63e7c7497"
      },
      "outputs": [],
      "source": [
        "# Load the display the Numerai training targets\n",
        "df_numerai_targets = pd.read_parquet(\"numerai_train_targets.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b92cb4fa-bd14-479f-8398-1436116152e5",
      "metadata": {
        "id": "b92cb4fa-bd14-479f-8398-1436116152e5"
      },
      "outputs": [],
      "source": [
        "display(df_numerai_targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37a01437-ea95-4026-a435-c9f5771bdc86",
      "metadata": {
        "id": "37a01437-ea95-4026-a435-c9f5771bdc86"
      },
      "outputs": [],
      "source": [
        "# Load and display the Numerai live universe\n",
        "df_numerai_universe = pd.read_parquet(\"numerai_live_universe.parquet\")\n",
        "display(df_numerai_universe)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "192e1431-6040-403e-bd8b-b459089bd972",
      "metadata": {
        "id": "192e1431-6040-403e-bd8b-b459089bd972"
      },
      "source": [
        "## Download YIEDL Crypto Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37c0f676-4922-486e-bc4e-0bef0eecb99f",
      "metadata": {
        "id": "37c0f676-4922-486e-bc4e-0bef0eecb99f"
      },
      "outputs": [],
      "source": [
        "# Helper Function\n",
        "import requests\n",
        "\n",
        "def download_file(url, output_filename):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        with open(output_filename, 'wb') as file:\n",
        "            file.write(response.content)\n",
        "        print(f\"File downloaded successfully as {output_filename}\")\n",
        "    else:\n",
        "        print(\"Failed to download file\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81ec9aec-fdfc-494d-9666-e54637c6fcfd",
      "metadata": {
        "id": "81ec9aec-fdfc-494d-9666-e54637c6fcfd"
      },
      "outputs": [],
      "source": [
        "# Download YIEDL crypto latest dataset to current directory\n",
        "url = 'https://api.yiedl.ai/yiedl/v1/downloadDataset?type=latest'\n",
        "output_filename = 'yiedl_latest.parquet'\n",
        "download_file(url, output_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88c7b490-d5b3-4eaf-8576-153b4c935446",
      "metadata": {
        "id": "88c7b490-d5b3-4eaf-8576-153b4c935446"
      },
      "outputs": [],
      "source": [
        "# Download YIEDL crypto historical dataset to current directory\n",
        "# NOTE: it is a huge file in zip format. We need to unzip it afterwards\n",
        "url = 'https://api.yiedl.ai/yiedl/v1/downloadDataset?type=historical'\n",
        "output_filename = 'yiedl_historical.zip'\n",
        "download_file(url, output_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d456724-cb04-4be3-9fe0-6e84d9c0e570",
      "metadata": {
        "id": "2d456724-cb04-4be3-9fe0-6e84d9c0e570"
      },
      "outputs": [],
      "source": [
        "# Unzip and rename the file\n",
        "!unzip -p yiedl_historical.zip > yiedl_historical.parquet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b647c95-39ce-4222-a9ab-ce2f52a87f9b",
      "metadata": {
        "id": "4b647c95-39ce-4222-a9ab-ce2f52a87f9b"
      },
      "source": [
        "## Import and Display the YIEDL Crypto Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4de19ee-3a39-477e-a319-da3634688397",
      "metadata": {
        "id": "b4de19ee-3a39-477e-a319-da3634688397"
      },
      "outputs": [],
      "source": [
        "# Load and display the YIEDL historical crypto dataset\n",
        "df_yield_historical = pd.read_parquet(\"yiedl_historical.parquet\",\n",
        "                                      engine = \"pyarrow\",\n",
        "                                      dtype_backend = \"numpy_nullable\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec1e2cd0-0679-4730-99e4-cbcc39e27677",
      "metadata": {
        "id": "ec1e2cd0-0679-4730-99e4-cbcc39e27677"
      },
      "outputs": [],
      "source": [
        "# Check dtypes\n",
        "df_yield_historical.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a55f778-6178-4e53-8baf-f7d80ce4af97",
      "metadata": {
        "id": "8a55f778-6178-4e53-8baf-f7d80ce4af97"
      },
      "outputs": [],
      "source": [
        "# Display\n",
        "display(df_yield_historical)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e248ac4f-214e-4491-9a62-5a6a0f86ee09",
      "metadata": {
        "id": "e248ac4f-214e-4491-9a62-5a6a0f86ee09"
      },
      "outputs": [],
      "source": [
        "# Load and display the YIEDL latest crypto dataset\n",
        "df_yield_latest = pd.read_parquet(\"yiedl_latest.parquet\",\n",
        "                                  engine = \"pyarrow\",\n",
        "                                  dtype_backend = \"numpy_nullable\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fda35267-3923-414c-ab64-46f5df3a5fa1",
      "metadata": {
        "id": "fda35267-3923-414c-ab64-46f5df3a5fa1"
      },
      "outputs": [],
      "source": [
        "# Check dtypes\n",
        "df_yield_latest.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7399bca7-4856-4651-ad86-6102fdccd757",
      "metadata": {
        "id": "7399bca7-4856-4651-ad86-6102fdccd757"
      },
      "outputs": [],
      "source": [
        "# Display\n",
        "display(df_yield_latest)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}